<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>Release Notes for SUSE Linux Enterprise Server 11 Service Pack 2</title><meta name="generator" content="DocBook XSL Stylesheets V1.74.0"><meta name="description" content="These release notes are generic for all products of our SUSE Linux Enterprise Server 11 product line. Some parts may not apply to a particular architecture or product. Where this is not obvious, the specific architectures or products are explicitly listed. Installation Quick Start and Deployment Guides can be found in the docu language directories on the media. Documentation (if installed) is available below the /usr/share/doc/ directory of an installed system. This SUSE product includes materials licensed to SUSE under the GNU General Public License (GPL). The GPL requires SUSE to provide the source code that corresponds to the GPL-licensed material. The source code is available for download at . Also, for up to three years after distribution of the SUSE product, upon request, Novell will mail a copy of the source code. Requests should be sent by e-mail to or as otherwise instructed at . Novell may charge a reasonable fee to recover distribution costs."></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="book" lang="en"><div class="titlepage"><div><div><h1 class="title"><a name="rnotes"></a>Release Notes for SUSE Linux Enterprise Server 11 Service Pack 2</h1></div><div><p class="releaseinfo">Version 11.2.0.28 (2012-02-06)</p></div><div><div class="abstract"><p class="title"><b>Abstract</b></p><p>
    These release notes are generic for all products of our SUSE Linux Enterprise Server 11
    product line. Some parts may not apply to a particular architecture
    or product. Where this is not obvious, the specific architectures or
    products are explicitly listed.
   </p><p>
    Installation Quick Start and Deployment Guides can be found in the
    <code class="filename">docu</code> language directories on the
    media. Documentation (if installed) is available below the
    <code class="filename">/usr/share/doc/</code> directory of an installed
    system.
   </p><p>
     This SUSE product includes materials licensed to SUSE under the
     GNU General Public License (GPL). The GPL requires SUSE to provide the
     source code that corresponds to the GPL-licensed
     material. The source code is available for download at
     <a class="ulink" href="http://www.suse.com/download-linux/source-code.html" target="_top">http://www.suse.com/download-linux/source-code.html</a>. Also, for up to
     three years after distribution of the SUSE product,
     upon request, Novell will mail a copy of the source code. Requests
     should be sent by e-mail to
     <a class="ulink" href="mailto:sle_source_request@novell.com" target="_top">mailto:sle_source_request@novell.com</a> or as otherwise
     instructed at <a class="ulink" href="http://www.suse.com/download-linux/source-code.html" target="_top">http://www.suse.com/download-linux/source-code.html</a>.
     Novell may charge a reasonable fee to recover distribution costs.
   </p></div></div></div><hr></div><div class="toc"><dl><dt><span class="chapter"><a href="#rnotes-purpose">1. SUSE Linux Enterprise Server</a></span></dt><dt><span class="chapter"><a href="#mustread">2. Read Me First</a></span></dt><dt><span class="chapter"><a href="#Support">3. Support Statement for SUSE Linux Enterprise Server</a></span></dt><dd><dl><dt><span class="section"><a href="#Support.General">3.1. General Support Statement</a></span></dt><dd><dl><dt><span class="section"><a href="#id1165424">3.1.1. Tomcat6 and Related Packages</a></span></dt><dt><span class="section"><a href="#id1165431">3.1.2. SELinux</a></span></dt></dl></dd><dt><span class="section"><a href="#Support.Software">3.2. Software Requiring Specific Contracts</a></span></dt><dt><span class="section"><a href="#Support.TechPreviews">3.3. Technology Previews</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-313188">3.3.1. Limit the Linux Kernel's page cache</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#id1165569">4. Miscellaneous</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-312295">4.1. Disable cpuplugd by default</a></span></dt><dt><span class="section"><a href="#fate-311919">4.2. Extend and improve zFCP trace utilities</a></span></dt><dt><span class="section"><a href="#fate-311912">4.3. IPv6 support for qetharp tool</a></span></dt><dt><span class="section"><a href="#fate-311894">4.4. Safely start getty through init</a></span></dt></dl></dd><dt><span class="chapter"><a href="#Installation">5. Installation</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-311333">5.1. Map network interface names to the names written on the chassis (biosdevname)</a></span></dt><dt><span class="section"><a href="#fate-310590">5.2. Amazon EC2 Availability</a></span></dt><dt><span class="section"><a href="#Installation.Deployment">5.3. Deployment</a></span></dt><dt><span class="section"><a href="#Installation.CJK">5.4. CJK Languages Support in Text-mode Installation</a></span></dt><dt><span class="section"><a href="#Installation.BootGrub2TB">5.5. Booting from Harddisks larger than 2 TiB in Non-UEFI Mode</a></span></dt><dt><span class="section"><a href="#Installation.PerstDevNames">5.6. Installation Using Persistent Device Names</a></span></dt><dt><span class="section"><a href="#Installation.qla">5.7. Using qla3xxx and qla4xxx Drivers at the Same Time</a></span></dt><dt><span class="section"><a href="#Installation.iSCSI">5.8. Using iSCSI Disks when Installing</a></span></dt><dt><span class="section"><a href="#Installation.EDD">5.9. Using EDD Information for Storage Device Identification</a></span></dt><dt><span class="section"><a href="#Installation.autoyast">5.10. Automatic Installation with AutoYaST in an LPAR (System z)</a></span></dt><dt><span class="section"><a href="#Installation.Systemz">5.11. Adding DASD or zFCP Disks During Installation (System z)</a></span></dt><dt><span class="section"><a href="#Installation.Network">5.12. Network Installation via eHEA on POWER</a></span></dt><dt><span class="section"><a href="#id1166448">5.13. For More Information</a></span></dt></dl></dd><dt><span class="chapter"><a href="#Features">6. Features and Versions</a></span></dt><dd><dl><dt><span class="section"><a href="#Features.Kernel">6.1. Linux Kernel and Toolchain</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-311931">6.1.1. Transparent Huge Pages (THP) Support</a></span></dt><dt><span class="section"><a href="#fate-311682">6.1.2. CFS Bandwidth Control (aka CPU Hard Limits)</a></span></dt></dl></dd><dt><span class="section"><a href="#Features.Server">6.2. Server</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-312717">6.2.1. Support for Tomcat Servlet Container</a></span></dt><dt><span class="section"><a href="#fate-312667">6.2.2. HPLIP Version Upgrade</a></span></dt><dt><span class="section"><a href="#fate-311973">6.2.3. Virtual Hosting: Supporting Multiple SSL Based Domains on One IP Address through Server Name Indication (SNI)</a></span></dt></dl></dd><dt><span class="section"><a href="#Features.Desktop">6.3. Desktop</a></span></dt><dt><span class="section"><a href="#Features.Security">6.4. Security</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-312696">6.4.1. Stricter SSL Certificate Checks for LDAP Clients</a></span></dt><dt><span class="section"><a href="#fate-312654">6.4.2. Managing Access Control Lists over NFSv4</a></span></dt><dt><span class="section"><a href="#fate-310820">6.4.3. Added System Security Services Daemon (sssd) for LDAP/Kerberos Authentication</a></span></dt><dt><span class="section"><a href="#fate-310517">6.4.4. Activating DKIM Support</a></span></dt><dt><span class="section"><a href="#fate-308239">6.4.5. openSSH with Cryptographic Hardware Acceleration</a></span></dt><dt><span class="section"><a href="#id1166863">6.4.6. PAM Configuration</a></span></dt><dt><span class="section"><a href="#id1166624">6.4.7. SELinux Enablement</a></span></dt><dt><span class="section"><a href="#id1166850">6.4.8. Enablement for TPM/Trusted Computing</a></span></dt><dt><span class="section"><a href="#id1166579">6.4.9. Linux File System Capabilities</a></span></dt></dl></dd><dt><span class="section"><a href="#Features.Network">6.5. Network</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-311802">6.5.1. YaST GUI tool available to configure FCoE capable network interfaces</a></span></dt><dt><span class="section"><a href="#fate-311333-1">6.5.2. Map network interface names to the names written on the chassis (biosdevname)</a></span></dt></dl></dd><dt><span class="section"><a href="#Features.ResourceManagement">6.6. Resource Management</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-312040">6.6.1. OS level virtualization: Linux Container (LXC)</a></span></dt></dl></dd><dt><span class="section"><a href="#Features.SystemManagement">6.7. Systems Management</a></span></dt><dt><span class="section"><a href="#Features.Other">6.8. Other</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-311718">6.8.1. Enhanced yast to support SCSI tape devices</a></span></dt></dl></dd><dt><span class="section"><a href="#Features.SystemZ">6.9. System z</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-311856">6.9.1. Exploitation of new z196 / z114 processor instructions</a></span></dt><dt><span class="section"><a href="#fate-311848">6.9.2. FICON IPL and device discovery hardening</a></span></dt><dt><span class="section"><a href="#fate-311763">6.9.3. Userspace handle to wait for cio pending work</a></span></dt><dt><span class="section"><a href="#Features.SystemZ.Hardware">6.9.4. Hardware</a></span></dt><dt><span class="section"><a href="#Features.SystemZ.Virtualization">6.9.5. Virtualization</a></span></dt><dt><span class="section"><a href="#Features.SystemZ.Storage">6.9.6. Storage</a></span></dt><dt><span class="section"><a href="#Features.SystemZ.Network">6.9.7. Network</a></span></dt><dt><span class="section"><a href="#Features.SystemZ.Security">6.9.8. Security</a></span></dt><dt><span class="section"><a href="#Features.SystemZ.RAS">6.9.9. RAS</a></span></dt><dt><span class="section"><a href="#Features.SystemZ.Web20">6.9.10. Web 2.0 Open Source Stack in SUSE Linux Enterprise Software Development Kit</a></span></dt><dt><span class="section"><a href="#Features.SystemZ.sles10">6.9.11. Functionality implemented in SUSE Linux Enterprise Server 11 (and SUSE Linux Enterprise Server 10 Service Pack 2.)</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#Drivers">7. Driver Updates</a></span></dt><dd><dl><dt><span class="section"><a href="#Drivers.Network">7.1. Network Drivers</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-311957">7.1.1. ixgbe Driver Update to version 3.3.8</a></span></dt><dt><span class="section"><a href="#fate-311956">7.1.2. Added the ixgbevf Driver, Version 2.0.0</a></span></dt><dt><span class="section"><a href="#fate-311863">7.1.3. igb Driver Update to version 3.0.6</a></span></dt><dt><span class="section"><a href="#fate-311862">7.1.4. igbvf Driver Update to Version 1.0.8</a></span></dt><dt><span class="section"><a href="#fate-311825">7.1.5. e1000e Driver Update to version 1.3.16</a></span></dt><dt><span class="section"><a href="#fate-311470">7.1.6. IBM Power Chelsio T4 Adapter cxgb4i Driver</a></span></dt><dt><span class="section"><a href="#fate-311451">7.1.7. Brocade 10G PCIe Ethernet Adapters (bna)</a></span></dt></dl></dd><dt><span class="section"><a href="#Drivers.Storage">7.2. Storage Drivers</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-311812">7.2.1. Support for Intel RSTe3.0 (Intel Rapid Storage Technology)</a></span></dt><dt><span class="section"><a href="#fate-311808">7.2.2. Support for Intel SAS Controller Unit (SCU) driver "isci"</a></span></dt><dt><span class="section"><a href="#fate-311801">7.2.3. Major advances in supporting iSCSI and FCoE</a></span></dt><dt><span class="section"><a href="#fate-311466">7.2.4. Open-iSCSI supported added to the QLogic iSCSI qla4xxx driver</a></span></dt><dt><span class="section"><a href="#fate-311462">7.2.5. Broadcom FCoE and iSCSI Enhanced Support for SLE11SP2</a></span></dt><dt><span class="section"><a href="#fate-311450">7.2.6. Brocade FC/FCOE Adapters (bfa) Update Notes</a></span></dt></dl></dd><dt><span class="section"><a href="#Drivers.Other">7.3. Other Drivers</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-311827">7.3.1. Support for Universal Serial Bus Version 3.0 (USB 3.0)</a></span></dt><dt><span class="section"><a href="#fate-311819">7.3.2. Support Intel(R) HD Graphics 2000/3000 used in 2nd Generation Intel(R) Core&#8482; i7/i5/i3 processor family</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#OtherUpdates">8. Other Updates</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-312669">8.1. Upgrade to gawk 3.1.8</a></span></dt><dt><span class="section"><a href="#fate-310741">8.2. Update gdb to Version 7.3</a></span></dt></dl></dd><dt><span class="chapter"><a href="#SDK">9. Software Development Kit</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-311554">9.1. PowerPC64 GCC Large TOC Support</a></span></dt></dl></dd><dt><span class="chapter"><a href="#Update">10. Update-Related Notes</a></span></dt><dd><dl><dt><span class="section"><a href="#Update.General">10.1. General Notes</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-310481">10.1.1. Automated Upgrade Using AutoYaST</a></span></dt><dt><span class="section"><a href="#id1169165">10.1.2. Online Migration from SP1 to SP2 via "YaST waggon"</a></span></dt><dt><span class="section"><a href="#id1169175">10.1.3. Online Migration with Debuginfo Packages Not Supported</a></span></dt><dt><span class="section"><a href="#id1169186">10.1.4. Migrating to SLE 11 SP2</a></span></dt><dt><span class="section"><a href="#id1166397">10.1.5. Migration from SUSE Linux Enterprise Server 10 SP4 via Bootable Media</a></span></dt><dt><span class="section"><a href="#id1169308">10.1.6. Upgrading from SLES 10 SPx</a></span></dt><dt><span class="section"><a href="#id1169130">10.1.7. Upgrading to SLES 11 SP2 with Root File System on iSCSI</a></span></dt><dt><span class="section"><a href="#id1169495">10.1.8. 
      Kernel Split in Different Packages
    </a></span></dt><dt><span class="section"><a href="#id1169624">10.1.9. 
     Tickless Idle
   </a></span></dt><dt><span class="section"><a href="#id1169549">10.1.10. 
    Development Packages
   </a></span></dt><dt><span class="section"><a href="#id1169432">10.1.11. Displaying Manual Pages with the Same Name</a></span></dt><dt><span class="section"><a href="#id1169118">10.1.12. 
    YaST LDAP Server No Longer Uses
    /etc/openldap/slapd.conf
   </a></span></dt><dt><span class="section"><a href="#id1169652">10.1.13. 
    AppArmor
   </a></span></dt><dt><span class="section"><a href="#id1169655">10.1.14. 
    Updating with Alternative Boot Loader (Non-Linux) or Multiple Boot Loader Programs
   </a></span></dt><dt><span class="section"><a href="#id1169346">10.1.15. 
    Upgrading MySQL to SUSE Linux Enterprise Server 11
   </a></span></dt><dt><span class="section"><a href="#id1169802">10.1.16. 
    Fine-Tuning Firewall Settings
   </a></span></dt><dt><span class="section"><a href="#id1169812">10.1.17. Upgrading from SUSE Linux Enterprise Server 10 SP4 with the Xen Hypervisor May Have
   Incorrect Network Configuration</a></span></dt><dt><span class="section"><a href="#id1169844">10.1.18. LILO Configuration Via YaST or AutoYaST</a></span></dt></dl></dd><dt><span class="section"><a href="#Update.GAToSP2">10.2. Update from SUSE Linux Enterprise Server 11</a></span></dt><dd><dl><dt><span class="section"><a href="#id1168999">10.2.1. Changed Routing Behavior</a></span></dt><dt><span class="section"><a href="#id1169424">10.2.2. Kernel Devel Packages</a></span></dt></dl></dd><dt><span class="section"><a href="#Update.SP1ToSP2">10.3. Update from SUSE Linux Enterprise Server 11 SP 1</a></span></dt><dd><dl><dt><span class="section"><a href="#id1169367">10.3.1. Update from SUSE Linux Enterprise Server 11 SP 1</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#Deprecated">11. Deprecated Functionality</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-311895">11.1. Remove Support for Multi-Volume Tape Dumps</a></span></dt><dt><span class="section"><a href="#fate-313160">11.2. Moving Novfs Kernel Module</a></span></dt><dt><span class="section"><a href="#fate-313016">11.3. Support for portmap will end with SUSE Linux Enterprise 11 SP3</a></span></dt><dt><span class="section"><a href="#fate-312991">11.4. Replacing xpdf-tools</a></span></dt><dt><span class="section"><a href="#fate-312973">11.5. L3 Support for Openswan Is Scheduled to Expire</a></span></dt><dt><span class="section"><a href="#fate-312764">11.6. Support for IBM Java 1.4.2 Ending 2013</a></span></dt><dt><span class="section"><a href="#fate-312620">11.7. Intel Active Management (IAMT)</a></span></dt><dt><span class="section"><a href="#fate-311983">11.8. PHP 5.2 Is Deprecated</a></span></dt><dt><span class="section"><a href="#fate-311111">11.9. Read-only Support for the ext4 File System for Migration Purposes</a></span></dt></dl></dd><dt><span class="chapter"><a href="#InfraPackArch">12. Infrastructure, Package and Architecture Specific Information</a></span></dt><dd><dl><dt><span class="section"><a href="#InfraPackArch.SystemManagement">12.1. Systems Management</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-310064">12.1.1. xrdp</a></span></dt><dt><span class="section"><a href="#fate-305278">12.1.2. YaST AppArmor Configuration Module</a></span></dt><dt><span class="section"><a href="#id1170258">12.1.3. Modified Operation against Novell Customer Center</a></span></dt><dt><span class="section"><a href="#id1170292">12.1.4. Operation against Subscription Management Tool</a></span></dt><dt><span class="section"><a href="#id1170302">12.1.5. Minimal Pattern</a></span></dt><dt><span class="section"><a href="#id1170318">12.1.6. SPident</a></span></dt></dl></dd><dt><span class="section"><a href="#InfraPackArch.Performance">12.2. Performance Related Information</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-311769">12.2.1. AES-NI Instruction Set Extension Support in OpenSSL</a></span></dt><dt><span class="section"><a href="#id1170175">12.2.2. Linux Completely Fair Scheduler Affects Java Performance</a></span></dt><dt><span class="section"><a href="#id1170379">12.2.3. Tuning Performance of Simple Database Engines</a></span></dt></dl></dd><dt><span class="section"><a href="#InfraPackArch.Storage">12.3. Storage</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-311699">12.3.1. Host Protected Area</a></span></dt><dt><span class="section"><a href="#fate-310783">12.3.2. Allow Settable permission/ownership on mp devices from multipath.conf</a></span></dt><dt><span class="section"><a href="#id1170358">12.3.3. Multipathing - SCSI Hardware Handler</a></span></dt><dt><span class="section"><a href="#id1170506">12.3.4. Multipathing: Failed Paths Do Not Return after a Path Failure.</a></span></dt><dt><span class="section"><a href="#id1169946">12.3.5. Local Mounts of iSCSI Shares</a></span></dt></dl></dd><dt><span class="section"><a href="#InfraPackArch.HyperV">12.4. Hyper-V</a></span></dt><dd><dl><dt><span class="section"><a href="#id1170210">12.4.1. Change of Kernel Device Names in Hyper-V Guests</a></span></dt><dt><span class="section"><a href="#id1170727">12.4.2. Using the "Virtual Machine Snapshot" Feature</a></span></dt></dl></dd><dt><span class="section"><a href="#InfraPackArch.ArchIndependent">12.5. Architecture Independent Information</a></span></dt><dd><dl><dt><span class="section"><a href="#InfraPackArch.ArchIndependent.Package">12.5.1. Changes in Packaging and Delivery</a></span></dt><dt><span class="section"><a href="#InfraPackArch.ArchIndependent.Security">12.5.2. Security</a></span></dt><dt><span class="section"><a href="#InfraPackArch.ArchIndependent.Network">12.5.3. Networking</a></span></dt><dt><span class="section"><a href="#InfraPackArch.ArchIndependent.Cross">12.5.4. Cross Architecture Information</a></span></dt></dl></dd><dt><span class="section"><a href="#InfraPackArch.x86_64_x86">12.6. AMD64/Intel64 64-Bit (x86_64) and Intel/AMD 32-Bit (x86) Specific Information</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-312141">12.6.1. Support of new Intel processors</a></span></dt><dt><span class="section"><a href="#fate-312140">12.6.2. Generic support for the PCI Express Gen3</a></span></dt><dt><span class="section"><a href="#fate-312084">12.6.3. Support for new Intel(R) Platforms</a></span></dt><dt><span class="section"><a href="#fate-311820">12.6.4. Support for Intel(R) Trusted Execution Technology (TXT)</a></span></dt><dt><span class="section"><a href="#InfraPackArch.x86_64_x86.System">12.6.5. System and Vendor Specific Information</a></span></dt><dt><span class="section"><a href="#InfraPackArch.x86_64_x86.Virtualization">12.6.6. Virtualization</a></span></dt><dt><span class="section"><a href="#InfraPackArch.x86_64_x86.RAS">12.6.7. RAS</a></span></dt></dl></dd><dt><span class="section"><a href="#InfraPackArch.IA64">12.7. Intel Itanium (ia64) Specific Information</a></span></dt><dt><span class="section"><a href="#InfraPackArch.Power">12.8. POWER (ppc64) Specific Information</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-312950">12.8.1. Suspend and Resume Support</a></span></dt><dt><span class="section"><a href="#fate-311687">12.8.2. Capture Oops and Panic Reports to NVRAM</a></span></dt><dt><span class="section"><a href="#fate-311672">12.8.3. Page Hinting for Active Memory Deduplication</a></span></dt><dt><span class="section"><a href="#fate-311669">12.8.4. IBM Power Virtual Fibre Channel Driver Update</a></span></dt><dt><span class="section"><a href="#fate-311656">12.8.5. ITrace Package Removed</a></span></dt><dt><span class="section"><a href="#fate-311650">12.8.6. IBM Power Virtual Ethernet Driver Update</a></span></dt><dt><span class="section"><a href="#fate-311630">12.8.7. IBM Power Shared Storage Pools</a></span></dt></dl></dd><dt><span class="section"><a href="#InfraPackArch.SystemZ">12.9. System z (s390x) Specific Information</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-311678">12.9.1. Libdfp updated to version 1.0.7</a></span></dt><dt><span class="section"><a href="#fate-309479">12.9.2. Suspend to Disk for System z</a></span></dt><dt><span class="section"><a href="#InfraPackArch.SystemZ.Hardware">12.9.3. Hardware</a></span></dt><dt><span class="section"><a href="#InfraPackArch.SystemZ.Virtualization">12.9.4. Virtualization</a></span></dt><dt><span class="section"><a href="#InfraPackArch.SystemZ.Storage">12.9.5. Storage</a></span></dt><dt><span class="section"><a href="#InfraPackArch.SystemZ.Network">12.9.6. Network</a></span></dt><dt><span class="section"><a href="#InfraPackArch.SystemZ.Security">12.9.7. Security</a></span></dt><dt><span class="section"><a href="#InfraPackArch.SystemZ.RAS">12.9.8. RAS</a></span></dt><dt><span class="section"><a href="#InfraPackArch.SystemZ.Performance">12.9.9. Performance</a></span></dt><dt><span class="section"><a href="#InfraPackArch.SystemZ.Misc">12.9.10. Miscellaneous</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#Resolved">13. Resolved Issues</a></span></dt><dt><span class="chapter"><a href="#TechInfo">14. Technical Information</a></span></dt><dd><dl><dt><span class="section"><a href="#TechInfo.Kernel">14.1. Kernel Limits</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-313187">14.1.1. Howto Run Applications that Do Not Recognize Linux Kernel 3.0 as Valid and Require Kernel 2.6 Instead</a></span></dt></dl></dd><dt><span class="section"><a href="#TechInfo.KVM">14.2. KVM Limits</a></span></dt><dt><span class="section"><a href="#TechInfo.Xen">14.3. Xen Limits</a></span></dt><dt><span class="section"><a href="#TechInfo.Filesystems">14.4. File Systems</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-306585">14.4.1. Support for the btrfs File System</a></span></dt></dl></dd><dt><span class="section"><a href="#TechInfo.KernelModules">14.5. Kernel Modules</a></span></dt><dt><span class="section"><a href="#TechInfo.IPv6">14.6. IPv6 Implementation and Compliance</a></span></dt><dt><span class="section"><a href="#TechInfo.Other">14.7. Other Technical Information</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-312605">14.7.1. Storing Log Files on the tmpfs File System Is Unsupported</a></span></dt><dt><span class="section"><a href="#id1175798">14.7.2. libica 2.0.2 is available in SLES 11 SP2 for s390x customers</a></span></dt><dt><span class="section"><a href="#id1175275">14.7.3. Yast support for layer 2 devices</a></span></dt><dt><span class="section"><a href="#id1173123">14.7.4. Changes to Network Setup</a></span></dt><dt><span class="section"><a href="#id1173872">14.7.5. Memory cgroups</a></span></dt><dt><span class="section"><a href="#id1175859">14.7.6. MCELog</a></span></dt><dt><span class="section"><a href="#id1175878">14.7.7. Locale Settings in ~/.i18n</a></span></dt><dt><span class="section"><a href="#id1174674">14.7.8. Configuration of kdump</a></span></dt><dt><span class="section"><a href="#id1175520">14.7.9. Configuring Authentication for kdump through YaST with ssh/scp as
    Target</a></span></dt><dt><span class="section"><a href="#id1175968">14.7.10. JPackage Standard for Java Packages</a></span></dt><dt><span class="section"><a href="#id1175677">14.7.11. Pulseaudio</a></span></dt><dt><span class="section"><a href="#id1175933">14.7.12. Stopping Cron Status Messages</a></span></dt></dl></dd></dl></dd><dt><span class="chapter"><a href="#Documentation">15. Documentation and Other Information</a></span></dt><dd><dl><dt><span class="section"><a href="#fate-310907">15.1. AutoYaST Documentation</a></span></dt></dl></dd><dt><span class="chapter"><a href="#Legal">16. Legal Notices</a></span></dt></dl></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="rnotes-purpose"></a>Chapter 1. SUSE Linux Enterprise Server</h2></div></div></div><p>
	  SUSE Linux Enterprise Server is a highly reliable, scalable, and secure server operating system, built to power mission-critical workloads in both physical and virtual environments.   It is an affordable, interoperable, and manageable open source foundation.  With it, enterprises can cost-effectively deliver core business services, enable secure networks, and simplify the management of their heterogeneous IT infrastructure, maximizing efficiency and value.
  </p><p>
	  The only enterprise Linux recommended by Microsoft and SAP, SUSE Linux Enterprise Server is optimized to deliver high-performance mission-critical services, as well as edge of network, and web infrastructure workloads.
  </p><p>
	  Designed for interoperability, 
	  SUSE Linux Enterprise Server lives in classical Unix as well as Windows environments,
	  supports open standard CIM interfaces for systems management,
	  and has been certified for IPv6 compatibility,
  </p><p>
	  This modular, general purpose operating system runs on five processor architectures and is available with optional extensions that provide advanced capabilities fortasks such as real time computing and high availability clustering.
  </p><p>
	  SUSE Linux Enterprise Server is optimized to run as a high performance guest on leading hypervisors and supports an unlimited number of virtual machines per physical system with a single subscription, making it the perfect guest operating system for virtual computing.
  </p><p>
	  SUSE Linux Enterprise Server is backed by award-winning support from SUSE, an established technology leader with a proven history of delivering enterprise-quality support services.
  </p><p>With the release of SUSE Linux Enterprise Server 11 Service Pack 2 the former SUSE Linux Enterprise Server 11 Service Pack 1
enters the 6 months migration window, during which time
SUSE will continue to provide security updates and full support to maintain its
customers' operations safe. At the end of the
six-month parallel support period, on 2012-08-31, support for SUSE Linux Enterprise Server 11 Service Pack 1
will be discontinued. Long Term Service Pack Support (LTSS) for SUSE Linux Enterprise Server 11 Service Pack 1
is available as a separate option. </p></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="mustread"></a>Chapter 2. Read Me First</h2></div></div></div><p>
   Users upgrading from a previous SUSE Linux Enterprise Server release are recommended to
   take a look at the following topics:
  </p><div class="itemizedlist"><ul type="disc"><li><p>
     <a class="xref" href="#Support" title="Chapter 3. Support Statement for SUSE Linux Enterprise Server">Chapter 3, <i>Support Statement for SUSE Linux Enterprise Server</i></a>
    </p></li><li><p>
     <a class="xref" href="#Update" title="Chapter 10. Update-Related Notes">Chapter 10, <i>Update-Related Notes</i></a>
    </p></li><li><p>
     <a class="xref" href="#TechInfo" title="Chapter 14. Technical Information">Chapter 14, <i>Technical Information</i></a>
    </p></li></ul></div><p>
   These Release Notes are identical across all architectures, and the most
   recent version is always available online at <a class="ulink" href="http://www.suse.com/releasenotes/" target="_top">http://www.suse.com/releasenotes/</a>.
  </p></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="Support"></a>Chapter 3. Support Statement for SUSE Linux Enterprise Server</h2></div></div></div><p>
   To receive support, customers need an appropriate subscription with
   SUSE; for more information, see <a class="ulink" href="http://www.suse.com/products/server/services-and-support/" target="_top">http://www.suse.com/products/server/services-and-support/</a>.
  </p><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Support.General"></a>3.1. General Support Statement</h2></div></div></div><p>
  The following definitions apply:
 </p><div class="itemizedlist"><ul type="disc"><li><p>
    L1: Installation and problem determination, which means technical
    support designed to provide compatibility information, installation
    configuration assistance, usage support, on-going maintenance and
    basic troubleshooting. Level 1 Support is not intended to correct
    product defect errors.
   </p></li><li><p>
    L2: Reproduction and isolation of problem, which means technical
    support designed to duplicate customer problems, isolate problem
    area and potential issues, and provide resolution for problems not
    resolved by Level 1 Support.
   </p></li><li><p>
    L3: Code debugging and problem resolution, which means technical
    support designed to resolve complex problems by engaging engineering
    in patch provision, and resolution of product defects which have
    been identified by Level 2 Support.
   </p></li></ul></div><p>
  For contracted customers and partners, SUSE Linux Enterprise Server 11 will be delivered
  with L3 support for all packages, except the following:
 </p><div class="itemizedlist"><ul type="disc"><li><p>
    technology previews;
   </p></li><li><p>
    sounds, graphics, fonts and artwork;
   </p></li><li><p>
    packages, which require an additional customer contract;
   </p></li><li><p>
    packages on the Software Development Kit (SDK).
   </p></li></ul></div><p>
  SUSE will only support the usage of original (e.g., unchanged or
  un-recompiled) packages.
 </p><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1165424"></a>3.1.1. Tomcat6 and Related Packages</h3></div></div></div><p>
   Tomcat6 and related packages are fully supported on the Intel/AMD x86
   (32bit), AMD64/Intel64, IBM POWER, and IBM System z architectures.
  </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1165431"></a>3.1.2. SELinux</h3></div></div></div><p>
	The SELinux subsystem is supported.
	Arbitrary SELinux policies running on SLES are not supported, though.
	Customers and Partners who have an interest in using
	SELinux in their solutions, are encouraged to contact SUSE to
	evaluate the level of support that is needed, and how support
	and services for the specific SELinux policies will be granted.
  </p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Support.Software"></a>3.2. Software Requiring Specific Contracts</h2></div></div></div><p>
 The following packages require additional support contracts to be
 obtained by the customer in order to receive full support:
</p><div class="itemizedlist"><ul type="disc"><li><p>
     BEA Java (Itanium only)
   </p></li><li><p>
     MySQL Database
   </p></li><li><p>
		  PostgreSQL Database
	  </p></li><li><p>
		  WebSphere CE Application Server
	  </p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Support.TechPreviews"></a>3.3. Technology Previews</h2></div></div></div><p>
  Technology previews are packages, stacks, or features delivered by
  SUSE. These features are not supported. They may be functionally
  incomplete, unstable or in other ways not suitable for production use.
  They are mainly included for customer convenience and give customers a
  chance to test new technologies within an enterprise
  environment.
 </p><p>
  Whether a technical preview will be moved to a fully supported package
  later, depends on customer and market feedback. A technical preview
  does not automatically result in support at a later point in time.
  Technical previews could be dropped at any time and SUSE is not
  committed to provide a technical preview later in the product
  cycle.
 </p><p>
  Please, give your SUSE representative feedback, including your
  experience and use case. Alternatively, use the Novell Requirements Portal
  at <a class="ulink" href="http://www.novell.com/rms" target="_top">http://www.novell.com/rms</a>.
  </p><div class="itemizedlist"><ul type="disc"><li><p>Hot-Add Memory</p><p>
     Hot-add memory is currently only supported on the following hardware:
   </p><div class="itemizedlist"><ul type="circle"><li><p>IBM eServer xSeries x260, single node x460, x3800, x3850, single node x3950,</p></li><li><p>certified systems based on recent Intel Xeon Architecture,</p></li><li><p>certified systems based on recent Intel IPF Architecture,</p></li><li><p>all IBM servers and blades with POWER5, POWER6, or POWER7 processors and recent firmware.</p></li></ul></div><p>
    If your specific machine is not listed, please call SUSE support to
    confirm whether or not your machine has been successfully
    tested. Also, regularly check our maintenance update information,
    which will explicitly mention the general availability of this
    feature.
   </p><p>
     Restriction on using IBM eHCA InfiniBand adapters in conjunction with
     hot-add memory on IBM System p:
   </p><p>
     The current eHCA Device Driver will prevent dynamic memory operations on
     a partition as long as the driver is loaded. If the driver is unloaded
     prior to the operation and then loaded again afterwards, adapter
     initialization may fail. A Partition Shutdown / Activate sequence on the
     HMC may be needed to recover from this situation.
   </p></li><li><p>Internet Storage Naming Service (iSNS)</p><p>The Internet Storage Naming Service (iSNS) package is by design
   suitable for secure internal networks only. SUSE will continue to
   work with the community on improving security.</p></li><li><p>Read-Only Root File System</p><p>It is possible to run SUSE Linux Enterprise Server 11 on a shared read-only root
   file system.  A read-only root setup consists of the read-only root
   file system, a scratch and a state file system.  The
   <code class="filename">/etc/rwtab</code> file defines which files and
   directories on the read-only root file system are replaced by which
   files on the state and scratch file systems for each system
   instance.</p><p>The <code class="filename">readonlyroot</code> kernel command line
   option enables read-only root mode; the <code class="filename">state=</code>
   and <code class="filename">scratch=</code> kernel command line options
   determine the devices on which the state and scratch file systems are
   located.</p><p>In order to set up a system with a read-only root file system,
   set up a scratch file system, set up a file system to use for storing
   persistent per-instance state,
 adjust
   <code class="filename">/etc/rwtab</code> as needed, add the appropriate kernel
   command line options to your boot loader configuration, replace
   <code class="filename">/etc/mtab</code> with a symlink to
   <code class="filename">/proc/mounts</code> as described below, and (re)boot
   the system.</p><p>To replace <code class="filename">/etc/mtab</code> with the appropriate
   symlinks, call:</p><pre class="screen">ln -sf /proc/mounts /etc/mtab</pre><p>See the rwtab(5) manual page for further details and <a class="ulink" href="http://www.redbooks.ibm.com/abstracts/redp4322.html" target="_top">http://www.redbooks.ibm.com/abstracts/redp4322.html</a> for
   limitations on System z.</p></li></ul></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-313188"></a>3.3.1. Limit the Linux Kernel's page cache</h3></div></div></div><p><span class="emphasis"><em>The Linux Kernel swaps out rarely accessed memory pages in order to use freed memory pages as cache to speed up file-system operations, for instance during backup operations.</em></span></p><p><span class="emphasis"><em>Some Enterprise applications, such as SAP solutions, use large amounts of memory for accelerated access to business data. Parts of this memory are very seldom accessed. When a user request then needs to access paged out memory, the response time is poor - being even worse, when a SAP solution running on Java incurs a Java garbage collection. The system starts heavy page-in (disc I/O) activity and incurs poor response time for an extended period of time.</em></span></p><p>The pagecache_limit feature is a technology preview in SUSE Linux Enterprise Server 11 SP1 and SP2, and only supported for SUSE Linux Enterprise Server for SAP Applications 11 SP1 and later.</p><p>For SUSE Linux Enterprise Server 12 we expect an upstream solution based on Control Groups.</p></div></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="id1165569"></a>Chapter 4. Miscellaneous</h2></div></div></div><p></p><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-312295"></a>4.1. Disable cpuplugd by default</h2></div></div></div><p>cpuplugd is supposed to optimize the processor utilization if the workload does not need the full capacity. The latest Linux scheduler is optimized to achieve the same result without the cost intensive operation of CPU plug and unplug. If the use case is not fully exploited, it is advisable to disable the cpuplugd by default.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-311919"></a>4.2. Extend and improve zFCP trace utilities</h2></div></div></div><p>I/O statistics gathering are essential tools for performance analysis and problem determination. Various parts of the infrastructure have been improved to allow better serviceability by introducing enhanced trace support for FCP.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-311912"></a>4.3. IPv6 support for qetharp tool</h2></div></div></div><p>This feature adds IPv6 support to the qetharp tool for inspection and modification of the ARP cache of OSA cards or HiperSockets (real and virtual) operated in layer 3 mode.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-311894"></a>4.4. Safely start getty through init</h2></div></div></div><p>This feature integrates a new tool 'ttyrun', which safely starts getty programs and prevents re-spawns through the init program if a terminal is not available. This is very useful when integrated in inittab. Depending on your setup, Linux on System z might or might not provide a particular terminal or console.</p></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="Installation"></a>Chapter 5. Installation</h2></div></div></div><p></p><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-311333"></a>5.1. Map network interface names to the names written on the chassis (biosdevname)</h2></div></div></div><p><span class="emphasis"><em>This feature addresses the issue of eth0 does not map to em1 as labeled on server chassis when a server has multiple network adapters.</em></span></p><p>This issue is solved for Dell hardware, which has the corresponding BIOS support, by renaming onboard network interfaces to em[1234], which maps to Embedded NIC[1234] as labeled on server chassis.  (em stands for ethernet-on-motherboard.)</p><p>The renaming will be done by using the biosdevname utility.</p><p>biosdevname is automatically installed and used if YaST2 detects hardware suitable to be used with biosdevname. biosdevname can be disabled during installation by using "biosdevname=0" on the kernel commandline. The usage of biosdevname can be enforced on every hardware with "biosdevname=1". If the BIOS has no support, no network interface names are renamed.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-310590"></a>5.2. Amazon EC2 Availability</h2></div></div></div><p>SUSE Linux Enterprise Server 11 SP2 is available immediately for use on Amazon Web Services EC2. For more information about Amazon EC2 Running SUSE Linux Enterprise Server, please visit http://aws.amazon.com/suse</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Installation.Deployment"></a>5.3. Deployment</h2></div></div></div><p>SUSE Linux Enterprise Server can be deployed in three ways:</p><div class="itemizedlist"><ul type="disc"><li><p>Physical Machine,</p></li><li><p>Virtual Host,</p></li><li><p>Virtual Machine in paravirtualized environments.</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Installation.CJK"></a>5.4. CJK Languages Support in Text-mode Installation</h2></div></div></div><p>
   CJK (Chinese, Japanese, and Korean) languages do not work properly
   during text-mode installation if the framebuffer is not used
   (Text Mode selected in boot loader).
  </p><p>There are three alternatives to resolve this issue:</p><div class="orderedlist"><ol type="1"><li><p>
     Use English or some other non-CJK language for installation then switch to the CJK language later on a running system using
<span class="guimenu">YaST</span>+<span class="guimenu">System</span>+<span class="guimenu">Language</span>.
    </p></li><li><p>Use your CJK language during installation, but do not choose
    <span class="guimenu">Text Mode</span> in the boot loader using <span class="guimenu">F3
    Video Mode</span>. Select one of the other VGA modes
    instead. Select the CJK language of your choice using <span class="guimenu">F2
    Language</span>, add <span class="command"><strong>textmode=1</strong></span> to the boot
    loader command-line and start the installation.
    </p></li><li><p>Use graphical installation (or install remotely via SSH or VNC).
    </p></li></ol></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Installation.BootGrub2TB"></a>5.5. Booting from Harddisks larger than 2 TiB in Non-UEFI Mode</h2></div></div></div><p>
   Booting from harddisks larger than 2 TiB in non-UEFI mode (but with
   GPT partition table) fails.
  </p><p>
   To successfully use harddisks larger than 2 TiB in non-UEFI mode, but
   with GPT partition table (i.e., grub bootloader), consider one of the
   following options:
  </p><div class="itemizedlist"><ul type="disc"><li><p>
     Use a 4k sector harddisk in 4k mode (in this case, the 2 TiB limit will
     become a 16 TiB limit).
    </p></li><li><p>
     Use a separate <code class="filename">/boot</code> partition. This partition
     must be one of the first 3 partitions and end below the 2 TiB
     limit.
    </p></li><li><p>
     Switch from legacy mode to UEFI mode, if this is an option for you.
    </p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Installation.PerstDevNames"></a>5.6. Installation Using Persistent Device Names</h2></div></div></div><p>
     The installer uses persistent device names by default. If you plan
     to add storage devices to your system after the installation, we
     strongly recommend you use persistent device names for all storage
     devices.
    </p><p>
     To switch to persistent device names on a system that has already
     been installed, start the YaST2 partitioner. For each partition,
     select <span class="guimenu">Edit</span> and go to the <span class="guimenu">Fstab
     Options</span> dialog. Any mount option except <span class="guimenu">Device
     name</span> provides you persistent device names. In addition,
     rerun the Boot Loader module in YaST and select <span class="guimenu">Propose
     New Config</span> to switch the boot loader to using the
     persistent device name, or manually adjust all boot loader
     sections. Then select <span class="guimenu">Finish</span> to write the new
     proposed configuration to disk.  Alternatively, edit
     <code class="filename">/boot/grub/menu.lst</code> and
     <code class="filename">/boot/grub/device.map</code> according to your needs.
    </p><p>
     This needs to be done before adding new storage devices.
    </p><p>
      For further information, see the &#8220;<span class="quote">Storage Administration
      Guide</span>&#8221; about "Device Name Persistence".
    </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Installation.qla"></a>5.7. Using qla3xxx and qla4xxx Drivers at the Same Time</h2></div></div></div><p>
   QLogic iSCSI Expansion Card for IBM BladeCenter provides both
   Ethernet and iSCSI functions. Some parts on the card are shared by
   both functions. The current qla3xxx (Ethernet) and qla4xxx (iSCSI)
   drivers support Ethernet and iSCSI function individually. They do not
   support using both functions at the same time. Simultaneous use of
   both Ethernet and iSCSI functions may cause the device to hang with
   possible data loss and file system corruptions on iSCSI devices or
   network disruptions on Ethernet.
  </p><p>
   Boot the installation with brokenmodules=qla3xxx or
   brokenmodules=qla4xxx to prevent one of the drivers from loading.
  </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Installation.iSCSI"></a>5.8. Using iSCSI Disks when Installing</h2></div></div></div><p>
   To use iSCSI disks during installation, add the following parameter
   to the boot option line: <code class="literal">withiscsi=1</code>.
  </p><p>
   During installation, an additional screen provides the option to
   attach iSCSI disks to the system and use them in the installation
   process.
  </p><p>
   Booting from an iSCSI server on i386, x86_64 and ppc64 is supported
   if iSCSI-enabled firmware is used.
  </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Installation.EDD"></a>5.9. Using EDD Information for Storage Device Identification</h2></div></div></div><p>
   EDD information (in
   <code class="filename">/sys/firmware/edd/&lt;device&gt;</code>) is used by
   default to identify your storage devices.
  </p><p>EDD Requirements:</p><div class="itemizedlist"><ul type="disc"><li><p>
     BIOS provides full EDD information (found in
     <code class="filename">/sys/firmware/edd/&lt;device&gt;</code>)
    </p></li><li><p>
     Disks are signed with a unique MBR signature (found in
     <code class="filename">/sys/firmware/edd/&lt;device&gt;/mbr_signature</code>).
    </p></li></ul></div><p>
   Add <code class="literal">edd=off</code> to the kernel parameters to disable
   EDD.
  </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Installation.autoyast"></a>5.10. Automatic Installation with AutoYaST in an LPAR (System z)</h2></div></div></div><p>
   For automatic installation with AutoYaST in an LPAR, the
   <code class="filename">parmfile</code> used for such an installation must have
   blank characters at the beginning and at the end of each line (the
   first line does not need to start with a blank). The number of
   characters in one line should not exceed 80.
  </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Installation.Systemz"></a>5.11. Adding DASD or zFCP Disks During Installation (System z)</h2></div></div></div><p>
   Adding of DASD or zFCP disks is not only possible during the
   installation workflow, but also when the installation proposal is
   shown. To add disks at this stage, please click on the
   <span class="guimenu">Expert</span> tab and scroll down. There the DASD and/or
   zFCP entry is shown. These added disks are not displayed in the
   partitioner automatically. To make the disks visible in the
   partitioner, you have to click on <span class="guimenu">Expert</span> and
   select <span class="guimenu">reread partition table</span>. This may reset any
   previously entered information.
  </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Installation.Network"></a>5.12. Network Installation via eHEA on POWER</h2></div></div></div><p>
   If you want to carry out a network installation via the IBM eHEA
   Ethernet Adapter on POWER systems, no huge (16GB) pages may be
   assigned to the partition during installation.
  </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="id1166448"></a>5.13. For More Information</h2></div></div></div><p>For more information, see <a class="xref" href="#InfraPackArch" title="Chapter 12. Infrastructure, Package and Architecture Specific Information">Chapter 12, <i>Infrastructure, Package and Architecture Specific Information</i></a>.</p></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="Features"></a>Chapter 6. Features and Versions</h2></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Features.Kernel"></a>6.1. Linux Kernel and Toolchain</h2></div></div></div><p>
</p><div class="itemizedlist"><ul type="disc"><li><p>
		  GCC 4.3.4
	  </p></li><li><p>
		  glibc 2.11.1
	  </p></li><li><p>
    Linux kernel 3.0.10
   </p></li><li><p>
		  perl 5.10
	  </p></li><li><p>
		  php 5.2.6
	  </p></li><li><p>
		  python 2.6.0
	  </p></li><li><p>
		  ruby 1.8.7
	  </p></li></ul></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311931"></a>6.1.1. Transparent Huge Pages (THP) Support</h3></div></div></div><p><span class="emphasis"><em>On systems with large memory, frequent access to the  Translation Lookaside Buffer  (TLB) may slow down the system significantly.</em></span></p><p>Transparent huge pages thus are of most use on systems with very large (128GB or more)  memory, and help to drive performance. In SUSE Linux Enterprise, THP is enabled by default where it is expected to give a performance boost to a large number of workloads.</p><p>There are cases where THP may regress performance, particularly when under memory pressure due to pages being reclaimed in an effort to promote to huge pages. It is also possible that performance will suffer on CPUs with a limited number of huge page TLB entries for workloads that sparsely reference large amounts of memory. If necessary, THP can be disabled via the sysfs file "/sys/kernel/mm/transparent_hugepage/enabled", which accepts one of the values "always", "madvise", or "never".</p><p>To disable THP via sysfs and confirm it is disabled, do the following as root:</p><pre class="screen">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled
cat /sys/kernel/mm/transparent_hugepage/enabled 
always madvise [never]</pre></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311682"></a>6.1.2. CFS Bandwidth Control (aka CPU Hard Limits)</h3></div></div></div><div class="itemizedlist"><ul type="disc"><li><p><span class="emphasis"><em>Limiting the maximum CPU usage of a group or VM and ensuring that user is not provided more CPU resource than what he has paid for.</em></span></p></li><li><p><span class="emphasis"><em>Providing consistent and repeatable VM performance in a Cloud environment.</em></span></p></li></ul></div><p>CFS bandwidth control can be used to set a hard limit on the CPU usage of a group or VM. With this it becomes possible to limit a group's or VM's maximum CPU usage to say 0.5CPU or 2CPUs. The bandwidth is specified as quota/period where a group will not be allowed to consume more than 'quota' milliseconds worth of CPU time in every 'period' interval. If a group or VM's CPU usage exceeds the limit, it will be throttled until the time its quota gets refreshed.</p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Features.Server"></a>6.2. Server</h2></div></div></div><p>
	Note: in the following text version numbers do not necessarily give the 
	final patch- and security-status of an application, as SUSE may have added
	additional patches to the specific version of an application.

        

        
 </p><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-312717"></a>6.2.1. Support for Tomcat Servlet Container</h3></div></div></div><p><span class="emphasis"><em>In SUSE Linux Enterprise 11 Service Pack 1 and earlier releases, the Tomcat servlet container has been provided as part of the Software Development Kit. We learned that our customers  demand full runtime support for this infrastructure.</em></span></p><p>Starting with SUSE Linux Enterprise Server 11 Service Pack2, Tomcat6 and related packages are part of the Server product. Based on customer and partner feedback we fully support this on the architectures Intel/AMD x86 (32bit), AMD64/Intel64, IBM POWER, IBM System z.</p><p>The following packages are affected: tomcat6, tomcat6-servlet-2_5-api, tomcat6-webapps, tomcat6-docs-webapp, tomcat6-admin-webapps, tomcat6-lib, tomcat6-jsp-2_1-api, libtcnative-1-0, apache2-mod_jk, jakarta-taglibs-standard, jakarta-commons-collections, jakarta-commons-dbcp, jakarta-commons-pool, jakarta-commons-httpclient3, jakarta-commons-beanutils, jakarta-commons-codec, jakarta-commons-collections, jakarta-commons-collections-tomcat5, jakarta-commons-daemon, jakarta-commons-dbcp-tomcat5, jakarta-commons-digester, jakarta-commons-discovery, jakarta-commons-el, jakarta-commons-fileupload, jakarta-commons-io, jakarta-commons-lang, jakarta-commons-launcher, jakarta-commons-logging, jakarta-commons-modeler, jakarta-commons-pool-tomcat5, jakarta-commons-validator, tomcat6-javadoc, jakarta-taglibs-standard-javadoc, jakarta-commons-*-javadoc, tomcat_apparmor, ant, ant-junit, ant-trax, and mx4j.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-312667"></a>6.2.2. HPLIP Version Upgrade</h3></div></div></div><p><span class="emphasis"><em>With the changes in the printer market that have happened since SUSE Linux Enterprise 11 SP1 was released, it is highly probable that parts of HPLIP are outdated.</em></span></p><p>The version upgrade to HPLIP version 3.11.5 keeps SUSE Linux Enterprise 11 SP2 up-to-date regarding to HP printer and all-in-one devices.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311973"></a>6.2.3. Virtual Hosting: Supporting Multiple SSL Based Domains on One IP Address through Server Name Indication (SNI)</h3></div></div></div><p><span class="emphasis"><em>Having multiple domains residing in virtual hosts, only the first domain can be served for secure Web browsing. Other domains are prevented from using secure communications. Many servers in virtual hosting environments circumvent this by using a wrong certificate, which causes the browser to warn the user.</em></span></p><p>An extension to TLS called Server Name Indication (SNI) addresses this issue by sending the name of the virtual domain as part of the TLS negotiation.This enables the server to "switch" to the correct virtual domain early and present the browser with the certificate containing the correct CN. Apache version 2.2.12 has server support for SNI extension.</p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Features.Desktop"></a>6.3. Desktop</h2></div></div></div><p></p><div class="itemizedlist"><ul type="disc"><li><p>
     GNOME 2.28
   </p><p>
     GNOME was updated and uses PulseAudio for sound.
   </p></li><li><p>
     KDE 4.3.5
   </p><p>
     KDE was updated.
   </p></li><li><p>
     X.org 7.4
   </p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Features.Security"></a>6.4. Security</h2></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-312696"></a>6.4.1. Stricter SSL Certificate Checks for LDAP Clients</h3></div></div></div><p>With SP2 LDAP clients default to a stricter default setting for certificate
verification. For that to work correctly, the CA certificate used to sign
the LDAP server's certificate needs to be available on the client's
file system. The YaST LDAP client module was enhanced to provide a way to
download the CA certificate from a URL or to configure a file or directory
from which the LDAP client should load the CA certificate.</p><p> When updating from an SP1 system, this settings is not enabled automatically.
To enable it, start the YaST LDAP client configuration wizard and configure a
valid CA certificate to verify your LDAP server's certificate. Then
make sure that <code class="literal">/etc/openldap/ldap.conf</code> either contains no <code class="literal">TLS_REQCERT</code> setting or set it to  "demand" or "hard". </p><p>For details, see the
ldap.conf(5) man page.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-312654"></a>6.4.2. Managing Access Control Lists over NFSv4</h3></div></div></div><p><span class="emphasis"><em>There is no single standard for Access Control Lists (ACL) in Linux and Unix beyond the simple user/group/others-rwx flags. One option for finer control are so-called "Draft Posix ACLs", which were never formally standardized by Posix. Another is the NFSv4 ACLs, which were designed to be part of the NFSv4 network file system with the goal of making something that provided reasonable compatibility between Posix systems (like Linux) and WIN32 systems (like Microsoft Windows).</em></span></p><p><span class="emphasis"><em> It turned out that NFSv4 ACLs are not sufficient to correctly implement Draft Posix ACLs.  Thus no attempt has been made to map ACL accesses on an NFSv4 client (using e.g. </em></span><code class="literal"><span class="emphasis"><em>setfacl</em></span></code><span class="emphasis"><em> ). </em></span></p><p><span class="emphasis"><em> Therefore, when using NFSv4, Draft Posix ACLs cannot be used even in emulation.  NFSv4 ACLs need to be used directly; i.e., while </em></span><code class="literal"><span class="emphasis"><em>setfacl</em></span></code><span class="emphasis"><em> can work on NFSv3, it cannot work on NFSv4. </em></span></p><p>To allow NFSv4 ACLs to be used on an NFSv4 file system we provide the "nfs4-acl-tools" package, which contains:</p><div class="itemizedlist"><ul type="disc"><li><p>
            <code class="literal">nfs4_getfacl</code>
          </p></li><li><p>
            <code class="literal">nfs4_setfacl</code>
          </p></li><li><p>
            <code class="literal">nfs4_editfacl</code>
          </p></li></ul></div><p> These operate in a generally similar way to <code class="literal">getfacl</code> and <code class="literal">setfacl</code> for examining and modifying NFSv4 ACLs. </p><p>Note: This can only be effective if the file system on the NFS server  provides full support for NFSv4 ACLs.  Any limitation imposed by the server will affect programs running on the client in that some particular combinations of Access Control Entries (ACEs) may not be possible.</p><p>A future release of Linux may support "richacls", which are designed to provide access to NFSv4 ACLs in a way that is more integrated with other file sytems. If and when these become available, we will need to transition from using nfs4-acl-tools towards support tools coming with "richacls".</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-310820"></a>6.4.3. Added System Security Services Daemon (sssd) for LDAP/Kerberos Authentication</h3></div></div></div><p>The System Security Services Daemon (sssd) was added to SLE 11 SP2 to provide an alternative method to retrieve user and group information from LDAP directories and to perform authentication through LDAP or Kerberos. It is provided as an alternative to the nss_ldap and pam_ldap (or pam_krb5) Modules. Compared to those modules sssd offers some advantages:</p><div class="itemizedlist"><ul type="disc"><li><p>due to it's daemon based architecture possible symbol conflicts between different implementations of LDAP client libraries can be avoided</p></li><li><p>offline authentication is supported (disabled by default)</p></li><li><p>builtin support for Kerberos Authentication (no separate PAM module needed)</p></li></ul></div><p>With SLE 11 SP2 the YaST2 ldap-client module can be used to setup sssd for LDAP (and/or Kerberos) Authentication. The YaST to ldap-client module can also be used to switch from a nss_ldap/pam_ldap based setup to sssd and back.</p><p>Some additional notes:</p><div class="itemizedlist"><ul type="disc"><li><p>sssd requires a Transport Layer Encryption to be in place when using LDAP based authentication (e.g., LDAPS or StartTLS),</p></li><li><p>sssd does currently only support the passwd, shadow and group NSS databases</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-310517"></a>6.4.4. Activating DKIM Support</h3></div></div></div><p>After a new installation of SLES-11-SP2 this new feature is enabled when the mail system was configured with using amavis.</p><p> Updating from SLES-11-SP1 this feature must be enabled by editing <code class="literal">/etc/mail/spamassassin/v312.pre</code> .  The comment sign <code class="literal">#</code> must be removed from the last line: </p><p>before:</p><pre class="screen">#loadplugin Mail::SpamAssassin::Plugin::DKIM</pre><p>after:</p><pre class="screen">loadplugin Mail::SpamAssassin::Plugin::DKIM</pre></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-308239"></a>6.4.5. openSSH with Cryptographic Hardware Acceleration</h3></div></div></div><p>openSSH now makes use of cryptographic hardware acceleration. As a result, the transfer of large quantities of data through a ssh connection is considerably faster. As an additional benefit, the CPU of the system with cryptographic hardware will see a significant reduction in load.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1166863"></a>6.4.6. PAM Configuration</h3></div></div></div><p>
    The common PAM configuration files
    (<code class="filename">/etc/pam.d/common-*</code>) are now created and
    managed with <span class="command"><strong>pam-config</strong></span>.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1166624"></a>6.4.7. SELinux Enablement</h3></div></div></div><p>
    In addition to AppArmor, SELinux capabilities have been added to
    SUSE Linux Enterprise Server.  While these capabilities are not enabled by default,
    customers can run SELinux with SUSE Linux Enterprise Server if they choose to.
   </p><p>
    What does SELinux enablement mean?
   </p><div class="itemizedlist"><ul type="disc"><li><p>The kernel ships with SELinux support.</p></li><li><p>
      We will apply SELinux patches to all &#8220;common&#8221; userland packages.
     </p></li><li><p>
      The libraries required for SELinux (<code class="systemitem">libselinux, libsepol, libsemanage</code>,
      etc.) have been added to openSUSE and SUSE Linux Enterprise.
     </p></li><li><p>
      Quality Assurance is performed with SELinux
      disabled&#8212;to make sure that SELinux patches do not break the
      default delivery and the majority of packages.
     </p></li><li><p>
      The SELinux specific tools are shipped as part of the default
      distribution delivery. 
     </p></li><li><p>
	Arbitrary SELinux policies running on SLES are not supported, though,
	and we will not be shipping any SELinux policies in the
	distribution. Reference and minimal policies may be available
	from the repositories at some future point.
     </p></li><li><p>
	Customers and Partners who have an interest in using
	SELinux in their solutions, are encouraged to contact SUSE to
	evaluate the level of support that is needed, and how support
	and services for the specific SELinux policies will be granted.
     </p></li></ul></div><p>
    By enabling SELinux in our codebase, we add community code to offer
    customers the option to use SELinux without replacing significant
    parts of the distribution.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1166850"></a>6.4.8. Enablement for TPM/Trusted Computing</h3></div></div></div><p>
    SUSE Linux Enterprise Server 11 comes with support for Trusted Computing technology.  To
    enable your system's TPM chip, make sure that the "security chip"
    option in your BIOS is selected. TPM support is entirely passive,
    meaning that measurements are being performed, but no action is
    taken based on any TPM-related activity. TPM chips manufactured by
    Infineon, NSC and Atmel are supported, in addition to the virtual
    TPM device for Xen.
   </p><p>
      The corresponding kernel drivers are not loaded automatically. To
      do so, enter:
     </p><pre class="screen">find /lib/modules -type f -name "tpm*.ko"</pre><p>
      and load the kernel modules for your system manually or via
      <span class="command"><strong>MODULES_LOADED_ON_BOOT</strong></span> in
      <code class="filename">/etc/sysconfig/kernel</code>.
     </p><p>
      If your TPM chip with taken ownership is configured in Linux and
      available for use, you may read PCRs from
      <code class="filename">/sys/devices/*/*/pcrs</code>.
     </p><p>
      The <code class="systemitem">tpm-tools</code> package
      contains utilities to administer your TPM chip, and the
      <code class="systemitem">trousers</code> package
      provides <code class="systemitem">tcsd</code>&#8212;the daemon
      that allows userland programs to communicate with the TPM driver
      in the Linux kernel. <code class="systemitem">tcsd</code>
      can be enabled as a service for the runlevels of your choice.
     </p><p>
      To implement a trusted ("measured") boot path, use the package
      <code class="systemitem">trustedgrub</code> instead of
      the <code class="systemitem">grub</code> package as your
      bootloader. The trustedgrub bootloader does not display any
      graphical representation of a boot menu for informational reasons.
     </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1166579"></a>6.4.9. Linux File System Capabilities</h3></div></div></div><p>
      Our kernel is compiled with support for Linux File System Capabilities.
      This is disabled by default. The feature can be enabled by adding
      <code class="option">file_caps=1</code> as kernel boot option.
    </p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Features.Network"></a>6.5. Network</h2></div></div></div><p></p><div class="variablelist"><dl><dt><span class="term">
     IPv6 Improvements
   </span></dt><dd><p>
    
    

    SUSE Linux Enterprise Server has successfully completed the USGv6 test program designated
    by NIST that provides a proof of compliance to IPv6 specifications
    outlined in current industry standards for common network products.
   </p><p>
    Being IPv6 Consortium Member and Contributor Novell/SUSE have worked
    successfully with University of New Hampshire InterOperability
    Laboratory (UNH-IOL) to verify compliance to IPv6
    specifications. The UNH-IOL offers ISO/IEC 17025 accredited testing
    designed specifically for the USGv6 test program. The devices that
    have successfully completed the USGv6 testing at the UNH-IOL by
    March 2012 are SUSE Linux Enterprise Server 11 SP1.  Testing for
    subsequent releases of SUSE Linux Enterprise Server is in progress,
    and current and future results will be listed at
    <a class="ulink" href="http://www.iol.unh.edu/services/testing/ipv6/usgv6tested.php?company=105&amp;type=#eqplist" target="_top">http://www.iol.unh.edu/services/testing/ipv6/usgv6tested.php?company=105&amp;type=#eqplist</a>.
   </p><p>
    SUSE Linux Enterprise Server can be installed in an IPv6 environment and run IPv6
    applications. When installing via network, do not forget to boot
    with "<code class="literal">ipv6=1</code>" (accept v4 and v6) or
    "<code class="literal">ipv6only=1</code>" (only v6) on the kernel command
    line. For more information, see the Deployment Guide and also <a class="xref" href="#TechInfo.IPv6" title="14.6. IPv6 Implementation and Compliance">Section 14.6, &#8220;IPv6 Implementation and Compliance&#8221;</a>.
   </p></dd><dt><span class="term">
     10G Networking Capabilities
   </span></dt><dd><p></p></dd><dt><span class="term">
     OFED 1.5
   </span></dt><dd><p></p></dd><dt><span class="term">
    traceroute 1.2
   </span></dt><dd><p>
    Support for traceroute over TCP.
   </p></dd><dt><span class="term">
    FCoE
   </span></dt><dd><p>
    FCoE is an implementation of the Fibre Channel over Ethernet working
    draft. Fibre Channel over Ethernet is the encapsulation of Fibre
    Channel frames in Ethernet packets.  It allows users with a FCF
    (Fibre Channel over Ethernet Forwarder) to access their existing
    Fibre Channel storage using an Ethernet adapter. When leveraging
    DCB's PFC technology to provide a loss-less environment, FCoE can
    run SAN and LAN traffic over the same link.
   </p></dd><dt><span class="term">
    Data Center Bridging (DCB)
   </span></dt><dd><p>
    Data Center Bridging (DCB) is a collection of Ethernet enhancements
    designed to allow network traffic with differing requirements (e.g.,
    highly reliable, no drops vs. best effort vs. low latency) to
    operate and coexist on Ethernet. Current DCB features are:
   </p><div class="itemizedlist"><ul type="disc"><li><p>
      <span class="emphasis"><em>Enhanced Transmission Selection</em></span> (aka
      <span class="emphasis"><em>Priority Grouping</em></span>) to provide a framework for
      assigning bandwidth guarantees to traffic classes.
     </p></li><li><p>
      <span class="emphasis"><em>Priority-based Flow Control (PFC)</em></span> provides a
      flow control mechanism which can work independently for each
      802.1p priority.
     </p></li><li><p>
      <span class="emphasis"><em>Congestion Notification</em></span> provides a mechanism
      for end-to-end congestion control for protocols, which do not have
      built-in congestion management.
     </p></li></ul></div></dd></dl></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311802"></a>6.5.1. YaST GUI tool available to configure FCoE capable network interfaces</h3></div></div></div><p>The YaST module "FCoE Client Configuration" is a tool to configure FCoE capable network interfaces. During the installation workflow the FCoE configuration can be started on 'Disk Activation' screen. The FCoE interface can be configured and the connected disk will be available for installation.</p><p>The FCoE configuration should be automatically offered if the BIOS has activated FCoE. If not, add "withfcoe=1" to the kernel command line.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311333-1"></a>6.5.2. Map network interface names to the names written on the chassis (biosdevname)</h3></div></div></div><p><span class="emphasis"><em>This feature addresses the issue of eth0 does not map to em1 as labeled on server chassis when a server has multiple network adapters.</em></span></p><p>This issue is solved for Dell hardware, which has the corresponding BIOS support, by renaming onboard network interfaces to em[1234], which maps to Embedded NIC[1234] as labeled on server chassis.  (em stands for ethernet-on-motherboard.)</p><p>The renaming will be done by using the biosdevname utility.</p><p>biosdevname is automatically installed and used if YaST2 detects hardware suitable to be used with biosdevname. biosdevname can be disabled during installation by using "biosdevname=0" on the kernel commandline. The usage of biosdevname can be enforced on every hardware with "biosdevname=1". If the BIOS has no support, no network interface names are renamed.</p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Features.ResourceManagement"></a>6.6. Resource Management</h2></div></div></div><p></p><p></p><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-312040"></a>6.6.1. OS level virtualization: Linux Container (LXC)</h3></div></div></div><p>SUSE Linux Enterprise Server 11 SP2 supports "system containers" with the LXC (LinuX Container) infrastructure to achieve soft partitioning of large physical systems. In this infrastructure, instances of SLES 11 SP2 run within a host instance of SLES 11 SP2. In other words: other than with a hypervisor, all instances share one Linux Kernel, every instance has its own "init" process though.</p><p>While the host system has access to the guest instances and their filesystem, the guest instances do not see the host or the other guests other than via network or explicitly share storage (if configured). Thus, Linux Containers should not be used as the primary or only security measure around or inbetween highly secure environments.</p><p>More information about LXC can be found in the SUSE Linux Enterprise 11 documentation.</p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Features.SystemManagement"></a>6.7. Systems Management</h2></div></div></div><p></p><div class="itemizedlist"><ul type="disc"><li><p>
    Improved Update Stack
   </p><p>
    SUSE Linux Enterprise Server 11 provides an improved update stack and the new command line
    tool <span class="command"><strong>zypper</strong></span> to manage the repositories and
    install or update packages.
   </p></li><li><p>
    Enhanced YaST Partitioner
   </p></li><li><p>
    Extended Built-in Management Infrastructure
   </p><p>
    SUSE Linux Enterprise Server provides CIM/WBEM enablement with the SFCB CIMOM.
   </p><p>
    
   The following CIM providers are available:
   </p><div class="itemizedlist"><ul type="circle"><li><p>
cmpi-pywbem-base
</p></li><li><p>
cmpi-pywbem-power-management (DSP1027)
</p></li><li><p>
cmpi-pywbem-software (DSP1023)
</p></li><li><p>
libvirt-cim (DSP1041, DSP1043, DSP1045, DSP1057, DSP1059, DSP1076, DSP1081)
</p></li><li><p>
sblim-cmpi-base		
</p></li><li><p>
sblim-cmpi-dhcp		
</p></li><li><p>
sblim-cmpi-ethport_profile (DSP1014)
</p></li><li><p>
sblim-cmpi-fsvol		
</p></li><li><p>
sblim-cmpi-network		
</p></li><li><p>
sblim-cmpi-nfsv3
</p></li><li><p>
sblim-cmpi-nfsv4		
</p></li><li><p>
sblim-cmpi-sysfs		
</p></li><li><p>
sblim-gather-provider
</p></li><li><p>
smis-providers
</p></li><li><p>
sblim-cmpi-dns		
</p></li><li><p>
sblim-cmpi-samba		
</p></li><li><p>
sblim-cmpi-smbios
</p></li></ul></div></li><li><p>
    Support for Web Services for Management (WS-Management)
   </p><p>
    The WS-Management protocol is supported via Openwsman, providing
    client (package: openwsman-client) and server (package:
    openwsman-server) implementations.
   </p><p>
    This allows for interoperable management with the Windows 'winrm' stack.
   </p></li><li><p>
    WebYaST &#8212; Web-Based Remote Management
   </p><p>
    WebYaST is an easy to use, web-based administration tool targeted at
    casual Linux administrators.
   </p><p>
    SUSE Linux Enterprise Server 11 SP2 adds WebYaST via an online software repository. After
    successful registration you can install and start WebYaST by
    following these steps:
    </p><div class="itemizedlist"><ul type="circle"><li><p>Enable online repositories:</p><pre class="screen">zypper mr -e SLE11-WebYaST-SP2-Pool
zypper mr -e SLE11-WebYaST-SP2-Updates</pre></li><li><p>Install via pattern:</p><pre class="screen">zypper in -t pattern WebYaST-UI WebYaST-Service</pre></li><li><p>Open firewall ports:</p><pre class="screen">SuSEfirewall2 open EXT TCP 54984
SuSEfirewall2 restart</pre></li><li><p>Start services:</p><pre class="screen">rccollectd start
rcyastws start
rcyastwc start</pre></li></ul></div><p>
    The last command will display the URL to connect to with a Web browser.
   </p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Features.Other"></a>6.8. Other</h2></div></div></div><p></p><div class="variablelist"><dl><dt><span class="term">EVMS2 Replaced with LVM2</span></dt><dd><p></p></dd><dt><span class="term">Default File System</span></dt><dd><p>
     With SUSE Linux Enterprise Server 11, the default file system in new installations has
     been changed from ReiserFS to ext3. A public statement can be found
     at
     <a class="ulink" href="http://www.suse.com/products/server/technical-information/#FileSystem" target="_top">http://www.suse.com/products/server/technical-information/#FileSystem</a>.
   </p></dd><dt><span class="term">UEFI Enablement on AMD64/Intel64</span></dt><dd><p></p></dd><dt><span class="term">Xen Boot Via Native-UEFI Not Supported</span></dt><dd><p></p></dd><dt><span class="term">SWAP over NFS</span></dt><dd><p></p></dd><dt><span class="term">Linux Foundation's Carrier Grade Linux (CGL)</span></dt><dd><p>
    SUSE supports the Linux Foundation's Carrier Grade Linux (CGL)
    specification. SUSE Linux Enterprise 11 meets the latest CGL 4.0
    standard, and is CGL registered.  For more information, see <a class="ulink" href="http://www.suse.com/products/server/cgl/" target="_top">http://www.suse.com/products/server/cgl/</a>.
   </p></dd><dt><span class="term">Hot-Add Memory and CPU with vSphere 4.1 or Newer</span></dt><dd><p>Hot-add memory and CPU is supported and tested for both 32-bit
   and 64-bit systems when running vSphere 4.1 or newer. For more
   information, see the VMware Compatibility Guide at <a class="ulink" href="http://www.vmware.com/resources/compatibility/detail.php?device_cat=software&amp;device_id=11287~16&amp;release_id=24" target="_top">http://www.vmware.com/resources/compatibility/detail.php?device_cat=software&amp;device_id=11287~16&amp;release_id=24</a>.
   </p></dd></dl></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311718"></a>6.8.1. Enhanced yast to support SCSI tape devices</h3></div></div></div><p>Support for SCSI tapes in yast for interactive handling. Udev rules are adjusted to make changes persistent in case of reboot.</p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Features.SystemZ"></a>6.9. System z</h2></div></div></div><p>Additional information about the topics listed below, can be found at <a class="ulink" href="http://www.ibm.com/developerworks/linux/linux390/documentation_novell_suse.html" target="_top">http://www.ibm.com/developerworks/linux/linux390/documentation_novell_suse.html</a>.</p><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311856"></a>6.9.1. Exploitation of new z196 / z114 processor instructions</h3></div></div></div><p>Performance improvement through exploitation of new System z196 processor instructions by binutils and  alternate GCC on the SDK. This feature will be active when the GNU assembler is invoked with -march=z196.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311848"></a>6.9.2. FICON IPL and device discovery hardening</h3></div></div></div><p>Improves the DASD error recovery procedures used in the early phases of IPL and DASD device initialization with additional error recovery procedures.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311763"></a>6.9.3. Userspace handle to wait for cio pending work</h3></div></div></div><p>User space processes can delay I/O operations until all pending requests against the common I/O layer have been completed, eg. a user process wants to wait until a device is useable after a CP ATTACH command.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="Features.SystemZ.Hardware"></a>6.9.4. Hardware</h3></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>System z optimizations for gcc</p></li><li><p>Exploitation of z10 prefetching instructions</p></li><li><p>64-bit register support in 31-bit emulation</p></li><li><p>zEnterprise z196/z114 exploitation in toolchain</p></li><li><p>zEnterprise z196/z114 enhanced node affinity support</p></li><li><p>zEnterprise z196/z114 optimized support with add-on gcc</p></li><li><p>Performance indicator bytes</p></li><li><p>Spinning mutex performance enhancement</p></li><li><p>Get CPC name</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="Features.SystemZ.Virtualization"></a>6.9.5. Virtualization</h3></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>cmsfs read and write support for kernel 2.6</p></li><li><p>Deliver z/VM CP special messages to userspace using udev events (uevents)</p></li><li><p>Improve memory ballooning with <code class="systemitem">cpuplugd</code></p></li><li><p>snIPL support for z/VM 6</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="Features.SystemZ.Storage"></a>6.9.6. Storage</h3></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>[FICON] DS8k support - Solid state device support</p></li><li><p>[FICON] Dynamic PAV toleration</p></li><li><p>[FICON] Multi-Track extensions for High Performance FICON</p></li><li><p>Store I/O Operation Status and initiate logging (SIOSL)</p></li><li><p>Automatic detection of read only DASDs</p></li><li><p>Tunable default grace period for missing interrupts in DASD driver</p></li><li><p>Access to raw ECKD data from Linux</p></li><li><p>DASD Tools - implement new partition types</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="Features.SystemZ.Network"></a>6.9.7. Network</h3></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Offload Outbound Checksumming to OSA within qeth driver</p></li><li><p>Configuration tool for System z network devices</p></li><li><p>OSX (OSM) chpids for hybrid data (management) network</p></li><li><p>NAPI support for qeth and qdio</p></li><li><p>Optimized Latency Mode (OLM) toleration</p></li><li><p>IPv6 support for qetharp tool</p></li><li><p>Support assisted VLAN null tagging support</p></li><li><p>Optimal qeth default settings</p></li><li><p>New communication infrastructure for HiperSockets</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="Features.SystemZ.Security"></a>6.9.8. Security</h3></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>zEnterprise z196/114: CP ACF exploitation &#8212; in kernel crypto and libica</p></li><li><p>zEnterprise z196/114: Support for 4096-bit RSA FastPath</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="Features.SystemZ.RAS"></a>6.9.9. RAS</h3></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>[FICON] IPL &amp; device discovery hardening</p></li><li><p>cio: provide userspace handle to wait for pending work</p></li><li><p>virtualtop for s390tools</p></li><li><p>Valgrind - complete System z support</p></li><li><p>Breaking-event-address for userspace programs</p></li><li><p>reipl tool chreipl enhancements</p></li><li><p>Calculate Boot Device Ramdisk Address in zipl</p></li><li><p>Resume handling for re-ordered devices in cio layer</p></li><li><p>CHPID reconfiguration handling</p></li><li><p>Unit Check handling</p></li><li><p>Automatic menu support in zipl</p></li><li><p>[FICON] API &amp; Tool to query DASD reservation status</p></li><li><p>[FICON] Improve handling of stolen DASD reservation</p></li><li><p>Removed support for multi-volume tape dumps</p></li><li><p>Tool to safely start getty through init</p></li><li><p>SCSI dump on remote container</p></li><li><p>Handle channel path description changes in common I/O layer</p></li><li><p>SCSI Dump Device Configuration via YaST</p></li><li><p>Support blktrace in default kernel</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="Features.SystemZ.Web20"></a>6.9.10. Web 2.0 Open Source Stack in SUSE Linux Enterprise Software Development Kit</h3></div></div></div><p></p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="Features.SystemZ.sles10"></a>6.9.11. Functionality implemented in SUSE Linux Enterprise Server 11 (and SUSE Linux Enterprise Server 10 Service Pack 2.)</h3></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>AF_IUCV Support </p></li><li><p>Provide Linux file system data into z/VM monitor stream</p></li><li><p>Provide Linux process data into z/VM monitor stream</p></li><li><p>System z support for processor degradation</p></li><li><p>In-Kernel crypto exploitation of new CP Assist functions</p></li><li><p>Linux CPU Node Affinity</p></li><li><p>Support for OSA 2 Ports per CHPID</p></li><li><p>cpuplugd to automatically adapt CPU and/or memory</p></li><li><p>Dynamic CHPID reconfiguration via SCLP - tools</p></li><li><p>skb scatter-gather support for large incoming
    messages - QETH Exploitation</p></li><li><p>Support for HiperSockets in Layer 2 mode (with IPv4 and IPv6)</p></li></ul></div></div></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="Drivers"></a>Chapter 7. Driver Updates</h2></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Drivers.Network"></a>7.1. Network Drivers</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>
      Updated bnx driver to version 2.0.4
    </p></li><li><p>
      Updated bnx2x driver to version 1.52.1-7
    </p></li><li><p>
      Updated e100 driver to version 3.5.24-k2
    </p></li><li><p>
      Updated tg3 driver to version 3.106
    </p></li><li><p>
      Added bna driver for Brocade 10Gbit LAN card in version 2.1.2.1
    </p></li><li><p>
      Updated bfa driver to version 2.1.2.1
    </p></li><li><p>
      Updated qla3xxx driver to version 2.03.00-k5
    </p></li><li><p>
      Updated sky2 driver to version 1.25
    </p></li></ul></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311957"></a>7.1.1. ixgbe Driver Update to version 3.3.8</h3></div></div></div><p>This new ixgbe driver version adds support for the following devices:
82599EB 10 Gigabit Network Connection
82599EB 10 Gigabit TN Network Connection
X540-AT2 Ethernet Controller 10 Gigabit 
82599 10 Gigabit Dual Port Backplane Connection with FCoE
82599 10 Gigabit Dual port Network Connection with FCoE
82599EB 10 Gigabit SFP+ Network Connection
82599 10 Gigabit Dual Port Network Connection</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311956"></a>7.1.2. Added the ixgbevf Driver, Version 2.0.0</h3></div></div></div><p>This is a new virtual function driver added for SR-IOV support with the Intel ixgbe 10 Gigabit devices.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311863"></a>7.1.3. igb Driver Update to version 3.0.6</h3></div></div></div><p>Added support for the following devices:
82580 Gigabit Network Connection
82580 Gigabit Fiber Network Connection
82580 Gigabit Backplane Connection
82580 Gigabit SFP Connection
82580 Gigabit Network Connection
I350 Gigabit Network Connection
I350 Gigabit Fiber Network Connection
I350 Gigabit Backplane Connection
I350 Gigabit Connection
82576 Gigabit Network Connection
82580 Gigabit Fiber Network Connection</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311862"></a>7.1.4. igbvf Driver Update to Version 1.0.8</h3></div></div></div><p>This Service Pack adds SR-IOV support for the Intel(R) I350 devices.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311825"></a>7.1.5. e1000e Driver Update to version 1.3.16</h3></div></div></div><p>This new version of the e1000e driver adds support for the following devices:
82567LM Gigabit Network Connection
82574L Gigabit Network Connection
82567V-3 Gigabit Network Connection
82579LM Gigabit Network Connection
82579V Gigabit Network Connection
82583V Gigabit Network Connection
82567V-4 Gigabit Network Connection
82566DC-2 Gigabit Network Connection</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311470"></a>7.1.6. IBM Power Chelsio T4 Adapter cxgb4i Driver</h3></div></div></div><p><span class="emphasis"><em>The Chelsio T4 adapter with the cxgb4, cxgb4i, and iw_cxgb4 drivers support
10Ge NIC, iSCSI, and iWARP functions respectively.  IBM Power systems
support Enhanced Error Handling (EEH) and Hotplug removal.  When hotplug
operations are performed on a running adapter, a crash, hang or failure to
remove the adapter may occur.</em></span></p><p>A permanent solution in the device drivers is being investigated but may
not be ready in time for GM.  Until the maintenance driver is released, it
is necessary to unload all of the cxgb4, cxgb4i, and iw_cxgb4 drivers prior
to running any of the hotplug commands such as 'drmgr -r'.</p><p>Once the drivers are unloaded, the adapter can be hotplug moved to another
partition or removed from the system as necessary.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311451"></a>7.1.7. Brocade 10G PCIe Ethernet Adapters (bna)</h3></div></div></div><p>The bna 3.0.2.2 driver supports all Brocade FC/FCOE adapters. Below is a list of 
adapter models with corresponding PCIIDs:</p><pre class="screen">PCIID			Model 

1657:0014:1657:0014	1010 10Gbps single port CNA - LL 
1657:0014:1657:0014	1020 10Gbps dual port CNA - LL 
1657:0014:1657:0014	1007 10Gbps dual port CNA - LL 
1657:0014:1657:0014	1741 10Gbps dual port CNA - LL 

1657:0022:1657:0023	1860 10Gbps CNA - LL 
1657:0022:1657:0023	1860 10Gbps NIC - LL</pre><p>
          <span class="emphasis"><em>Firmware Download:</em></span> The latest Firmware package for 3.0.2.2 bna driver can be found at: 
 
http://www.brocade.com/services-support/drivers-downloads/adapters/Linux.page 
 
and then click following respective util package link: 
 
	Version			Link  
 
	v3.0.2.0	Linux Adapter Firmware package for RHEL 6.2, SLES 11SP2 </p><p>
          <span class="emphasis"><em>Configuration and Management utility download:</em></span> The latest driver configuration &amp; management utility for 3.0.2.2 bna driver can 
be found at <a class="ulink" href="http://www.brocade.com/services-support/drivers-downloads/adapters/Linux.page" target="_top">http://www.brocade.com/services-support/drivers-downloads/adapters/Linux.page</a> and then click version v3.0.2.0, "Linux Adapter Util package for RHEL 6.2, SLES 11SP2". </p><p>
          <span class="emphasis"><em>Documentation:</em></span> The latest Administration's Guide, Installation and Reference Manual, 
Troubleshooting Guide, and Release Notes for the corresponding out-of-box 
driver can be found at <a class="ulink" href="http://www.brocade.com/services-support/drivers-downloads/adapters/Linux.page" target="_top">http://www.brocade.com/services-support/drivers-downloads/adapters/Linux.page</a> and use the following inbox and out-of-box driver version mapping to find 
the corresponding documentation: </p><pre class="screen">Inbox Version		Out-of-box Version  
 
v3.0.2.2		v3.0.0.0</pre><p>
          <span class="emphasis"><em>Support:</em></span> For general product and support info, go to the Brocade website at <a class="ulink" href="http://www.brocade.com/services-support/index.page" target="_top">http://www.brocade.com/services-support/index.page</a> . </p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Drivers.Storage"></a>7.2. Storage Drivers</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>
        Updated qla2xxx to version 8.03.01.04.11.1-k8
      </p></li><li><p>
        Updated qla4xxx to version v5.01.00.00.11.01-k13
      </p></li><li><p>
        Updated megaraid_mbox driver to version 2.20.5.1
      </p></li><li><p>
        Updated megaraid_sas to version 4.27
      </p></li><li><p>
        Updated MPT Fusion to version 4.22.00.00
      </p></li><li><p>
        Updated mpt2sas driver to version 04.100.01.02
      </p></li><li><p>
        Updated lpfc driver to version 8.3.5.7
      </p></li><li><p>
      Added bnx2i driver for Broadcom NetXtreme II in version 2.1.1
    </p></li><li><p>
        Updated bfa driver to version 2.1.2.1
      </p></li><li><p>The enic driver was updated to version 1.4.2 to support newer Cisco
UCS systems. This update also replaces LRO (Large Receive Offload) to GRO
(Generic Receive Offload).</p></li></ul></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311812"></a>7.2.1. Support for Intel RSTe3.0 (Intel Rapid Storage Technology)</h3></div></div></div><p><span class="emphasis"><em>Rapid Storage Technology enterprise 3.0 for the Linux allows users to install/boot to Intel BIOS initialized SW RAID. New features supported with this version includes Disk Coercion, Email Alerting, RAID5 Xor, Hot Spare Disk, Read Patrol,
On Line Capacity Expansion, RAID Level Migrations, Check Pointing, smart alerting , Expanded Stripe Size, SAS &amp; SATA drive roaming, and Auto Rebuild.</em></span></p><p>This service pack includes the proper upstream md raid userspace (mdadm/mdmon) software raid utilities to ensure full feature functionality including install/boot support.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311808"></a>7.2.2. Support for Intel SAS Controller Unit (SCU) driver "isci"</h3></div></div></div><p><span class="emphasis"><em>The Intel 6 Series/C200 Series Chipset Platform Controller Hub (PCH) for mainstream Servers requires the isci driver for the Intel SAS Controller Unit (SCU).</em></span></p><p>This service pack includes the official SCU "isci.c" driver to ensure full SCU support including install/boot support.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311801"></a>7.2.3. Major advances in supporting iSCSI and FCoE</h3></div></div></div><p>
          <span class="emphasis"><em>Instructions to setup iSCSI initiator over DCB:</em></span>
        </p><p>The iSCSI initiator will automatically set packet priority based on the DCB iSCSI application priority in effect on the egress interface. The priority is set once at session establishment. If the DCB priority is to be changed, it will be necessary to reestablish the session to apply the changed priority.</p><p>Because the priority is set based on the egress interface, the priority cannot be set until the egress interface is known. This means that by default, the initial TCP packets to establish the session will not have a priority set, but subsequent packets will. If a session is bound to an interface, then the priority associated with that interface will be used even for the initial packet exchange. If a routing change results in a different egress interface being used, the same priority will continue to be used unless or until the session is re-established.</p><p>It is specifically recommended to bind to a VLAN interface. This allows the DCB-iSCSI priority to be communicated in the VLAN header. Without a VLAN header to convey the priority, the priority will only affect packet scheduling within the host. Commands such as the following demonstrate binding to a VLAN interface:</p><pre class="screen">iscsiadm -m iface -I iface3 --op=new
iscsiadm -m iface -I iface3 --op=update -n iface.net_ifacename -v eth3.3260</pre><p>By binding to the interface, every packet will carry the correct priority.</p><p>Make sure that the app tlv for iSCSI is enabled on the system and that the switchport is configured to use iscsi-default cee map AND lldp iscsi-priority-bits 0x10 is set:</p><p>For example, to configure the switchport on Brocade:</p><pre class="screen">no cee
cee iscsi-default
lldp iscsi-priority-bits 0x10</pre><p>This sets iSCSI to use priority 4. Assuming that the host is willing (will accept DCB configuration from the switch), iSCSI should then operate at priority 4.</p><p>The following will set the app tlv in CEE mode from the host:</p><pre class="screen">dcbtool sc ethX app:1 e:1 a:1 w:1 appcfg:10</pre><p>To enable app tlv in IEEE mode from the host:</p><pre class="screen">lldptool -T -i eth2 -V APP app=4,2,3260</pre><p>Note that the 3260 above is the well-known port number for iSCSI. The iSCSI app priority is always communicated using the well-known port number and will be used even if iSCSI has been configured to operate on a non-standard port. A non-standard port number is never used to determine the iSCSI initiator priority.</p><p>There probably is a lot that could be said about DCB. It would probably simplify things to make some assumptions about how it will be used. For example, I expect that the DCB parameters will be nearly always managed from the switch, so perhaps the only real host configuration that should be needed is just turning on DCB. The rest of it probably just adds confusion.</p><p>
          <span class="emphasis"><em>FCoE target setup:</em></span>
        </p><p>Refer this wiki
http://www.open-fcoe.org/open-fcoe/wiki/tcm-fcoe-target-guide</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311466"></a>7.2.4. Open-iSCSI supported added to the QLogic iSCSI qla4xxx driver</h3></div></div></div><p>Open-iSCSI support is added to the QLogic iSCSI qla4xxx driver in SUSE Linux
Enterprise Server 11 Service Pack 2.
Using iscsiadm the features supported for qla4xxx are:</p><div class="itemizedlist"><ul type="disc"><li><p>Network configuration</p></li><li><p>iSCSI Target management enabling Discovery, Login/Logout of iSCSI targets</p></li></ul></div><p> For more details, see Open-iSCSI README at <a class="ulink" href="http://www.open-iscsi.org/docs/README" target="_top">http://www.open-iscsi.org/docs/README</a> . </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311462"></a>7.2.5. Broadcom FCoE and iSCSI Enhanced Support for SLE11SP2</h3></div></div></div><p>
          <span class="emphasis"><em>Using bnx2fc driver for installation:</em></span>
        </p><p>Broadcom's NetXtreme II 57712 device provides networking as well as
storage functionality. Boot from SAN on this device is supported over
FCoE network using bnx2fc driver. Add "withfcoe=1" to the boot option
line.  Since the DCBX protocol is offloaded and performed by the device
firmware, 'dcb' feature should be turned off during installation when
prompted.</p><p>Note that FCoE boot from SAN on Broadcom 10G devices is only
supported using the bnx2fc driver.  Boot from SAN using the software
fcoe driver is not supported.</p><p>For detailed information, refer to "Broadcom
NetXtreme II(tm)  Network Adapter User Guide".</p><p>
          <span class="emphasis"><em>Using iSCSI Disks When Installing:</em></span>
        </p><p>Note: The installer for SLES 11 SP2 now supports iscsi install using
software iscsi method and native Broadcom offload method on Broadcom
NetXtreme II devices.</p><p>To use Broadcom offload iSCSI during install, the iSCSI option ROM
on the Broadcom device must be set to HBA mode.  Refer to  "iSCSI Boot
Broadcom NetXtreme II(tm)  Network Adapter User Guide" for detailed
information on iSCSI install/boot for Broadcom devices.</p><p>To use software iSCSI install, disable HBA mode in the Broadcom
iSCSI option ROM.</p><p>
          <span class="emphasis"><em>Storage Drivers:</em></span>
        </p><div class="itemizedlist"><ul type="disc"><li><p>Added bnx2i driver for Broadcom NetXtreme II in version 2.7.0.3</p></li><li><p>Added new bnx2fc driver for Broadcom NetXtreme II 57712</p></li></ul></div><p>Bnx2fc is a
FCoE offload driver, that uses open-fcoe's stack and fcoeutils. Note
that SLES 11 SP2 only supports offload FCoE on NetXtreme II 57712. Refer
to Documentation/scsi/bnx2fc.txt in linux kernel source for the driver
usage information.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311450"></a>7.2.6. Brocade FC/FCOE Adapters (bfa) Update Notes</h3></div></div></div><p>The bfa 3.0.2.2 driver supports all Brocade FC/FCOE adapters. Below is a list of  
adapter models with corresponding PCIIDs:</p><pre class="screen">PCIID		  	Model		 

1657:0013:1657:0014	425 4Gbps dual port FC HBA 
1657:0013:1657:0014	825 8Gbps PCIe dual port FC HBA 
1657:0013:103c:1742	HP 82B 8Gbps PCIedual port FC HBA 
1657:0013:103c:1744	HP 42B 4Gbps dual port FC HBA 
1657:0017:1657:0014	415 4Gbps single port FC HBA 
1657:0017:1657:0014	815 8Gbps single port FC HBA 
1657:0017:103c:1741	HP 41B 4Gbps single port FC HBA 
1657:0017:103c 1743	HP 81B 8Gbps single port FC HBA 
1657:0021:103c:1779	804 8Gbps FC HBA for HP Bladesystem c-class 
 
1657:0014:1657:0014	1010 10Gbps single port CNA - FCOE 
1657:0014:1657:0014	1020 10Gbps dual port CNA - FCOE 
1657:0014:1657:0014	1007 10Gbps dual port CNA - FCOE 
1657:0014:1657:0014	1741 10Gbps dual port CNA - FCOE 

1657:0022:1657:0024	1860 16Gbps FC HBA 
1657:0022:1657:0022	1860 10Gbps CNA - FCOE</pre><p>
          <span class="emphasis"><em>Firmware Download:</em></span> The latest Firmware package for the 3.0.2.2 bfa driver can be found at <a class="ulink" href="http://www.brocade.com/services-support/drivers-downloads/adapters/Linux.page" target="_top">http://www.brocade.com/services-support/drivers-downloads/adapters/Linux.page</a> ,
then click version v3.0.2.0, "Linux Adapter Firmware package for RHEL 6.2, SLES 11SP2". </p><p>
          <span class="emphasis"><em>Configuration and Management Utility Download:</em></span> The latest driver configuration and management utility for 3.0.2.2 bfa driver can 
be found at <a class="ulink" href="http://www.brocade.com/services-support/drivers-downloads/adapters/Linux.page" target="_top">http://www.brocade.com/services-support/drivers-downloads/adapters/Linux.page</a> , then click version v3.0.2.0 "Linux Adapter Firmware package for RHEL 6.2, SLES 11SP2". </p><p>
          <span class="emphasis"><em>Documentation:</em></span> The latest Administration's Guide, Installation and Reference Manual, 
Troubleshooting Guide, and Release Notes for the corresponding out-of-box 
driver can be found at <a class="ulink" href="http://www.brocade.com/services-support/drivers-downloads/adapters/Linux.page" target="_top">http://www.brocade.com/services-support/drivers-downloads/adapters/Linux.page</a> and use the following inbox and out-of-box driver version mapping to find 
the corresponding documentation: </p><pre class="screen">Inbox Version		Out-of-box Version  
v3.0.2.2		v3.0.0.0</pre><p>
          <span class="emphasis"><em>Support:</em></span> For general product and support info, go to the Brocade website at <a class="ulink" href="http://www.brocade.com/services-support/index.page" target="_top">http://www.brocade.com/services-support/index.page</a> . </p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Drivers.Other"></a>7.3. Other Drivers</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>
        Updated CIFS to version 1.74
      </p></li><li><p>
        Updated intel-i810 driver
      </p></li><li><p>
        Added X11 driver for AMD Geode LX 2D (xorg-x11-driver-video-amd)
      </p></li><li><p>
        Updated X11 driver for Radeon cards
      </p></li><li><p>
        Updated XFS and DMAPI driver
      </p></li><li><p>
        Updated Wacom driver to version 1.46
      </p></li></ul></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311827"></a>7.3.1. Support for Universal Serial Bus Version 3.0 (USB 3.0)</h3></div></div></div><p><span class="emphasis"><em>USB 3.0 is the third major revision of the USB standard, which brings faster data transfer and increases power savings. More and more USB 3.0 consumer products are launched in market. 
Intel starts to support USB 3.0 in the Intel(R) 7 Series/C216 Chipset Family.</em></span></p><p>This SP introduces support for USB 3.0 by adding patches for xHCI (eXtensible Host Controller Interface), USB 3.0 hub support and USB 3.0 support for Intel(R) 7 Series/C216 Chipset Family.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311819"></a>7.3.2. Support Intel(R) HD Graphics 2000/3000 used in 2nd Generation Intel(R) Core&#8482; i7/i5/i3 processor family</h3></div></div></div><p><span class="emphasis"><em>The processor graphics is provided in the 2nd Generation Intel(R) Core&#8482; i7/i5/i3 processor family.</em></span></p><p>This service pack adds support for the processor graphics in the 2nd Generation Intel(R) Core&#8482; i7/i5/i3 processor family by updating the required kernel module, xserver, xf86-video-intel driver, Mesa and dri driver.</p></div></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="OtherUpdates"></a>Chapter 8. Other Updates</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>
      Added support for installation from an NFSv4 server.
    </p></li><li><p>
      Updated binutils to version 2.21.1
    </p></li><li><p>
      Updated bluez to version 4.51
    </p></li><li><p>
      Updated clamav to version 0.97.3
    </p></li><li><p>
      Updated crash to version 5.1.9
    </p></li><li><p>
      Updated dhcp to version 4.2.3.P2
    </p></li><li><p>
      Updated gdb to version 7.3
    </p></li><li><p>
      Updated hplip to version 3.11.10
    </p></li><li><p>
      Updated ipsec-tools to version 0.7.3
    </p></li><li><p>
      Updated IBM Java 1.4.2 (java-1_4_2-ibm) to SR13 FP11
    </p></li><li><p>
      Updated IBM Java 1.6.0 (java-1_6_0-ibm) to SR9.3
    </p></li><li><p>
      Updated libcgroup1 to version 0.37.1
    </p></li><li><p>
      Updated libcmpiutil to version 0.5.6
    </p></li><li><p>
      Updated libelf to version 0.8.12
    </p></li><li><p>
      Updated QT4 (libqt4) to version 4.6.3
    </p></li><li><p>
    Updated libvirt to version 0.9.6
   </p></li><li><p>
    Updated libvirt-cim to version 0.5.12
   </p></li><li><p>
      Updated mdadm to version 3.2.2
    </p></li><li><p>
      Updated module-init-tools to version 3.11.1
    </p></li><li><p>
      Updated MozillaFirefox to version 10
    </p></li><li><p>
      Added mt_st version 0.9b
    </p></li><li><p>
      Added netlabel version 0.19
    </p></li><li><p>
      Updated numactl to version 2.0.7
    </p></li><li><p>
      Updated openCryptoki to version 2.4
    </p></li><li><p>
      Updated openldap2 to version 2.4.26
    </p></li><li><p>
      Added openvas version 3.0
    </p></li><li><p>
      Added perf: Performance Counters For Linux
    </p></li><li><p>
      Added perl-WWW-Curl version 4.09
    </p></li><li><p>
      Added rng-tools: Support daemon for hardware random device
    </p></li><li><p>
      Updated sblim-cim-client2 to version 2.1.3
    </p></li><li><p>
      Updated sblim-cmpi-base to version 1.6.1
    </p></li><li><p>
      Updated sblim-cmpi-fsvol to version 1.5.0
    </p></li><li><p>
      Updated sblim-cmpi-network to version 1.4.0
    </p></li><li><p>
      Updated sblim-cmpi-nfsv3 to version 1.1.0
    </p></li><li><p>
      Updated sblim-cmpi-nfsv4 to version 1.1.0
    </p></li><li><p>
      Updated sblim-cmpi-params to version 1.3.0
    </p></li><li><p>
      Updated sblim-cmpi-sysfs to version 1.2.0
    </p></li><li><p>
      Updated sblim-gather to version 2.2.0
    </p></li><li><p>
      Updated sblim-sfcb to version 1.3.11
    </p></li><li><p>
      Updated sblim-sfcc to version 2.2.1
    </p></li><li><p>
      Updated sblim-wbemcli to version 1.6.1
    </p></li><li><p>
      Updated strongswan to version 4.4.0
    </p></li><li><p>
      Added stunnel version 4.36
    </p></li><li><p>
    Updated virt-viewer to version 0.4.1
   </p></li><li><p>
    Updated virt-manager to version 0.9.0
   </p></li><li><p>
    Updated kvm to version 0.15.1
   </p></li><li><p>
    Updated Xen (xen) to version 4.1.2
   </p></li><li><p>
      Updated dcbd to version 0.9.24
    </p></li><li><p>
      Updated e2fsprogs to version 1.41.9
    </p></li><li><p>
      Updated iprutils to version 2.3.7
    </p></li><li><p>
      Updated iscsitarget to version 1.4.20
    </p></li><li><p>
      Updated nfs-utils to version 1.2.3 for improved IPv6 support
    </p></li><li><p>
      Added apport, a tool to collect data automatically from crashed processes
    </p></li></ul></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-312669"></a>8.1. Upgrade to gawk 3.1.8</h2></div></div></div><p><span class="emphasis"><em>gawk as delivered in SUSE Linux Enterprise 11 SP1, has a low performance with respect to multibyte string operations.</em></span></p><p>Carefully considering the changes from 3.1.6 to 3.1.8 we decided that a version upgrade will significantly help in other areas as well. Find below the list of important changes:</p><div class="itemizedlist"><ul type="disc"><li><p>The zero flag no longer applies to %c and %s.</p></li><li><p>Failure to open a socket is no longer a fatal error.</p></li><li><p>The ' flag (%'d) is now just ignored on systems that cannot support it.</p></li><li><p>Gawk now handles multibyte strings better in [s]printf with field widths and such.</p></li><li><p>A getline from a directory is no longer fatal; instead it returns -1.</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-310741"></a>8.2. Update gdb to Version 7.3</h2></div></div></div><p><span class="emphasis"><em>Several bugfixes for gdb version 7.1 accumulated, and upstream gdb
gained better support for some languages (e.g. Fortran and C++).
Backporting those changes to gdb 7.1 is not worthwhile.</em></span></p><p>Update gdb to Version 7.3</p></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="SDK"></a>Chapter 9. Software Development Kit</h2></div></div></div><p>
   SUSE provides a Software Development Kit (SDK) for SUSE Linux Enterprise 11 Service Pack
   2.  This SDK contains libraries, development-environments
   and tools along the following patterns:
  </p><div class="itemizedlist"><ul type="disc"><li><p>C/C++ Development</p></li><li><p>Certification</p></li><li><p>Documentation Tools</p></li><li><p>GNOME Development</p></li><li><p>Java Development</p></li><li><p>KDE Development</p></li><li><p>Linux Kernel Development</p></li><li><p>Programming Libraries</p></li><li><p>.NET Development</p></li><li><p>Miscellaneous</p></li><li><p>Perl Development</p></li><li><p>Python Development</p></li><li><p>Qt 4 Development</p></li><li><p>Ruby on Rails Development</p></li><li><p>Ruby Development</p></li><li><p>Version Control Systems</p></li><li><p>Web Development</p></li><li><p>YaST Development</p></li></ul></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-311554"></a>9.1. PowerPC64 GCC Large TOC Support</h2></div></div></div><p><span class="emphasis"><em>Previous versions of GCC limited the size of the TOC to 64kB.  Options like -mminimal-toc and the linker automatic multiple TOC section support extended the effective size of the TOC, but some very large programs required source changes to break up large functions in order to compile and link.</em></span></p><p>PowerPC64 GCC now supports -mcmodel=small, -mcmodel=medium and -mcmodel=large.  The latter two generate code for a 2G TOC. -mcmodel=medium optimizes accesses to local data but limits the total size of all data sections to 2G, in most cases giving a speed improvement over -mminimal-toc and may even give a speed improvement over the default -mcmodel=small. The linker supports mixing of object files compiled with any of these options.</p></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="Update"></a>Chapter 10. Update-Related Notes</h2></div></div></div><p>
   This section includes update-related information for this release.
  </p><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Update.General"></a>10.1. General Notes</h2></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-310481"></a>10.1.1. Automated Upgrade Using AutoYaST</h3></div></div></div><p>For an automated upgrade from SLES 10 SP4 or SLES 11 SP1 using AutoYaST see the Deployment Guide, Part "Automated Installations".  The Deployment Guide is part of the system documentation that comes with the product.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169165"></a>10.1.2. Online Migration from SP1 to SP2 via "YaST waggon"</h3></div></div></div><p>
    The online migration from SP1 to SP2 is supported via the "YaST
    waggon" module.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169175"></a>10.1.3. Online Migration with Debuginfo Packages Not Supported</h3></div></div></div><p>
    Online migration from SP1 to SP2 is not supported if debuginfo
    packages  are installed.
    </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169186"></a>10.1.4. Migrating to SLE 11 SP2</h3></div></div></div><p>
   To migrate the system to the Service Pack 2 level with
   <span class="command"><strong>zypper</strong></span>, use the <span class="command"><strong>dup</strong></span> subcommand
   with the <code class="option">--from</code> option and then finalize it with the
   <span class="command"><strong>patch</strong></span> subcommand as follows:
  </p><pre class="screen">zypper dup --from &lt;new SP2 repos&gt;
zypper patch</pre><p>
   If you use <span class="command"><strong>zypper dup</strong></span> without the
   <code class="option">--from</code> option, <span class="command"><strong>zypper</strong></span> will do a
   full update to the latest possible release on all channels, but not
   migrate to the Service Pack 2 level.
  </p><p>
   For more information about migrating the system to SLE 11 SP2, see
   the Deployment Guide.
  </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1166397"></a>10.1.5. Migration from SUSE Linux Enterprise Server 10 SP4 via Bootable Media</h3></div></div></div><p>
    Migration is supported from SUSE Linux Enterprise Server 10 SP4 via bootable media
    (incl. PXE boot).
    </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169308"></a>10.1.6. Upgrading from SLES 10 SPx</h3></div></div></div><p>
    There are supported ways to upgrade from SLES 10 SPx to SLES 11
    SP2, which may require intermediate upgrade steps:</p><div class="itemizedlist"><ul type="disc"><li><p>SLES 10 SP4 -&gt; SLES 11 SP2, or</p></li><li><p>SLES 10 SP4 -&gt; SLES 11 GA -&gt; SLES 11 SP2, or</p></li><li><p>SLES 10 SP2 -&gt; SLES 10 SP3 -&gt; SLES 10 SP4 -&gt; SLES 11 SP1</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169130"></a>10.1.7. Upgrading to SLES 11 SP2 with Root File System on iSCSI</h3></div></div></div><p>
The upgrade or the automated migration from SLES 10 to SLES 11 SP2 may
fail if the root file system of the machine is located on iSCSI because
of missing boot options.
   </p><p>
There are two approaches to solve it, if you are using AutoYaST (adjust
IP addresses and hostnames according to your environment!):
   </p><div class="variablelist"><dl><dt><span class="term">With Manual Intervention:</span></dt><dd><p>
Use as boot options:
      </p><p><code class="literal">withiscsi=1 autoupgrade=1 autoyast=http://myserver/autoupgrade.xml</code></p><p>Then, in the dialog of the iSCSI initiator, configure the
      iSCSI device.</p><p>
After successful configuration of the iSCSI device, YaST will find the
installed system for the upgrade.
      </p></dd><dt><span class="term">Fully Automated Upgrade:</span></dt><dd><p>
       Add or modify the &lt;iscsi-client&gt; section in your
       <code class="filename">autoupgrade.xml</code> as follows:
      </p><pre class="screen">&lt;iscsi-client&gt;
  &lt;initiatorname&gt;iqn.2012-01.com.example:initiator-example&lt;/initiatorname&gt;
  &lt;targets config:type="list"&gt;
    &lt;listentry&gt;
      &lt;authmethod&gt;None&lt;/authmethod&gt;
      &lt;iface&gt;default&lt;/iface&gt;
      &lt;portal&gt;10.10.42.84:3260&lt;/portal&gt;
      &lt;startup&gt;onboot&lt;/startup&gt;
      &lt;target&gt;iqn.2000-05.com.example:disk01-example&lt;/target&gt;
    &lt;/listentry&gt;
  &lt;/targets&gt;
  &lt;version&gt;1.0&lt;/version&gt;
&lt;/iscsi-client&gt;</pre><p>Then, run the automated upgrade with these boot options:</p><p><code class="literal">autoupgrade=1 autoyast=http://myserver/autoupgrade.xml</code></p></dd></dl></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169495"></a>10.1.8. 
      Kernel Split in Different Packages
    </h3></div></div></div><p>
      With SUSE Linux Enterprise Server 11 the kernel RPMs are split in different parts:
    </p><div class="itemizedlist"><ul type="disc"><li><p>kernel-flavor-base</p><p>
          Very reduced hardware support, intended
          to be used in virtual machine images.
        </p></li><li><p>kernel-flavor</p><p>
          Extends the base package; contains all supported
          kernel modules.
        </p></li><li><p>kernel-flavor-extra</p><p>
          All other kernel modules which may be
          useful but are not supported. This package
          will not be installed by default.
        </p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169624"></a>10.1.9. 
     Tickless Idle
   </h3></div></div></div><p>
     SUSE Linux Enterprise Server uses tickless timers. This can be disabled by adding
    <code class="option">nohz=off</code> as a boot option.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169549"></a>10.1.10. 
    Development Packages
   </h3></div></div></div><p>
    SUSE Linux Enterprise Server will no longer contain any development packages, with the
    exception of some core development packages necessary to compile
    kernel modules. Development packages are available in the SUSE Linux Enterprise Software Development Kit.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169432"></a>10.1.11. Displaying Manual Pages with the Same Name</h3></div></div></div><p>
    The <span class="command"><strong>man</strong></span> command now asks which manual page the
    user wants to see if manual pages with the same name exist in
    different sections. The user is expected to type the section number
    to make this manual page visible.
    </p><p>
     If you want to revert back to the previously used method, please
     set <code class="literal">MAN_POSIXLY_CORRECT=1</code> in a shell
     initialization file such as <code class="filename">~/.bashrc</code>.
    </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169118"></a>10.1.12. 
    YaST LDAP Server No Longer Uses
    <code class="filename">/etc/openldap/slapd.conf</code>
   </h3></div></div></div><p>
    The YaST LDAP Server module no longer stores the configuration of
    the LDAP Server in the file
    <code class="filename">/etc/openldap/slapd.conf</code>.  It uses OpenLDAP's
    dynamic configuration backend, which stores the configuration in an
    LDAP database itself. That database consists of a set of
    <code class="filename">.ldif</code> files in the directory
    <code class="filename">/etc/openldap/slapd.d</code>.  You should - usually -
    not need to access those files directly. To access the configuration
    you can either use the <span class="command"><strong>yast2-ldap-server</strong></span> module
    or any capable LDAP client (e.g., ldapmodify, ldapsearch, etc.). For
    details on the dynamic configuration of OpenLDAP, refer to the
    OpenLDAP Administration Guide.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169652"></a>10.1.13. 
    AppArmor
   </h3></div></div></div><p>
     This release of SUSE Linux Enterprise Server ships with AppArmor.
     The AppArmor intrusion prevention framework builds a firewall around your
     applications by limiting the access to files, directories, and POSIX
     capabilities to the minimum required for normal operation. AppArmor
     protection can be enabled via the AppArmor control panel, located
     in YaST under Security and Users. For detailed information about using
     AppArmor, see the documentation in
     <code class="filename">/usr/share/doc/packages/apparmor-docs</code>.
   </p><p>
     The AppArmor profiles included with SUSE Linux have been developed with
     our best efforts to reproduce how most users use their
     software. The profiles provided work unmodified for many
     users, but some users may find our profiles too restrictive for their
     environments.
   </p><p>
     If you discover that some of your applications do not function as you
     expected, you may need to use the AppArmor Update Profile Wizard in
     YaST (or use the aa-logprof(8) command line utility) to update your
     AppArmor profiles. Place all your profiles into learning mode with the
     following:
     <span class="command"><strong>aa-complain /etc/apparmor.d/*</strong></span>
   </p><p>
     When a program generates many complaints, the system's performance is
     degraded. To mitigate this, we recommend periodically running the Update
     Profile Wizard (or aa-logprof(8)) to update your profiles even if you
     choose to leave them in learning mode. This reduces the number of
     learning events logged to disk, which improves the performance of
      the system.
    </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169655"></a>10.1.14. 
    Updating with Alternative Boot Loader (Non-Linux) or Multiple Boot Loader Programs
   </h3></div></div></div><p>
    Note: Before updating, check the configuration of your boot loader to
    assure that it is not configured to modify any system areas (MBR,
    settings active partition or similar). This will reduce the amount
    of system areas that you need to restore after update.
   </p><p>
    Updating a system where an alternative boot loader (not grub) or an
    additional boot loader is installed in the MBR (Master Boot Record)
    might override the MBR and place grub as the primary boot loader
    into the system.
   </p><p>
    In this case, we recommend the following: First backup your data.
    Then either do a fresh installation and restore your data, or run
    the update nevertheless and restore the affected system areas (in
    particular, the MBR).  It is always recommended to keep data
    separated from the system software.  In other words,
    <code class="filename">/home</code>, <code class="filename">/srv</code>, and other
    volumes containing data should be on separate partitions, volume
    groups or logical volumes.  The YaST partitioning module will
    propose doing this.
   </p><p>
    Other update strategies (except booting the install media) are safe
    if the boot loader is configured properly.  But the other strategies
    are not available, if you update from SUSE Linux Enterprise Server
    10.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169346"></a>10.1.15. 
    Upgrading MySQL to SUSE Linux Enterprise Server 11
   </h3></div></div></div><p>
    During the upgrade to SUSE Linux Enterprise Server 11  MySQL is also upgraded to the latest version.
    To complete this migration you may have to upgrade your data as described
    in the MySQL documentation.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169802"></a>10.1.16. 
    Fine-Tuning Firewall Settings
   </h3></div></div></div><p>
    SuSEfirewall2 is enabled by default, which means you cannot log in
    from remote systems. This also interferes with network browsing and
    multicast applications, such as SLP and Samba ("Network
    Neighborhood"). You can fine-tune the firewall settings using
    YaST.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169812"></a>10.1.17. Upgrading from SUSE Linux Enterprise Server 10 SP4 with the Xen Hypervisor May Have
   Incorrect Network Configuration</h3></div></div></div><p>
    We have improved the network configuration: If you install SUSE Linux Enterprise Server 11
    SP2 and configure Xen, you get a bridged setup through YaST.
   </p><p>
    However, if you upgrade from SUSE Linux Enterprise Server 10 SP4 to SUSE Linux Enterprise Server 11 SP2, the
    upgrade does not configure the bridged setup automatically.
   </p><p>
    To start the bridge proposal for networking, start the "YaST Control
    Center", choose "Virtualization", then "Install Hypervisor and
    Tools".  Alternatively, call <span class="command"><strong>yast2 xen</strong></span> on the
    commandline.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169844"></a>10.1.18. LILO Configuration Via YaST or AutoYaST</h3></div></div></div><p>The configuration of the LILO boot loader on the x86 and x86_64
   architecture via YaST or AutoYaST is deprecated, and not supported
   anymore. For more information, see Novell TID 7003226 <a class="ulink" href="http://www.novell.com/support/documentLink.do?externalID=7003226" target="_top">http://www.novell.com/support/documentLink.do?externalID=7003226</a>.</p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Update.GAToSP2"></a>10.2. Update from SUSE Linux Enterprise Server 11</h2></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1168999"></a>10.2.1. Changed Routing Behavior</h3></div></div></div><p>
      SUSE Linux Enterprise Server 10 and SUSE Linux Enterprise Server 11 set <code class="literal">net.ipv4.conf.all.rp_filter =
      1</code> in <code class="filename">/etc/sysctl.conf</code> with the
      intention of enabling route path filtering.  However, the kernel
      fails to enable routing path filtering, as intended, by default in
      these products.
    </p><p>
      Since SLES 11 SP1, this bug is fixed and most simple single-homed
      unicast server setups will not notice a change. But it may cause
      issues for applications that relied on reverse path filtering
      being disabled (e.g., multicast routing or multi-homed servers).
    </p><p>
      For more details, see
      <a class="ulink" href="http://ifup.org/2011/02/03/reverse-path-filter-rp_filter-by-example/" target="_top">http://ifup.org/2011/02/03/reverse-path-filter-rp_filter-by-example/</a>.
    </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169424"></a>10.2.2. Kernel Devel Packages</h3></div></div></div><p>
      Starting with SUSE Linux Enterprise Server 11 Service Pack 1 the configuration files for recompiling the
      kernel were moved into their own sub-package:
    </p><div class="variablelist"><dl><dt><span class="term">kernel-flavor-devel</span></dt><dd><p>
        This package contains only the configuration for one kernel type
        (&#8220;<span class="quote">flavor</span>&#8221;), such as <code class="literal">default</code> or
        <code class="literal">desktop</code>.
       </p></dd></dl></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="Update.SP1ToSP2"></a>10.3. Update from SUSE Linux Enterprise Server 11 SP 1</h2></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169367"></a>10.3.1. Update from SUSE Linux Enterprise Server 11 SP 1</h3></div></div></div><p>
     Updating from SUSE Linux Enterprise Server 11 SP 1 with AutoYaST is
     supported.
    </p></div></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="Deprecated"></a>Chapter 11. Deprecated Functionality</h2></div></div></div><p>
    The following packages were removed with
    the release of SUSE Linux Enterprise Server 11 Service Pack 2:
  </p><div class="itemizedlist"><ul type="disc"><li><p>hyper-v-kmp</p><p>hyper-v-kmp has been removed.</p></li><li><p>
    The 32-bit Xen hypervisor as a virtualization host is not supported anymore. 32-bit virtual
    guests are not affected and fully supported with the provided 64-bit
    hypervisor.
   </p></li></ul></div><p>
    The following packages were removed with
    the release of SUSE Linux Enterprise Server 11 Service Pack 1:
  </p><div class="itemizedlist"><ul type="disc"><li><p>brocade-bfa</p><p>The brocade-bfa kernel module is
  now part of the main kernel package.</p></li><li><p>enic-kmp</p><p>The enic kernel module is
  now part of the main kernel package.</p></li><li><p>fnic-kmp</p><p>The fnic kernel module is
  now part of the main kernel package.</p></li><li><p>kvm-kmp</p><p>The KVM kernel modules are
  now part of the main kernel package.</p></li><li><p>java-1_6_0-ibm-x86</p></li></ul></div><p>
    The following packages were removed with
    the major release of SUSE Linux Enterprise Server 11:
  </p><div class="itemizedlist"><ul type="disc"><li><p>dante</p></li><li><p>JFS</p><p>The JFS file system is no longer
  supported and the utilities have been removed from the distribution.</p></li><li><p>EVMS</p><p>
     
     For the future strategy and development with respect to volume and
     storage management on SUSE Linux Enterprise, refer to:
     <a class="ulink" href="http://www.novell.com/linux/volumemanagement/strategy.html" target="_top">http://www.novell.com/linux/volumemanagement/strategy.html</a>
    </p></li><li><p>ippl</p></li><li><p>powertweak</p></li><li><p>SUN Java</p></li><li><p>uw-imapd</p></li><li><p>
      The mapped-base functionality, which is used by 32-bit applications
      that need a larger dynamic data space (such as database management systems),
      has been replaced with flexmap.
    </p></li><li><p>zmd</p></li></ul></div><p>
    The following packages and features are deprecated
    and will be removed with the next Service Pack or major release of SUSE Linux Enterprise Server:
  </p><div class="itemizedlist"><ul type="disc"><li><p>
   The <span class="command"><strong>reiserfs</strong></span> file system is fully supported for the
   lifetime of SUSE Linux Enterprise Server 11 specifically for migration purposes. We will
   however remove support for creating new reiserfs file systems starting
   with SUSE Linux Enterprise Server 12.
  </p></li><li><p>
   The <code class="systemitem">sendmail</code> package is
   deprecated and might be discontinued with SUSE Linux Enterprise Server 12.
    </p></li><li><p>
    The <code class="systemitem">lprng</code> package is
    deprecated and will be discontinued with SUSE Linux Enterprise Server 12.
    </p></li><li><p>
    The <code class="systemitem">dhcp-client</code> package is
    deprecated and will be discontinued with SUSE Linux Enterprise Server 12.
   </p></li><li><p>
    The <code class="systemitem">qt3</code> package is
    deprecated and will be discontinued with SUSE Linux Enterprise Server 12.
   </p></li><li><p>
    <code class="systemitem">syslog-ng</code> will be replaced
    with <code class="systemitem">rsyslog</code>.
   </p></li><li><p>
    The <code class="systemitem">smpppd</code> package is
    deprecated and will be discontinued with one of the next Service Packs or
    SUSE Linux Enterprise Server 12.
   </p></li><li><p>
    The raw block devices (major 162) are deprecated and will be
    discontinued with one of the next Service Packs or SUSE Linux Enterprise Server 12.
   </p></li></ul></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-311895"></a>11.1. Remove Support for Multi-Volume Tape Dumps</h2></div></div></div><p>The multi-volume tape dump support will be removed from zipl and zgetdump. The reason for this decision is that current tape cartridges have hundreds of gigabyte capacity and therefore the multi-volume support is not needed any more.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-313160"></a>11.2. Moving Novfs Kernel Module</h2></div></div></div><p><span class="emphasis"><em>Novfs and NCL are tightly coupled, the first is packaged on the SUSE Linux Enterprise Server media but the second is not.</em></span></p><p>To prepare the move of novfs into an external repository together with NCL the novfs kernel module is dropped from the SLES media. Customers find the new novfs and NCL package at //URL//</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-313016"></a>11.3. Support for portmap will end with SUSE Linux Enterprise 11 SP3</h2></div></div></div><p>In SUSE Linux Enterprise we provide "rpcbind" which for example provides full IPv6 support; it is compatible with portmap. Thus portmap is deprecated, and support for portmap will end end with SUSE Linux Enterprise 11 SP3.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-312991"></a>11.4. Replacing xpdf-tools</h2></div></div></div><p>With SP2 we are switching from xpdf-tools to poppler-tools for PDF rendering. This is based on xpdf-tools, but more stable and better maintained and it is a seamless replacement.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-312973"></a>11.5. L3 Support for Openswan Is Scheduled to Expire</h2></div></div></div><p><span class="emphasis"><em>L3 support for Openswan is scheduled to expire. This
decision is driven by the fact that Openswan development
stalled substantially and there are no tangible signs that this will
change in the future.</em></span></p><p>In contrast to this the strongSwan project is
vivid and able to deliver a complete implementation of current
standards. Compared to Openswan all relevant features are available by
the package strongSwan plus strongSwan is the only complete Open Source
implementation of the RFC 5996 IKEv2 standard whereas Openswan only
implements a small mandatory subset. For now
and the expected future only strongSwan qualifies to be an enterprise-ready solution for encrypted TCP/IP connectivity.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-312764"></a>11.6. Support for IBM Java 1.4.2 Ending 2013</h2></div></div></div><p>IBM Java 1.4.2 is supported with SUSE Linux Enterprise Server 11 specifically for migration purposes. We will however remove support for this specific Java version with SUSE Linux Enterprise Server 11 SP3 and SUSE Linux Enterprise Server 12. We recommend to upgrade your environments.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-312620"></a>11.7. Intel Active Management (IAMT)</h2></div></div></div><p>Intel Active Management (IAMT) drivers have been removed from SUSE Linux Enterprise due to incompatibilities and no longer being maintained. Refer to the Intel documentation on how to access newer versions of IAMT drivers for SUSE Linux Enterprise.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-311983"></a>11.8. PHP 5.2 Is Deprecated</h2></div></div></div><p>Based on significant customer demand, we ship PHP 5.3 parallel to PHP 5.2 with SUSE Linux Enterprise 11 SP1 and SP2.</p><p>PHP 5.2 is deprecated though, and will be removed with SLES 11 SP3.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-311111"></a>11.9. Read-only Support for the ext4 File System for Migration Purposes</h2></div></div></div><p>To facilitate the migration of an ext4 file system to another, supported file system, the SLE 11 SP2 kernel now contains a fully supported ext4 file system module, which provides solely read-only access to the file system.</p><p> If read-write access to an ext4 file system is still required, you may install the <code class="literal">ext4-writeable</code> KMP (kernel module package). This package contains a kernel module that provides read-write access to an ext4 file system. Be aware, that this kernel module is unsupported. </p><p>ext4 is not supported for the installation of the SUSE Linux Enterprise operating system files</p><p>With SUSE Linux Enterprise 11 SP2 we support offline migration from ext4 to the supported btrfs filesystem.</p></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="InfraPackArch"></a>Chapter 12. Infrastructure, Package and Architecture Specific Information</h2></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="InfraPackArch.SystemManagement"></a>12.1. Systems Management</h2></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-310064"></a>12.1.1. xrdp</h3></div></div></div><p>Remote systems can now be served with xrdp.  Windows clients are able to administer such servers.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-305278"></a>12.1.2. YaST AppArmor Configuration Module</h3></div></div></div><p>Find the AppArmor Configuration module now in the "Security and Users" section of the YaST Control Center.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1170258"></a>12.1.3. Modified Operation against Novell Customer Center</h3></div></div></div><p>
    Effective on 2009-01-13, provisional registrations have been disabled
    in the Novell Customer Center. Registering an instance of SUSE Linux Enterprise Server or
    Open Enterprise Server (OES) products now requires a valid, entitled
    activation code. Evaluation codes for reviews or proofs of concept
    can be obtained from the product pages and from the download pages
    on novell.com.
   </p><p>
    If a device is registered without a code at setup time, a
    provisional code is assigned to it by Novell Customer Center (NCC),
    and it will be entered in your NCC list of devices. No update
    repositories are assigned to the device at this time.
   </p><p>
    Once you are ready to assign a code to the device, start the YaST
    Novell Customer Center registration module and replace the
    un-entitled provisional code that NCC generated with the appropriate
    one to fully entitle the device and activate the related update
    repositories.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1170292"></a>12.1.4. Operation against Subscription Management Tool</h3></div></div></div><p>
    Operation under the Subscription Management Tool (SMT) package and
    registration proxy is not affected. Registration against SMT will
    assign codes automatically from your default pool in NCC until all
    entitlements have been assigned. Registering additional devices once
    the pool is depleted will result in the new device being assigned a
    provisional code (with local access to updates) The SMT server will
    notify the administrator that these new devices need to be
    entitled.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1170302"></a>12.1.5. Minimal Pattern</h3></div></div></div><p>
    The minimal pattern provided in YaST's Software Selection dialog
    targets experienced customers and should be used as a base for your
    own specific software selections.
   </p><p>
    Do not expect a minimal pattern to provide a useful basis for your
    business needs without installing additional software.
   </p><p>
    This pattern does not include any dump or logging tools. To fully
    support your configuration, Novell Technical Services (NTS) will
    request installation of all tools needed for further analysis in
    case of a support request.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1170318"></a>12.1.6. SPident</h3></div></div></div><p>
    SPident is a tool to identify the Service Pack level of the current
    installation. On SUSE Linux Enterprise Server 11 GA, this tool has been replaced by the
    new SAM tool (package "suse-sam").
   </p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="InfraPackArch.Performance"></a>12.2. Performance Related Information</h2></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311769"></a>12.2.1. AES-NI Instruction Set Extension Support in OpenSSL</h3></div></div></div><p><span class="emphasis"><em>Intel's AES-NI is a new set of Single Instruction Multiple Data (SIMD) instructions that is introduced in Intel(R) processor since 2009. These instructions enable fast and secure data encryption and decryption, using the Advanced Encryption Standard (AES), defined by FIPS Publication number 197.</em></span></p><p>This service pack adds patches to OpenSSL to support Intel's AES-NI.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1170175"></a>12.2.2. Linux Completely Fair Scheduler Affects Java Performance</h3></div></div></div><p>
    Problem (Abstract)
    </p><p>
     Java applications that use synchronization extensively might
     perform poorly on Linux systems that include the Completely Fair
     Scheduler.  If you encounter this problem, there are two possible
     workarounds.
    </p><p>Symptom</p><p>
     You may observe extremely high CPU usage by your Java application
     and very slow progress through synchronized blocks. The application
     may appear to hang due to the slow progress.
     </p><p>Cause</p><p>
      The Completely Fair Scheduler (CFS) was adopted into the mainline
      Linux kernel as of release 2.6.23. The CFS algorithm is different
      from previous Linux releases. It might change the performance
      properties of some applications. In particular, CFS implements
      sched_yield() differently, making it more likely that a thread
      that yields will be given CPU time regardless. More information on
      CFS can be found here: "Multiprocessing with the Completely Fair
      Scheduler",
     <a class="ulink" href="http://www.ibm.com/developerworks/linux/library/l-cfs/?ca=dgrlnxw06CFC4Linux" target="_top">http://www.ibm.com/developerworks/linux/library/l-cfs/?ca=dgrlnxw06CFC4Linux</a>
     </p><p>
      The new behavior of sched_yield() might adversely affect the
      performance of synchronization in the IBM JVM.
     </p><p>
      Environment
     </p><p>
      This problem may affect IBM JDK 5.0 and 6.0 (all versions) running
      on Linux kernels that include the Completely Fair Scheduler,
      including Linux kernel 2.6.27 in SUSE Linux Enterprise Server 11.
     </p><p>
      Resolving the Problem
     </p><p>
      If you observe poor performance of your Java application, there
      are two possible workarounds:
     </p><div class="itemizedlist"><ul type="disc"><li><p>Either invoke the JVM with the additional argument <code class="option">"-Xthr:minimizeUserCPU"</code>. </p></li><li><p>Or configure the Linux kernel to use the more
      backward-compatible heuristic for sched_yield() by setting the
      sched_compat_yield tunable kernel property to
      <code class="option">1</code>. For example:</p><pre class="screen">echo "1" &gt; /proc/sys/kernel/sched_compat_yield</pre></li></ul></div><p>
     You should not use these workarounds unless you are experiencing
     poor performance.
     </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1170379"></a>12.2.3. Tuning Performance of Simple Database Engines</h3></div></div></div><p>
    Simple database engines like Berkeley DB use memory mappings
    (mmap(2)) to manipulate database files. When the mapped memory is
    modified, those changes need to be written back to disk. In SUSE Linux Enterprise
    11, the kernel includes modified mapped memory in its calculations
    for deciding when to start background writeback and when to throttle
    processes which modify additional memory.  (In previous versions,
    mapped dirty pages were not accounted for and the amount of modified
    memory could exceed the overall limit defined.) This can lead to a
    decrease in performance; the fix is to increase the overall limit.
   </p><p>
    The maximum amount of dirty memory is 40% in SUSE Linux Enterprise 11 by
    default. This value is chosen for average workloads, so that enough
    memory remains available for other uses. The following settings may
    be relevant when tuning for database workloads:
   </p><div class="itemizedlist"><ul type="disc"><li><p>
         vm.dirty_ratio
        </p><p>
	Maximum percentage of dirty system memory (default 40).
        </p></li><li><p>
         vm.dirty_background_ratio
        </p><p>
         
         Percentage of dirty system memory at which background writeback
         will start (default 40).
        </p></li><li><p>
         vm.dirty_expire_centisecs
        </p><p>
         Duration after which dirty system memory is considered old
         enough to be eligible for background writeback (in
         centiseconds).
        </p></li></ul></div><p>These limits can be observed or modified with the sysctl
   utility (see sysctl(1) and sysctl.conf(5)).
   </p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="InfraPackArch.Storage"></a>12.3. Storage</h2></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311699"></a>12.3.1. Host Protected Area</h3></div></div></div><p>The host protected area (HPA), is an area of a hard drive that is not normally visible to an operating system and usually used by system vendors to store recovery data. The Linux kernel offers mechanisms to make the host protected area visible to the OS.</p><p>SUSE Linux Enterprise defaults to the host protected area being visible.</p><p>In rare cases this might be an unwanted setup (for example when using some RAID solutions etc.). In that case please use the option "Keep HPA" during installation or boot an already installed system using this kernel parameter:</p><pre class="screen">libata.ignore_hpa=0</pre><p>Note: Changing handling of host protected area for already installed systems may lead to data loss and should therefore be used with cautions.</p><p>Future SUSE Linux Enterprise releases will change the default to honor the host protected area.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-310783"></a>12.3.2. Allow Settable permission/ownership on mp devices from multipath.conf</h3></div></div></div><p><span class="emphasis"><em>Setting permissions/ownership on multipath devices is becoming a problem as raw devices are now deprecated in the Linux kernel and database systems such as Oracle. Setting permissions on raw devices is pretty straightforward as you can write udev rules for that. Doing the same for multipath devices is challenging since all you have at the udev level is dm-X as device name, but the associated WWID is not known.</em></span></p><p>To set Permission/Ownership on Multipath Devices, please copy the file "/usr/share/doc/packages/device-mapper/12-dm-permissions.rules" to /etc/udev/rules.d and adopt it to your needs. This file has four parts for different device type: PLAIN DM, LVM, ENCRYPTED, MULTIPATH. Add the parameters suitable to your envinronment here. Changes to udev rules might only become active after a reboot of the system.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1170358"></a>12.3.3. Multipathing - SCSI Hardware Handler</h3></div></div></div><p>
    Some storage devices, e.g. IBM DS4K, require special handling for
    path failover and failback. In SUSE Linux Enterprise Server 10 SP2, dm layer served as
    hardware handler.
   </p><p>
    One drawback of this implementation was that the underlying SCSI
    layer did not know about the existence of the hardware
    handler. Hence, during device probing, SCSI would send I/O on the
    passive path, which would fail after a timeout and also print
    extraneous error messages in the console.
   </p><p>
    In SUSE Linux Enterprise Server 11, this problem is resolved by moving the hardware
    handler to the SCSI layer, hence the term SCSI Hardware
    Handler. These handlers are modules created under the SCSI directory
    in the Linux Kernel.
   </p><p>
    In SUSE Linux Enterprise Server 11, there are four SCSI Hardware Handlers: scsi_dh_alua,
    scsi_dh_rdac, scsi_dh_hp_sw, scsi_dh_emc.
   </p><p>
    These modules need to be included in the initrd image so that SCSI
    knows about the special handling during probe time itself.
   </p><p>
    To do so, carry out the following steps:
   </p><div class="itemizedlist"><ul type="disc"><li><p>Add the device handler modules to the
    <code class="literal">INITRD_MODULES</code> variable in
    <code class="filename">/etc/sysconfig/kernel</code></p></li><li><p>Create a new initrd with:</p><pre class="screen">mkinitrd -k /boot/vmlinux-&lt;flavour&gt; \
-i /boot/initrd-&lt;flavour&gt;-scsi_dh \
-M /boot/System.map-&lt;flavour&gt;</pre></li><li><p>Update the <code class="filename">grub.conf/lilo.conf/yaboot.conf</code> file with the newly built initrd.</p></li><li><p>Reboot.</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1170506"></a>12.3.4. Multipathing: Failed Paths Do Not Return after a Path Failure.</h3></div></div></div><p>To work in a fully certified environment with all storage
   backend systems, fully supported by SUSE and your storage vendor,
   install at least
   <code class="filename">multipath-tools-0.4.8-40.2</code> or a later
   version. Appropriate packages are available as a maintenance update
   for SUSE Linux Enterprise 11.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1169946"></a>12.3.5. Local Mounts of iSCSI Shares</h3></div></div></div><p>
    An iSCSI shared device should never be mounted directly on the local
    machine. In an OCFS2 environment, doing so causes all hardware to
    hard hang.
   </p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="InfraPackArch.HyperV"></a>12.4. Hyper-V</h2></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1170210"></a>12.4.1. Change of Kernel Device Names in Hyper-V Guests</h3></div></div></div><p>
    SLES 11 SP2 has a newer block device driver, which presents all
    configured virtual disks as SCSI devices.  Disks, which used to
    appear as <code class="filename">/dev/hda</code> in SLES 11 SP1 will from now
    on appear as <code class="filename">/dev/sda</code>.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1170727"></a>12.4.2. Using the "Virtual Machine Snapshot" Feature</h3></div></div></div><p>
    The Windows Server Manager GUI allows to take snapshots of a Hyper-V
    guest.  After a snapshot is taken the guest will fail to reboot.  By
    default, the guest's root file system is referenced by the serial
    number of the virtual disk.  This serial number changes with each
    snapshot.  Since the guest expects the initial serial number,
    booting will fail.
   </p><p>
    The solution is to either delete all snapshots
    using the Windows GUI, or configure the guest to mount partitions by
    file system UUID.  This change can be made with the YaST partitioner
    and boot loader configurator.
   </p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="InfraPackArch.ArchIndependent"></a>12.5. Architecture Independent Information</h2></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="InfraPackArch.ArchIndependent.Package"></a>12.5.1. Changes in Packaging and Delivery</h3></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-312428"></a>12.5.1.1. Update Squid Web Proxy</h4></div></div></div><p><span class="emphasis"><em>With global IPv4 addresses getting scarce, the switch to IPv6 is inevitable and needs compatible software. Squid2 does not support IPv6.</em></span></p><p>Squid version 3.1 has been added, which provides native IPv6 support.</p><p> The configuration file /etc/squid/squid.conf has changed in an incompatible manner, some options do not exist anymore, others are not backward compatible.
For complete details on changes, refer to the Squid 3.1 release
  notes at <a class="ulink" href="http://www.squid-cache.org/Versions/v3/3.1/RELEASENOTES.html" target="_top">http://www.squid-cache.org/Versions/v3/3.1/RELEASENOTES.html</a> . </p><p>With SUSE Linux Enterprise 11 SP3, Squid 2.7 packages will be deprecated and unsupported.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-310294"></a>12.5.1.2. Update coreutils to Version 8.5</h4></div></div></div><p> For some locales the date format has changed from ISO date (YYYY-MM-DD) to a locale date format. Most significantly, this applies for the American English locale ( <code class="literal">LANG=en_US.UTF-8</code> ). This means SUSE Linux Enterprise 11 SP2 will output the date as SUSE Linux Enterprise 10 did and thus ensure long-term backwards compatibility. </p><p> To keep the ISO date, set the environment variable <code class="literal">export TIME_STYLE=long-iso</code> . </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="id1170761"></a>12.5.1.3. SUSE Linux Enterprise High Availability Extension 11</h4></div></div></div><p>
     With the <span class="emphasis"><em>SUSE Linux Enterprise High Availability Extension 11</em></span>, SUSE offers the most
     modern open source High Availability Stack for Mission Critical
     environments.
    </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="id1170666"></a>12.5.1.4. Kernel Has Memory Cgroup Support Enabled By Default</h4></div></div></div><p>
     While this functionality is welcomed in most environments, it
     requires about 1% of memory. Memory allocation is done at boot time
     and is using 40 Bytes per 4 KiB page which results in 1% of memory.
    </p><p>
     In virtualized environments, specifically but not exclusively on
     s390x systems, this may lead to a higher basic memory consumption:
     e.g., a 20GiB host with 200 x 1GiB guests consumes 10% of the real
     memory.
    </p><p>
     This memory is not swappable by Linux itself, but the guest cgroup
     memory is pageable by a z/VM host on an s390x system and might be
     swappable on other hypervisors as well.
    </p><p>
     Cgroup memory support is activated by default but it can be
     deactivated by adding the Kernel Parameter
     <code class="literal">cgroup_disable=memory</code>
    </p><p>
     A reboot is required to deactivate or activate this setting.
    </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="id1170342"></a>12.5.1.5. Live Migration of KVM Guest with Device Hot-Plugging</h4></div></div></div><p>
     Hot-plugging a device (network, disk) works fine for a KVM guest on
     a SLES 11 host since SP1.  However, migrating the same guest with the
     hotplugged device (available on the destination host) fails.
    </p><p>
     Since SLES 11 SP1, supports the hotplugging of the device to the
     KVM guest, but migrating the guest with the hot-plugged device is
     not supported and expected to fail.
    </p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="InfraPackArch.ArchIndependent.Security"></a>12.5.2. Security</h3></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="id1170574"></a>12.5.2.1. Removable Media</h4></div></div></div><p>To allow a specific user (&#8220;<span class="quote">joe</span>&#8221;) to mount
   removable media, run the following command as root:</p><pre class="screen">polkit-auth --user joe \
--grant org.freedesktop.hal.storage.mount-removable
   </pre><p>
    To allow all locally logged in users on the active console to mount
    removable media, run the following commands as root:
   </p><pre class="screen">echo 'org.freedesktop.hal.storage.mount-removable no:no:yes' \
  &gt;&gt; /etc/polkit-default-privs.local
/sbin/set_polkit_default_privs
</pre></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="id1170241"></a>12.5.2.2. Verbose Audit Records for System User Management Tools</h4></div></div></div><p>
     Install the package "pwdutils-plugin-audit". To enable this plugin, add "audit" to <code class="filename">/etc/pwdutils/logging</code>. See the &#8220;<span class="quote">Security Guide</span>&#8221;
 for more information.
    </p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="InfraPackArch.ArchIndependent.Network"></a>12.5.3. Networking</h3></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="id1170760"></a>12.5.3.1. Loading the mlx4_en Adapter Driver with the Mellanox ConnectX2 Ethernet Adapter</h4></div></div></div><p>
There is a reported problem that the Mellanox ConnectX2 Ethernet adapter
does not trigger the automatic load of the mlx4_en adapter driver. If
you experience problems with the mlx4_en driver not automatically
loading when a Mellanox ConnectX2 interface is available, create the
file <code class="filename">mlx4.conf</code> in the directory
<code class="filename">/etc/modprobe.d</code> with the following command:
   </p><pre class="screen">install mlx4_core /sbin/modprobe --ignore-install mlx4_core \
  &amp;&amp; /sbin/modprobe mlx4_en</pre></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="id1169135"></a>12.5.3.2. Using the System as a Router</h4></div></div></div><p>
     As long as the firewall is active, the option
     <code class="filename">ip_forwarding</code> will be reset by the firewall
     module. To activate the system as a router, the variable
     <code class="filename">FW_ROUTE</code> has to be set too. This can be done
     through <span class="command"><strong>yast2-firewall</strong></span> or manually.
    </p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="InfraPackArch.ArchIndependent.Cross"></a>12.5.4. Cross Architecture Information</h3></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="id1170589"></a>12.5.4.1. Myricom 10-Gigabit Ethernet Driver and Firmware</h4></div></div></div><p>
    SUSE Linux Enterprise 11 (x86, x86_64 and IA64) is using the Myri10GE driver from
    mainline Linux kernel. The driver requires a firmware file to be
    present, which is not being delivered with SUSE Linux Enterprise 11.
   </p><p>
    Download the required firmware at <a class="ulink" href="http://www.myricom.com" target="_top">http://www.myricom.com</a>.
   </p></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="InfraPackArch.x86_64_x86"></a>12.6. AMD64/Intel64 64-Bit (x86_64) and Intel/AMD 32-Bit (x86) Specific Information</h2></div></div></div><p></p><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-312141"></a>12.6.1. Support of new Intel processors</h3></div></div></div><p>This Service Pack will ensure support for the following new Intel processors:
1. The 2nd Generation Intel(R) Core&#8482; i7/i5/i3 processor family;
2. The 3rd Generation Intel(R) Core&#8482; processor family;
3. Intel(R) Xeon(R) processor E3-1200 series;
4. Intel(R) Xeon(R) processors E5-4600/2600/2400/1600 series;</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-312140"></a>12.6.2. Generic support for the PCI Express Gen3</h3></div></div></div><p><span class="emphasis"><em>Intel(R) Platforms based on Intel(R) Xeon(R) Processor E5-4600/2600/2400/1600 and Intel(R) C600 chipset product family will introduce PCI Express Gen3.</em></span></p><p>This Service Pack adds support for PCI Express Gen3 (ID-based Ordering, Latency Tolerance Reporting, Optimized Buffer Flush/Fill (OBFF)).</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-312084"></a>12.6.3. Support for new Intel(R) Platforms</h3></div></div></div><p>This Service Pack adds support for the following Intel(R) platforms:</p><p>
          </p><div class="itemizedlist"><ul type="disc"><li><p>Intel(R) platforms based on Intel(R) Xeon(R) Processor E3-1200 and Intel(R)
C200 chipset product family.</p></li><li><p>Intel(R) platforms based on Intel(R) Xeon(R) Processor E5-4600/2600/2400/1600
and Intel(R) C600 chipset product family.</p></li></ul></div><p>
        </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311820"></a>12.6.4. Support for Intel(R) Trusted Execution Technology (TXT)</h3></div></div></div><p><span class="emphasis"><em>Intel(R) TXT provides the solution of protecting IT infrastructure against software-based attacks within a server or PC at startup.</em></span></p><p>This Service Pack adds basic support for Intel(R) TXT by adding patches to the kernel and integrating tboot.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="InfraPackArch.x86_64_x86.System"></a>12.6.5. System and Vendor Specific Information</h3></div></div></div><p></p><div class="itemizedlist"><ul type="disc"><li><p>Boot Device Larger than 2 TiB</p><p>
     Due to limitations in the legacy x86/x86_64 BIOS implementations,
     booting from devices larger than 2 TiB is technically not possible
     using legacy partition tables (DOS MBR).
    </p><p>
     Since SUSE Linux Enterprise Server 11 Service Pack 1 we support installation and boot using uEFI
     on the x86_64 architecture and certified hardware.
    </p></li><li><p>
     i586 and i686 Machine with More than 16 GB of Memory
    </p><p>
     Depending on the workload, i586 and i686 machines with 16GB-48GB of
     memory can run into instabilities.  Machines with more than 48GB of
     memory are not supported at all. Lower the memory with the
     <code class="literal">mem=</code> kernel boot option.
    </p><p>
     In such memory scenarios, we strongly recommend using a x86-64
     system with 64-bit SUSE Linux Enterprise Server, and run the (32-bit) x86 applications on
     it.
    </p></li><li><p>
     Directly Addressable Memory on x86 Machines
    </p><p>
     When running SLES on an x86 machine, the kernel can only address
     896MB of memory directly. In some cases, the pressure on this
     memory zone increases linearly according to hardware resources such
     as number of CPUs, amount of physical memory, number of LUNs and
     disks, use of multipath, etc.
    </p><p>
     To workaround this issue, we recommend running an x86_64 kernel on
     such large server machines.
    </p></li><li><p>
     NetXen 10G Ethernet Expansion Card on IBM BladeCenter HS12 System
    </p><p>
     When installing SUSE Linux Enterprise Server 11 on a HS12 system with a "NetXen
     Incorporated BladeCenter-H 10 Gigabit Ethernet High Speed Daughter
     Card", the boot parameter <code class="literal">pcie_aspm=off</code> should
     be added.
    </p></li><li><p>NIC Enumeration</p><p>
     Ethernet interfaces on some hardware do not get enumerated in a way
     that matches the marking on the chassis.
    </p></li><li><p>HP Linux ProLiant Support Pack for SUSE Linux Enterprise Server 11</p><p>
     The hpilo driver is included in SUSE Linux Enterprise Server 11. Therefore, no hp-ilo
     package will be provided in the Linux ProLiant Support Pack for
     SUSE Linux Enterprise Server 11.
    </p><p>For more details, see Novell TID 700273.</p></li><li><p>HP High Performance Mouse for iLO Remote Console.</p><p>
     The desktop in SUSE Linux Enterprise Server 11 now recognizes the HP High Performance
     Mouse for iLO Remote Console and is configured to accept and
     process events from it.  For the desktop mouse and the HP High
     Performance Mouse to stay synchronized, it is necessary to turn off
     mouse acceleration. As a result, the HP iLO2 High-Performance mouse
     (hpmouse) package is no longer needed with SUSE Linux Enterprise Server 11 once one of
     the three following options are implemented.
    </p><div class="orderedlist"><ol type="1"><li><p>
       In a terminal run <code class="literal">xset m 1</code> &#8212; this
       setting will not survive a reset of the desktop.
      </p></li><li><p>
       (Gnome) In a terminal run <code class="literal">gconf-editor</code> and go
       to desktop-&gt;gnome-&gt;peripherals-&gt;mouse. Edit the "motion
       acceleration" field to be 1.
      </p><p>
       (KDE) Open "Personal Settings (Configure Desktop)" in the menu
       and go to "Computer
       Administration-&gt;Keyboard&amp;Mouse-&gt;Mouse-&gt;Advanced" and change
       "Pointer Acceleration" to 1.
      </p></li><li><p>
       (Gnome) In a terminal run "gnome-mouse-properties" and adjust the
       "Pointer Speed" slide scale until the HP High Performance Mouse
       and the desktop mouse run at the same speed across the screen.
       The recommended adjustment is close to the middle, slightly on
       the "Slow" side.
      </p></li></ol></div><p>
     After acceleration is turned off, sync the desktop mouse and the
     ILO mouse by moving to the edges and top of the desktop to line
     them up in the vertical and horizontal directions.  Also if the HP
     High Performance Mouse is disabled, pressing the &lt;Ctrl&gt; key
     will stop the desktop mouse and allow easier synching of the two
     pointers.
    </p><p>For more details, see Novell TID 7002735.</p></li><li><p>
     Missing 32-Bit Compatibility Libraries for libstdc++ and
     libg++ on 64-Bit Systems (x86_64)
    </p><p>
     32-bit (x86) compatibility libraries like
     "libstdc++-libc6.2-2.so.3" have been available on x86_64 in the
     package "compat-32-bit" with SUSE Linux Enterprise Server 9, SUSE Linux Enterprise Server 10, and are also
     available on the SUSE Linux Enterprise Desktop 11 medium (compat-32-bit-2009.1.19), but
     are not included in SUSE Linux Enterprise Server 11.
    </p><p>Background</p><p>
     The respective libraries have been deprecated back in 2001 and
     shipped in the compatibility package with the release of SUSE Linux Enterprise Server 9
     in 2004. The package was still shipped with SUSE Linux Enterprise Server 10 to provide a
     longer transition period for applications requiring the package.
    </p><p>
     With the release of SUSE Linux Enterprise Server 11 the compatibility package is no
     longer supported.
    </p><p>Solution</p><p>In an effort to enable a longer transition period for
applications still requiring this package, it has been moved to
the unsupported "Extras" channel. This channel is visible on every SUSE Linux Enterprise Server 11
system, which has been registered with the Novell Customer Center.
It is also mirrored via SMT alongside the supported and maintained
SUSE Linux Enterprise Server 11 channels.
    </p><p>Packages in the "Extras" channel are not supported or maintained.</p><p>The compatibility package is part of SUSE Linux Enterprise Desktop 11 due to a policy
    difference with respect to deprecation and deprecated packages as
compared to SUSE Linux Enterprise Server 11.
    </p><p>We encourage customers to work with SUSE and SUSE's partners
to resolve dependencies on these old libraries.</p></li><li><p>32-Bit Devel-Packages Missing from the Software Development Kit (x86_64)</p><p>Example: libpcap0-devel-32-bit package was available in Software Development Kit 10, but is missing from Software Development Kit 11</p><p>Background</p><p>SUSE supports running 32-bit applications on 64-bit architectures; respective runtime libraries are provided with SUSE Linux Enterprise Server 11 and fully supported. With SUSE Linux Enterprise 10 we also provided 32-bit devel packages on the 64-bit Software Development Kit. Having 32-bit devel packages and 64-bit devel packages installed in parallel may lead to side-effects during the build process. Thus with SUSE Linux Enterprise 11 we started to remove some  (but not yet all) of the 32-bit devel packages from the 64-bit Software Development Kit.</p><p>Solution</p><p>With the development tools provided in the Software Development Kit 11, customers and partners have two options to build 32-bit packages in a 64-bit environment (see below). Beyond that, SUSE's appliance offerings provide powerful environments for software building, packaging and delivery.</p><div class="itemizedlist"><ul type="circle"><li><p>Use the "build" tool, which creates a chroot environment for building packages.</p></li><li><p>The Software Development Kit contains the software used for the Open Build Service. Here the abstraction is provided by virtualization.</p></li></ul></div></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="InfraPackArch.x86_64_x86.Virtualization"></a>12.6.6. Virtualization</h3></div></div></div><p></p><div class="itemizedlist"><ul type="disc"><li><p>
     KVM
    </p><p>
     
     Since SUSE Linux Enterprise Server 11 SP1, KVM is fully supported on the x86_64
          architecture. KVM is designed around hardware virtualization
          features included in both AMD (AMD-V) and Intel (VT-x) CPUs
          produced within the past few years, as well as other
          virtualization features in even more recent PC chipsets and
          PCI devices. For example, device assignment using IOMMU and
          SR-IOV.
    </p><p>
     The following websites identify processors, which support
     hardware virtualization:
    </p><div class="itemizedlist"><ul type="circle"><li><p>
        http://wiki.xensource.com/xenwiki/HVM_Compatible_Processors
       </p></li><li><p>
             http://en.wikipedia.org/wiki/X86_virtualization
       </p></li></ul></div><p>
      The KVM kernel modules will not load if the basic hardware
      virtualization features are not present and enabled in the
      BIOS. If KVM does not start, please check the BIOS
      settings.
     </p><p>
      KVM allows for memory overcommit and disk space overcommit. It is
      up to the user to understand the impact of doing so. Hard errors
      resulting from exceeding available resources will result in guest
      failures.  CPU overcommit is supported but carries performance
      implications.
     </p><p>
      The following guest operating systems are supported:
     </p><div class="itemizedlist"><ul type="circle"><li><p>
        
        Starting with SLES 11 SP2,  Windows guest operating systems
        are fully supported on the KVM hypervisor, in addition to
        Xen. For the best experience, we recommend using
        WHQL-certified virtio drivers, which are part of SLE VMDP.
       </p><p>
             
             SUSE Linux Enterprise Server 11 SP1 and SP2 as fully virtualized. The following
             virtualization aware drivers are available: kvm-clock,
             virtio-net, virtio-block, virtio-balloon
            </p></li><li><p>
             
             
             SUSE Linux Enterprise Server 10 SP3 and SP4 as fully virtualized. The following
             virtualization aware drivers are available: kvm-clock,
             virtio-net, virtio-block, virtio-balloon
            </p></li><li><p>
             SUSE Linux Enterprise Server 9 SP4 as fully virtualized. For 32-bit kernel,
             specify <code class="literal">clock=pmtmr</code> on the Linux boot
             line; for 64-bit kernel, specify
             <code class="literal">ignore_lost_ticks</code> on the Linux boot
             line.
            </p></li></ul></div><p>
          For further details, see
          <code class="filename">/usr/share/doc/packages/kvm/kvm-supported.txt</code>.
         </p></li><li><p>
          VMI Kernel (x86, 32-bit only)
         </p><p>
          VMware, SUSE and the community improved the kernel
          infrastructure in a way that VMI is no longer
          necessary. Starting with SUSE Linux Enterprise Server 11 SP1, the separate VMI
          kernel flavor is obsolete and therefore has been dropped from
          the media. When upgrading the system, it will be automatically
          replaced by the PAE kernel flavor. The PAE kernel provides all
          features, which were included in the separate VMI kernel
          flavor.
         </p></li><li><p>CPU Overcommit and Fully Virtualized Guest</p><p>
          SUSE and our partners are currently evaluating reports that
          with CPU overcommit in place and under heavy load fully
          virtualized guests may become unresponsive or hang.
         </p><p>Paravirtualized guests work flawlessly with CPU overcommit
         under heavy load.
         </p><p>This problem is addressed with high priority. We will
         issue a maintenance update via <a class="ulink" href="http://support.novell.com/" target="_top">http://support.novell.com/</a> once this has been
         resolved.</p></li><li><p>IBM System X x3850/x3950 with ATI Radeon 7000/VE Video Cards and Xen Hypervisor</p><p>
          When installing SUSE Linux Enterprise Server 11 on IBM System X x3850/x3950 with ATI
          Radeon 7000/VE video cards, the boot parameter 'vga=0x317'
          needs to be added to avoid video corruption during the
          installation process.
         </p><p>
          Graphical environment (X11) in Xen is not supported on IBM
          System X x3850/x3950 with ATI Radeon 7000/VE video cards.
         </p></li><li><p>
          Video Mode Selection for Xen Kernels
         </p><p>In a few cases, following the installation of Xen, the
         hypervisor does not boot into the graphical environment. To
         work around this issue, modify
         <code class="filename">/boot/grub/menu.lst</code> and replace
         <code class="filename">vga=&lt;number&gt;</code> with
         <code class="filename">vga=mode-&lt;number&gt;</code>. For example, if
         the setting for your native kernel is vga=0x317, then for Xen
         you will need to use vga=mode-0x317.
         </p></li><li><p>
          Time Synchronization in Paravirtualized Domains with NTP.
         </p><p>
          Paravirtualized (PV) DomUs usually receive the time from the
          hypervisor. If you want to run "ntp" in PV DomUs, the DomU
          must be decoupled from the Dom0's time. At runtime, this is
          done with:
         </p><pre class="screen">echo 1 &gt; /proc/sys/xen/independent_wallclock</pre><p>
          To set this at boot time:
         </p><div class="orderedlist"><ol type="1"><li><p>either append "independent_wallclock=1" to kernel cmd line in DomU's grub configuration file </p></li><li><p>or append "xen.independent_wallclock = 1" to <code class="filename">/etc/sysctl.conf</code> in the DomU.</p></li></ol></div></li><li><p>If you encounter time synchronization issues with
         Paravirtualized Domains, we encourage you to use NTP.</p></li></ul></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311642"></a>12.6.6.1. VirtFS</h4></div></div></div><p><span class="emphasis"><em>Existing methods of exporting a file system from host to the guest include NFS and CIFS, which were not designed with virtualized environments in mind. There is need for a mechanism that provides faster access to exported file systems by exploiting the fact that the guest (client) is running on the same physical hardware as the host (server) that is exporting the file system.</em></span></p><p> SUSE Linux Enterprise Server 11 SP2 provides VirtFS, which is a new way to export file systems from the host and mount it on the QEMU/KVM guest.
VirtFS exploits virtio infrastructure provided by QEMU and hence provides the guest fast access to the exported file system. Conceptually, VirtFS is similar to running NFS server on the host and NFS mounting the exported file system on the guest. For more information about using VirtFS, refer to QEMU wiki at <a class="ulink" href="http://wiki.qemu.org/Documentation/9psetup" target="_top">http://wiki.qemu.org/Documentation/9psetup</a> . </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-310957"></a>12.6.6.2. Update to XEN Version 4.1.2</h4></div></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-310590-1"></a>12.6.6.3. Amazon EC2 Availability</h4></div></div></div><p>SUSE Linux Enterprise Server 11 SP2 is available immediately for use on Amazon Web Services EC2. For more information about Amazon EC2 Running SUSE Linux Enterprise Server, please visit http://aws.amazon.com/suse</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-308532"></a>12.6.6.4. Removing 32-Bit XEN Hypervisor</h4></div></div></div><p>With SLE 11 SP2, we removed the 32-bit hypervisor as a virtualization host. 32-bit virtual guests are not affected and are fully supported with the provided 64-bit hypervisor.</p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="InfraPackArch.x86_64_x86.RAS"></a>12.6.7. RAS</h3></div></div></div><p></p><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311831"></a>12.6.7.1. Support for Memory/CPU hotadd support at Intel(R) Xeon(R) Processor 7500 series-based Platforms</h4></div></div></div><p><span class="emphasis"><em>Today's business has come to rely on the uninterrupted availability of platforms and services, thus the feature of "Reliability", "Availability" and "Serviceability" (RAS) has been growing more and more critical to the real-time, always-on enterprise environment. 
Adding physical processors and memory to a running system without ending the Operation System or powering down the system is supported at Intel(R) Xeon(R) Processor 7500 series-based Platforms.</em></span></p><p>This Service Pack adds proper support for the mentioned Platforms.</p></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="InfraPackArch.IA64"></a>12.7. Intel Itanium (ia64) Specific Information</h2></div></div></div><p></p><div class="itemizedlist"><ul type="disc"><li><p>Installation on Systems with Many LUNs (Storage)</p><p>
    While the number of LUNs for a running system is virtually
    unlimited, we suggest not having more than 64 LUNs online while
    installing the system, to reduce the time to initialize and scan the
    devices and thus reduce the time to install the system in general.
   </p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="InfraPackArch.Power"></a>12.8. POWER (ppc64) Specific Information</h2></div></div></div><p></p><div class="itemizedlist"><ul type="disc"><li><p>Supported Hardware and Systems</p><p>
    All POWER3, POWER4, PPC970 and RS64&#8211;based models that were supported by SUSE Linux Enterprise Server 9 are no longer supported.
   </p></li><li><p>Misleading Buffer I/O Error Messages</p><p>
    During installation, you may encounter error messages such as:
   </p><pre class="screen">Buffer I/O error on device loop1, logical block 2632</pre><p>
These messages only occur during the initial setup, but the installation
will succeed nevertheless and newly created file systems will be fine.
   </p></li><li><p>
    Loading the Installation Kernel via Network on POWER
   </p><p>
     With SUSE Linux Enterprise Server 11 the bootfile <code class="filename">DVD1/suseboot/inst64</code>
     can not be
     booted directly via network anymore, because its size is larger
     than 12MB.  To load the installation kernel via network, copy the
     files <code class="filename">yaboot.ibm</code>, <code class="filename">yaboot.cnf</code>
     and <code class="filename">inst64</code> from the <code class="filename">DVD1/suseboot</code>
     directory to the TFTP server.
     Rename the <code class="filename">yaboot.cnf</code> file to
     <code class="filename">yaboot.conf</code>. yaboot can also load config files
     for specific Ethernet MAC addresses. Use a name like
     <code class="filename">yaboot.conf-01-23-45-ab-cd-ef</code> to match a MAC address.
     An example <code class="filename">yaboot.conf</code> for TFTP booting looks
     like this:
    </p><pre class="screen">default=sles11
timeout=100
image[64-bit]=inst64
    label=sles11
    append="quiet install=nfs://hostname/exported/sles11dir"
</pre><p>
    Note: This will not work on POWER4 systems. Their
    firmware only load files up to 12MB via TFTP.
   </p></li><li><p>
    Huge Page Memory Support on POWER
   </p><p>
    Huge Page Memory (16GB pages, enabled via HMC) is supported by the
    Linux kernel, but special kernel parameters must be used to enable
    this support. Boot with the parameters "<code class="literal">hugepagesz=16G
    hugepages=N</code>" in order to use the 16GB huge pages, where N
    is the number of 16GB pages assigned to the partition via the
    HMC. The number of 16GB huge pages available can not be changed once
    the partition is booted.  Also, there are some restrictions if huge
    pages are assigned to a partition in combination with eHEA / eHCA
    adapters:
   </p><p>
     IBM eHEA Ethernet Adapter:
   </p><p>
     The eHEA module will fail to initialize any eHEA ports if huge pages
     are assigned to the partition and Huge Page kernel parameters are
     missing. Thus, no huge pages should be assigned to the partition during
     a network installation.  To support huge pages after installation, the
     huge page kernel parameters need to be added to the boot loader
     configuration before huge pages are assigned to the partition.
   </p><p>
     IBM eHCA InfiniBand Adapter:
   </p><p>
     The current eHCA device driver is not compatible with huge pages. If
     huge pages are assigned to a partition, the device driver will fail to
     initialize any eHCA adapters assigned to the partition.
   </p></li><li><p>
    Installation on POWER onto IBM VSCSI Target
   </p><p>
    The installation on a vscsi client will fail with old versions of
    the AIX VIO server. Please upgrade the AIX VIO server to version
    1.5.2.1-FP-11.1 or later.
   </p></li><li><p>
    iSCSI Installations with Multiple NICs Losing
    Network Connectivity at the End of Firstboot Stage
   </p><p>
    After installing SLES 11 SP1 on an iSCSI target, the system boots
    properly, network is up and the iSCSI root device is found as
    expected. The install completes (firstboot part) as usual. However,
    at the end of firstboot, the network is shut down before the root
    file system is unmounted, leading to read failures accessing the root
    (iSCSI) device; the system hangs.
   </p><p>
    Solution: reboot the system.
   </p></li><li><p>IBM Linux VSCSI Server Support in SUSE Linux Enterprise Server 11</p><p>
    Customers using SLES 9 or SLES 10 to serve Virtual SCSI to other
    LPARs, using the ibmvscsis driver, who wish to migrate from these
    releases, should consider migrating to the IBM Virtual I/O server.
    The IBM Virtual I/O server supports all the IBM PowerVM virtual I/O
    features and also provides integration with the Virtual I/O
    management capabilities of the HMC.  It can be downloaded from:
    <a class="ulink" href="http://www14.software.ibm.com/webapp/set2/sas/f/vios/download/home.html" target="_top">http://www14.software.ibm.com/webapp/set2/sas/f/vios/download/home.html</a>
   </p></li><li><p>
    Virtual Fibre Channel Devices
   </p><p>
    When using IBM Power Virtual Fibre Channel devices utilizing N-Port
    ID Virtualization, the Virtual I/O Server may need to be updated in
    order to function correctly. Linux requires VIOS 2.1, Fixpack 20.1,
    and the LinuxNPIV I-Fix for this feature to work properly.  These
    updates can be downloaded from:
    <a class="ulink" href="http://www14.software.ibm.com/webapp/set2/sas/f/vios/home.html" target="_top">http://www14.software.ibm.com/webapp/set2/sas/f/vios/home.html</a>
   </p></li><li><p>Virtual Tape Devices</p><p>
    When using virtual tape devices served by an AIX VIO server, the
    Virtual I/O Server may need to be updated in order to function
    correctly. The latest updates can be downloaded from:
    <a class="ulink" href="http://www14.software.ibm.com/webapp/set2/sas/f/vios/home.html" target="_top">http://www14.software.ibm.com/webapp/set2/sas/f/vios/home.html</a>
   </p></li><li><p>For More Information</p><p>
    For more information about IBM Virtual I/O Server, see
    <a class="ulink" href="http://www14.software.ibm.com/webapp/set2/sas/f/vios/documentation/home.html" target="_top">http://www14.software.ibm.com/webapp/set2/sas/f/vios/documentation/home.html</a>.
   </p></li><li><p>Chelsio cxgb3 iSCSI Offload Engine</p><p>
    The Chelsio hardware supports ~16K packet size (the exact value
    depends on the system configuration). It is recommended that you set
    the parameter <code class="filename">MaxRecvDataSegmentLength</code> in
    <code class="filename">/etc/iscsid.conf</code> to 8192.
   </p><p>
    For the cxgb3i driver to work properly, this parameter needs to be
    set to 8192.
   </p><p>
    In order to use the cxgb3i offload engine, the cxgb3i module needs
    to be loaded manually after open-scsi has been started.
   </p><p>
    For additional information, refer to
    <code class="filename">/usr/src/linux/Documentation/scsi/cxgb3i.txt</code> in
    the kernel source tree.
   </p></li><li><p>Known TFTP Issues with Yaboot</p><p>When attempting to netboot yaboot, users may see the following error message:</p><pre class="screen">Can't claim memory for TFTP download (01800000 @ 01800000-04200000)</pre><p>and the netboot will stop and immediately display the yaboot "boot:" prompt.
 Use the following steps to work around the problem.</p><div class="itemizedlist"><ul type="circle"><li><p>Reboot the system and at the IBM splash screen select '8' to
   get to an Open Firmware prompt "0&gt;"</p></li><li><p>At the Open Firmware prompt, type the following commands:</p><pre class="screen">setenv load-base 4000
setenv real-base c00000
dev /packages/gui obe
</pre></li><li><p>The second command will take the system back to the IBM splash
   screen and the netboot can be attempted again.</p></li></ul></div></li><li><p>Graphical Administration of Remotely Installed Hardware</p><p>If you do a remote installation in text mode, but want to
   connect to the machine later in graphical mode, be sure to set the
   default runlevel to 5 via YaST. Otherwise xdm/kdm/gdm might not be
   started.</p></li><li><p>InfiniBand - SDP Protocol Not Supported on IBM Hardware</p><p>To disable SDP on IBM hardware set <code class="literal">SDP=no</code> in
   openib.conf so that by default SDP is not loaded. After you have set
   this setting in openib.conf to 'no' run <span class="command"><strong>openibd
   restart</strong></span> or reboot the system for this setting to take
   effect.</p></li><li><p>RDMA NFS Server May Hang During Shutdown (OFED)</p><p>
    If your system is configured as an NFS over RDMA server, the system
    may hang during a shutdown if a remote system has an active NFS over
    RDMA mount. To avoid this problem, prior to shutting down the
    system, run "openibd stop"; run it in the background, because the
    command will hang and otherwise block the console:
   </p><pre class="screen">/etc/init.d/openibd stop &amp;</pre><p>
    A shutdown can now be run cleanly.
    
  
  
   </p><p>Note: the steps to configure and start NFS over RDMA are as follows:</p><div class="itemizedlist"><ul type="circle"><li><p>On the server system:</p><div class="orderedlist"><ol type="1"><li><p>Add an entry to the file <code class="filename">/etc/exports</code>, for example:</p><pre class="screen">/home   192.168.0.34/255.255.255.0(fsid=0,rw,async,insecure,no_root_squash)</pre></li><li><p>As the root user run the commands:</p><pre class="screen">/etc/init.d/nfsserver start
echo rdma 20049 &gt; /proc/fs/nfsd/portlist</pre></li></ol></div></li><li><p>On the client system:</p><div class="orderedlist"><ol type="1"><li><p>Run the command:  <span class="command"><strong>modprobe xprtrdma</strong></span>.</p></li><li><p>Mount the remote file system using the command
       <span class="command"><strong>/sbin/mount.nfs</strong></span>.  Specify the ip address of
       the ip-over-ib network interface (ib0, ib1...) of the server and
       the options: <code class="option">proto=rdma,port=20049</code>, for
       example:</p><pre class="screen">/sbin/mount.nfs 192.168.0.64:/home /mnt \
-o proto=rdma,port=20049,nolock</pre></li></ol></div></li></ul></div></li></ul></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-312950"></a>12.8.1. Suspend and Resume Support</h3></div></div></div><p><span class="emphasis"><em>IBM Power 7 systems running firmware 7.2.0 SP1 or later along with version 2.2.0.11-FP24 SP01 or later of the Virtual I/O Server and HMC v7r7.2.0 or later include support for long term suspension of logical partitions. Logical partitions can be suspended and resumed from the HMC. All I/O resources must be virtual I/O resources at the time of suspending. Once suspended, the memory and processor resources associated with the suspended logical partition are free to be used by  other logical partitions.</em></span></p><p>SLES 11 SP2 has been enhanced to support logical partition suspend and resume.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311687"></a>12.8.2. Capture Oops and Panic Reports to NVRAM</h3></div></div></div><p>The kernel is able to capture the most recent oops or panic report from the dmesg buffer into NVRAM, where it can be examined after reboot.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311672"></a>12.8.3. Page Hinting for Active Memory Deduplication</h3></div></div></div><p><span class="emphasis"><em>PowerVM release 7.4 includes a new memory optimization feature called Active Memory Deduplication. This feature applies to logical partitions which are assigned to an Active Memory Sharing (AMS) pool. With Active Memory Deduplication, the PowerVM Hypervisor automatically detects memory pages in the pool that have identical contents, and remaps those pages to a single physical page, freeing up the duplicate pages for other purposes in the AMS pool.</em></span></p><p>SLES 11 SP2 has been enhanced to provide the PowerVM Hypervisor with page hints to indicate which pages are good candidates for merging. This feature is automatically enabled in the kernel for AMS LPARs with Active Memory Deduplication enabled. Statistics on page merging is available through the amsstat utility.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311669"></a>12.8.4. IBM Power Virtual Fibre Channel Driver Update</h3></div></div></div><p>The virtual fibre channel for IBM Power systems has been updated to support the 5729 PCIe 4-Port 8Gb FC adapter.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311656"></a>12.8.5. ITrace Package Removed</h3></div></div></div><p>The ppc64-specific instruction tracing tool, ITrace, is no longer available.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311650"></a>12.8.6. IBM Power Virtual Ethernet Driver Update</h3></div></div></div><p>The IBM Power Virtual Ethernet driver (ibmveth) has been updated with various performance enhancements, including support for IPv6 checksum offload.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311630"></a>12.8.7. IBM Power Shared Storage Pools</h3></div></div></div><p><span class="emphasis"><em> A shared storage pool is a server based storage virtualization that is clustered and is an extension of existing storage virtualization on the Virtual I/O Sever for IBM Power systems. Support for shared storage pools requires the latest Virtual I/O Server software, which can be obtained from </em></span><a class="ulink" href="http://www14.software.ibm.com/webapp/set2/sas/f/vios/home.html" target="_top"><span class="emphasis"><em>http://www14.software.ibm.com/webapp/set2/sas/f/vios/home.html</em></span></a><span class="emphasis"><em> . </em></span></p><p>SLES 11 SP2 adds multipath support for virtual disks backed by shared storage pools.</p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="InfraPackArch.SystemZ"></a>12.9. System z (s390x) Specific Information</h2></div></div></div><p>
  More information, see
  <a class="ulink" href="http://www.ibm.com/developerworks/linux/linux390/documentation_novell_suse.html" target="_top">http://www.ibm.com/developerworks/linux/linux390/documentation_novell_suse.html</a>.
 </p><p>
IBM zEnterprise 196 (z196) and IBM zEnterprise 114 (z114) further on
referred to as z196 and z114.
 </p><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-311678"></a>12.9.1. Libdfp updated to version 1.0.7</h3></div></div></div><p>Updated version with several corrections: previous versions of libdfp exhibited minor bugs in printf_dfp and strtod[32|64|128], inconsistencies with POSIX with regard to classification functions, dfp header override include order problems, and missing classification function exports.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-309479"></a>12.9.2. Suspend to Disk for System z</h3></div></div></div><p><span class="emphasis"><em>Fast shutdown and resume of Linux for System z in z/VM and LPAR.</em></span></p><p>Suspend to disk allows fast suspend (freeze) of a system and resume work where it stopped.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="InfraPackArch.SystemZ.Hardware"></a>12.9.3. Hardware</h3></div></div></div><p></p><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311854"></a>12.9.3.1. Performance indicator bytes</h4></div></div></div><p>Two new fields in /proc/sysinfo now export the contents of Capacity-Change Reason (CCR) and Capacity-Adjustment Indication (CAI) of SYSIB 1.1.1 introduced by the IBM zEnterprise. They provide additional information for enhanced problem analysis.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311860"></a>12.9.3.2. z196 and z114 enhanced node affinity support</h4></div></div></div><p>The System z196 &amp; z114 hardware adds another level to the CPU cache hierarchy. Enhancements have been added to allow more efficient task scheduling to optimize the Linux scheduler, increases cache hits and therefore overall performance.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311845"></a>12.9.3.3. Exploitation of System z10 prefetching instructions</h4></div></div></div><p>z10 has added more complexity for memory accesses and a faster processor. Pre-fetching instructions can be used to enhance memory access like all sorts of implementations of copying memory, zeroing out memory and predictable loops resulting in increased performance and better exploitation of the System z hardware. Requires System z optimizations from GCC 4.6 (available on the SDK).</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311873"></a>12.9.3.4. Access to raw ECKD data from Linux</h4></div></div></div><p>In raw-track access mode, the DASD device driver accesses full ECKD tracks, including record zero and the count and key data fields. With this mode, Linux can access an ECKD device regardless of the track layout. In particular, the device does not need to be formatted for Linux. This includes Linux ECKD disks that are used with LVM, Linux ECKD disks that are used directly, and z/OS ECKD disks.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-312955"></a>12.9.3.5. Oprofile System z10 Hardware Customer Mode Sampling</h4></div></div></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="InfraPackArch.SystemZ.Virtualization"></a>12.9.4. Virtualization</h3></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311855"></a>12.9.4.1. Deliver z/VM CP Special Messages to Userspace Using udev Events (uevents)</h4></div></div></div><p>This feature provides a new kernel device driver for receiving z/VM CP special messages (SMSG) and delivering these messages to user space as udev events (uevents). The device driver registers with the existing CP special message device driver to only receive messages starting with "APP". The created uevents contain message sender and content as environmental data.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311858"></a>12.9.4.2. s390-tools: cmsfs read and write support</h4></div></div></div><p>A CMS minidisk can be mounted to Linux (cmsfs-fuse). The files on the minidisk can now be accessed by common Linux tools. Text files and configuration files can be accessed and automatically converted from EBCDIC to ASCII without eg. the restriction to shutdown Linux before access. cmsfs-fuse support for CMS file systems is limited to EDF, other CMS file systems like SFS, CFS and BFS are not supported. This feature is used to eg. provide config data and personalization to Linux guest in a HA/DR scenario (machine, LPAR, guest name, IP addr data, etc).</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311758"></a>12.9.4.3. snIPL: Tool to trigger SCSI dump on remote container</h4></div></div></div><p>This feature enhances snIPL to take a remote SCSI dump using the snIPL interface.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-312069"></a>12.9.4.4. s390-tools: Improve memory ballooning with cpuplugd</h4></div></div></div><p>Large scale server consolidation requires a way to deal with limited memory resources. Ideally this is done by the hypervisor or by optimizing the individual guest in terms of memory utilization. 'cpuplugd' has a rule based scheme to control the size of the CMM1 memory balloon. An enhanced default set of rules allows the administrator to define a virtual machine with a larger memory size and have cpuplugd deal with the surplus automatically.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311909"></a>12.9.4.5. snIPL support for z/VM 6</h4></div></div></div><p>snIPL offers command line support for remote system management of LPARs and z/VM . This feature offers socket-based (AF_INET) remote system management of z/VM 6 guests with snipl and stonith if SMAPI support is available.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="id1172691"></a>12.9.4.6. Support of Live Guest Relocation (LGR) with z/VM 6.2 on SLES 11 SP2</h4></div></div></div><p>
Live guest relocation (LGR) with z/VM 6.2 on SLES 11 SP2 requires z/VM service
applied, especially with Collaborative Memory Management (CMMA) active
(<code class="literal">cmma=on</code>).
   </p><p>
Apply z/VM APAR  VM65134.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="id1171429"></a>12.9.4.7. Linux Guests Running on z/VM 5.4 and 6.1 Require z/VM Service
   Applied</h4></div></div></div><p>
    Linux guests using dedicated devices may experience a loop, if an
    available path to the device goes offline prior to the IPL of Linux.
   </p><p>
     Apply recommended z/VM service APARs VM65017 and VM64847
    </p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="InfraPackArch.SystemZ.Storage"></a>12.9.5. Storage</h3></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311756"></a>12.9.5.1. FICON, s390-tools: Additional device characteristics for Solid State Device displayed with dasdview</h4></div></div></div><p>Storage servers may provide solid state disks, which are transparent in use to the DASD device driver. A new flag in the device characteristics will show if a device is a solid state disk. The device characteristics are already exported per ioctl and can be read as binary data with the dasdview tool.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311760"></a>12.9.5.2. FICON: Dynamic PAV toleration</h4></div></div></div><p>The DASD device driver tolerates dynamic Parallel Access Volume (PAV) changes for base PAV. PAV changes in the hardware configuration are detected and the mapping of base and alias devices in Linux is adjusted accordingly. The user is informed about the change by a kernel message with log level info.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311870"></a>12.9.5.3. FICON: Multi-Track extensions for High Performance FICON</h4></div></div></div><p>Enables the DASD device driver to generate multi-track High Performance FICON (zHPF) requests. If the  storage systems support multi-track High Performance FICON requests, read or write data can be done to more than one track to enhance I/O performance.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311875"></a>12.9.5.4. zHW: Store I/O Operation Status and initiate logging (SIOSL)</h4></div></div></div><p>Logging I/O subchannel status information: a Linux interface for the store-I/O-operation-status-and-initiate-logging (SIOSL) CHSC command and its exploitation by the FCP device driver. It enhances the service toolset for determining field scenarios without interrupting operation, and can be used to synchronize log gathering between the operating system and the channel firmware.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311878"></a>12.9.5.5. Automatic detection of read only DASDs</h4></div></div></div><p>This feature prevents unintentional write requests and subsequent I/O errors, by detecting if a z/VM attached device is read-only using the z/VM DIAG 210 interface and setting the respective Linux block device to read-only as well.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311880"></a>12.9.5.6. Tunable default grace period for missing interrupts in DASD device driver</h4></div></div></div><p>This provides a new sysfs interface to specify the timeout for missing interrupts for standard I/O operations. The default value for this timeout was 300 seconds for standard ECKD and FBA I/O operations and 50 seconds for DIAG I/O operations. For ECKD devices the timeout value provided from the storage server is used as default instead of the generic 300 seconds. The timeout value can be read and set through a new DASD device attribute 'expires' in sysfs.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311905"></a>12.9.5.7. FICON: API &amp; tool to query DASD reservation status</h4></div></div></div><p>Allows the DASD device driver to determine the reservation status of a given DASD in relation to the current Linux instance.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311915"></a>12.9.5.8. FCP: End-To-End data consistency checking</h4></div></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311921"></a>12.9.5.9. s390-tools: Additional partition types supported by DASD tools</h4></div></div></div><p>This feature introduces new partition types like RAID and LVM to the Linux dasd tools beside the existing support for "Linux native" and "swap" partitions types.</p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="InfraPackArch.SystemZ.Network"></a>12.9.6. Network</h3></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311871"></a>12.9.6.1. qeth device driver: offload outbound checksumming to OSA hardware</h4></div></div></div><p>This feature introduces OSA adapter support for the checksum calculations which TCP and UDP use to ensure data integrity. Offloading this calculation to the OSA adapter (HW) will reduce the processor load compared to the current implementation where it is done in SW.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-312067"></a>12.9.6.2. s390-tools: Enhancements in the configuration tool for System z network devices</h4></div></div></div><p>This feature enhances the qethconf tool by providing improved information messages.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311898"></a>12.9.6.3. s390-tools: YaST Allows to Configure LLADDR for Network Devices of Type OSX and OSM But
Should Not</h4></div></div></div><p><span class="emphasis"><em>zEnterprise Unified Resource Manager is responsible for OSX- and OSM-setup.
It defines MAC-addresses for OSX and OSM devices.
The qeth driver retrieves those MAC-addresses during activation of OSX and OSM devices.
They must not be changed afterwards.
This means the YaST-created ifcfg-files must not contain an LLADDR-definition.</em></span></p><p>Remove the LLADDR entry from the ifcfg configuration file for an OSX- or OSM-device.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311890"></a>12.9.6.4. NAPI support for qeth and qdio</h4></div></div></div><p>This feature adapts qeth to the standard Linux kernel network interface: NAPI. The qdio interface is extended to allow direct processing of inbound data in qeth. Using NAPI, the device driver can disable interrupts to reduce CPU load under high network traffic. It provides increased throughput and less CPU consumption for high speed network connections.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311892"></a>12.9.6.5. Optimized Latency Mode (OLM) toleration</h4></div></div></div><p>This feature enhances the qeth driver with a meaningful message for the case that an OSA-connection fails due to an active OLM-connection on the shared OSA-adapter. OLM may be activated by z/OS on an OSA Express3 adapter, which reduces the number of allowed concurrent connections, if adapter is used in shared mode.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311922"></a>12.9.6.6. s390-tools: IPv6 support for qetharp tool</h4></div></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311899"></a>12.9.6.7. Assisted VLAN null tagging support</h4></div></div></div><p>This feature exploits OSA support for VLAN tagging and null tagging (VLAN ID 0 can be used in tags). Such frames can carry priority information and improve the communication capabilities with z/OS.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="id1173036"></a>12.9.6.8. Limitations with the "qetharp" Utility</h4></div></div></div><div class="variablelist"><dl><dt><span class="term"><span class="command"><strong>qetharp -d</strong></span></span></dt><dd><p>
      An ARP entry, which is part of Shared OSA should not get deleted
      from the arp cache.
     </p><p>
      Current Behaviour: An ARP entry, which is part of shared OSA is
      getting deleted from the arp cache.
     </p></dd><dt><span class="term"><span class="command"><strong>qetharp -p</strong></span></span></dt><dd><p>
       Purge - It should remove all the remote entries, which are not part
       of shared OSA.
      </p><p>
       Current Behaviour: It is only flushing out the remote entries,
       which are not part of shared OSA for first time. Then, if the
       user pings any of the purged ip address, the entry gets added
       back to the arp cache. Later, if the user runs purge for a second
       time, that particular entry is not getting removed from the arp
       cache.
      </p></dd></dl></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="InfraPackArch.SystemZ.Security"></a>12.9.7. Security</h3></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311924"></a>12.9.7.1. z196 / z114: CP ACF exploitation - kernel and libica</h4></div></div></div><p>This feature adds support to the kernel and libica to exploit new algorithms from Message Security Assist (CPACF) extension 4.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311901"></a>12.9.7.2. z196 / z114: Support for 4096 bit RSA FastPath</h4></div></div></div><p>This feature extends the support for current hardware acceleration of RSA encryption and decryption from 2048-bit keys to the new maximum of 4096-bit keys in zcrypt Linux device driver. This new support will allow to handle with a zEnterprise Crypto Express3 card RSA mod expo operations with 4096-bit RSA keys in ME (Modulus Exponent) and CRT (Chinese Remainder Theorem) format.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311665"></a>12.9.7.3. openCryptoki</h4></div></div></div><p><span class="emphasis"><em>Exploit z196 hardware accelerated crypto algorithms and Elliptic Curve cryptography features of the IBM PCIe Cryptographic Coprocessor.</em></span></p><p>Added support for new CPACF algorithms in z196, AES-CTR mode for key lengths 128, 192 and 256. Also added support for Elliptic Curve crypto for customers with the IBM PCIe Cryptographic Coprocessor.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="bnc#721253"></a>12.9.7.4. Existing Data Execution Protection Removed for System z</h4></div></div></div><p>
The existing data execution protection for Linux on System z relies on the
System z hardware to distinguish instructions and data through the secondary
memory space mode. As of System z10, new load-relative-long instructions do not
make this distinction. As a consequence, applications that have been compiled
for System z10 or later fail when running with the existing data execution
protection.
   </p><p>
Therefore, data execution protection for Linux on System z has been removed.
   </p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="InfraPackArch.SystemZ.RAS"></a>12.9.8. RAS</h3></div></div></div><p></p><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-312953"></a>12.9.8.1. Automated LUN scanning (NPIV only)</h4></div></div></div><p>For FCP subchannels running in NPIV mode, this features allows the Linux SCSI midlayer to scan and automatically attach SCSI devices that are available for the NPIV WWPN. The manual configuration of LUNs in zfcp is now only required for non-NPIV FCP subchannels. With this feature the behaviour of zfcp in NPIV mode is now similar to all other Linux SCSI drivers.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311850"></a>12.9.8.2. cio: provide userspace handle to wait for pending work</h4></div></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311764"></a>12.9.8.3. s390-tools: New "hyptop" tool for dynamic real-time view of a hypervisor environment on System z</h4></div></div></div><p>This feature provides the kernel infrastructure needed for a Linux tool called "hyptop" which provides a dynamic real-time view of a System z hypervisor environment. It works with either the z/VM or the LPAR hypervisor. Depending on the available data it shows for example CPU and memory consumption of active LPARs or z/VM guests. It provides a curses based user interface similar to the popular Linux "top" command.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311853"></a>12.9.8.4. Improved QDIO Performance Statistics</h4></div></div></div><p>Upgrading from SUSE Linux Enterprise Server 11 SP1 to SP2 does not preserve the qdio performance statistics under /proc/qdio_perf. The corresponding file /sys/bus/ccw/qdio_performance_stats is also removed.  SP2 adds support for qdio performance statistics by device. These statistics are located under &lt;debugfs mount point&gt;/qdio/&lt;device bus id&gt;/statistics. Writing 1 to the statistics file of a qdio device starts the collection of performance data for that device. Writing 0 to the statistics file of a qdio device stops the collection of performance data for that device. By default the statistics are disabled. For more information, see Chapter 8 of "Device Drivers, Features, and Commands on SUSE Linux Enterprise Server 11 SP2".</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311851"></a>12.9.8.5. Breaking-event-address for Userspace Programs</h4></div></div></div><p>This feature records breaking-event-addresses for user space processes using the PER-3 facility introduced with z10. There is one restriction in regard to the useable address range for the user space program. Any breaking-event in the range from 0 to 8MB will not be recorded. Useful for application development.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311861"></a>12.9.8.6. s390-tools: Enhanced re-IPL tool, chreipl</h4></div></div></div><p>This feature provides four extensions to the chreipl tool: a) add support to re-IPL from device-mapper devices, including mirror devices and multipath devices, b) add support to re-IPL from named saved systems (NSS), c) add support to specify additional kernel parameters for the next re-IPL, d) add "auto target" support. This improves the usability experience, by enhancing and simplifying the interface to setup how and what to reboot.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311877"></a>12.9.8.7. s390-tools: zipl tool automatically calculates Boot Device Ramdisk Address</h4></div></div></div><p>This feature will relax the need for a default address for the initial ramdisk on the boot device. The address is now calculated dependent on the locations of the other components.  If the user provides an initrd_addr then this one is used. If the user does not provide an initrd_addr then - instead of a fixed value (0x800000) - a suitable calculated value is used.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311908"></a>12.9.8.8. zipl automatic menu support</h4></div></div></div><p>This feature adds support for automatic menu generation to IBM's zipl package.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311876"></a>12.9.8.9. cio: resume handling for reordered devices</h4></div></div></div><p>Improves cio resume handling to cope with devices that were attached on different subchannels prior to the suspend operation.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311913"></a>12.9.8.10. cio: handle channel path description changes</h4></div></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311874"></a>12.9.8.11. Unit Check handling</h4></div></div></div><p>This feature improves handling of unit checks reported during CIO-internal operations. Control units such as the DS8000 storage server are using Unit Checks as a means to inform Linux of events which may affect the operational state of the devices provided.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311911"></a>12.9.8.12. CHPID reconfiguration handling</h4></div></div></div><p>Enhancements in the common I/O layer (CIO) that enable Linux in LPAR installation to handle dynamic IODF changes in the channel-path related setup and changed capabilities of channel paths, eg. the number of inbound/outbound queues of an OSA adapter or the maximum transmission unit.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311759"></a>12.9.8.13. FICON: IPL &amp; device discovery hardening</h4></div></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311906"></a>12.9.8.14. FICON: Improve handling of lost device reservations</h4></div></div></div><p>Allows to specify a policy for the DASD device driver behavior in case of a lost device reservation. The policy can be specified via a new DASD sysfs attribute reservation_policy. Possible values are: ignore, fail.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311757"></a>12.9.8.15. Dump on panic - Prevent re-IPL loop</h4></div></div></div><p>This feature provides tooling of a configurable time delay (activation of this trigger). A new keyword DELAY_MINUTES is introduced in the dumpconf configuration file. Using this keyword the activation of dumpconf can be delayed in order to prevent potential re-IPL loops.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311897"></a>12.9.8.16. makedumpfile support: convert Linux on z dumps to ELF</h4></div></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311895-1"></a>12.9.8.17. Remove Support for Multi-Volume Tape Dumps</h4></div></div></div><p>The multi-volume tape dump support will be removed from zipl and zgetdump. The reason for this decision is that current tape cartridges have hundreds of gigabyte capacity and therefore the multi-volume support is not needed any more.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-312296"></a>12.9.8.18. Tool to safely start getty through init</h4></div></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-312954"></a>12.9.8.19. OSA concurrent software/hardware trap</h4></div></div></div><p>This feature enables collective problem analysis through consolidated dumps of software and hardware. A command can be used to generate qeth/qdio trace data as well as trigger the internal dump of an OSA device.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311920"></a>12.9.8.20. CPC name represented in sysfs</h4></div></div></div><p>This feature enables for dynamic changes in the GDPS environment definition to avoid possible failures from manual or non applied changes.  GDPS changed to retrieve CPC and LPAR information dynamically - with the new function, GDPS is now able to always reset exactly the LPAR in which the OS is running.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311928"></a>12.9.8.21. Extend and Improve zFCP trace utilities</h4></div></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311654"></a>12.9.8.22. Crash Utility support to read compressed/filtered dumpfile generated by makedumpfile for s390x</h4></div></div></div><p><span class="emphasis"><em>s390x kernel dumps may now be filtered by the makedumpfile tool. The crash dump analysis tool must be able to analyze these filtered dumps.</em></span></p><p>The crash dump analysis tool was modified to recognize Linux on System z dumps filtered by makedumpfile</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311641"></a>12.9.8.23. makedumpfile for Linux on System z</h4></div></div></div><p>Linux on System z kernel crash dumps have traditionally not been in ELF core format. We now have infrastructure to convert the Linux on System z dumps to ELF core format. 'makedumpfile' can be used to compress system dumps by filtering out memory pages like free, user space or cache pages that are not necessary for dump analysis. Additionally, the 'crash' utility has been enhanced to read compressed/filtered s390x dumpfiles generated by 'makedumpfile'.</p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="InfraPackArch.SystemZ.Performance"></a>12.9.9. Performance</h3></div></div></div><p></p><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311917"></a>12.9.9.1. Optimized qeth default settings</h4></div></div></div><p>This feature delivers optimized default settings for several qeth parameters. See 'Device Drivers, Features, and Commands on SUSE Linux Enterprise Server 11 SP2 ' chap. 8, 'Setting up the qeth device driver' for details.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-312075"></a>12.9.9.2. Spinning mutex performance enhancement</h4></div></div></div><p>Depending on the usage of mutexes, thread scheduling and the status of the physical and virtual processors, additional information provided to the scheduler allows for more efficient and less costy decisions optimizing processor cycles. The status of a thread owning a locked mutex is examined and waiting threads are not scheduled unless the first is scheduled on a virtual and physical processor.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-312076"></a>12.9.9.3. Turn off Default Compression in OpenSSL</h4></div></div></div><p>With SLES11SP1 openSSL compresses data before encryption with impact on throughput (down) and CPU load (up) on platforms with cryptographic hardware. The behavior is now adjustable by the environment variable "OPENSSL_NO_DEFAULT_ZLIB"  depending on customer requirements. Set this environment variable per application or in a global config file.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h4 class="title"><a name="fate-311716"></a>12.9.9.4. openssl-ibmca: exploit z196 Hardware Accelerated Crypto Algorithms</h4></div></div></div><p>Added support for new CPACF algorithms in z196 / z114. New hardware accelerated algorithms are: AES-CFB and AES-OFB modes for key lengths 128, 192 and 256 / DES-CFB and DES-OFB modes / 3DES-CFB and 3DES-OFB modes.</p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="InfraPackArch.SystemZ.Misc"></a>12.9.10. Miscellaneous</h3></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>IBM System z Architecture Level Set (ALS) Preparation</p><p>
    To exploit new IBM System z architecture capabilities during the
    lifecycle of SUSE Linux Enterprise Server 11, support for machines of the types z900,
    z990, z800, z890 is deprecated in this release. SUSE plans to
    introduce an ALS earliest with SUSE Linux Enterprise Server 11 Service Pack 1 (SP1),
    latest with SP2.  After ALS, SUSE Linux Enterprise Server 11 only executes on z9 or newer
    processors.
   </p><p>
    With SUSE Linux Enterprise Server 11 GA, only machines of type z9 or newer are supported.
   </p><p>
    When developing software, we recommend to switch gcc to z9/z10 optimization:
   </p><div class="itemizedlist"><ul type="circle"><li><p>install gcc</p></li><li><p>install gcc-z9 package (change gcc options to -march=z9-109 -mtune=z10)</p></li></ul></div></li><li><p>The minimum required machine loader level for IPL of SUSE Linux Enterprise Server 11 from a SCSI disk is v1.4, which is included in the follow MCLs:</p><div class="itemizedlist"><ul type="circle"><li><p>z9, driver 67L, MCL G40943.001</p></li><li><p>z10, driver 75J, no MCL required on top of GA-level</p></li></ul></div><p>For older levels of the machine loader, the ramdisk load address of the installed SUSE Linux Enterprise Server 11 system needs to be manually changed from 0x2000000 to 0x1000000. To do this, open the <code class="filename">/etc/zipl.conf</code> file and change the lines containing
  <code class="option">ramdisk = &lt;initrd filename&gt;,0x2000000</code> to <code class="option">ramdisk = &lt;initrd filename&gt;,0x1000000</code> and run the <code class="filename">zipl</code> command afterwards. Note that this workaround may not work on systems with large amount of memory.</p></li><li><p>Minimum Storage Firmware Level for LUN Scanning</p><p>For LUN Scanning to work properly, the minimum storage firmware level should be:</p><div class="itemizedlist"><ul type="circle"><li><p>DS8000 Code Bundle Level 64.0.175.0</p></li><li><p> DS6000 Code Bundle Level 6.2.2.108</p></li></ul></div></li><li><p>Large Page Support in IBM System z</p><p>
   Large Page support allows processes to allocate process memory in chunks
   of 1 MiB instead of 4 KiB. This works through the hugetlbfs.
  </p></li><li><p>Collaborative memory management Stage II (CMM2) is currently not available.</p><p>IBM and SUSE are working to integrate this technology into the Linux Kernel
	and move it to a supported solution in SUSE Linux Enterprise Server as soon as available upstream.</p></li><li><p>Issue with SLES 11 and NSS under z/VM</p><p>Starting SLES 11 under z/VM with NSS sometimes causes a guest to logoff by itself.</p><p>Solution: IBM addresses this issue with APAR VM64578.</p></li></ul></div></div></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="Resolved"></a>Chapter 13. Resolved Issues</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>Bugfixes</p><p>
     This Service Pack contains all the latest bugfixes for each package
     released via the maintenance Web since the GA version.
    </p></li><li><p>Security Fixes</p><p>
     This Service Pack contains all the latest security fixes for each package
     released via the maintenance Web since the GA version.
    </p></li><li><p>Program Temporary Fixes</p><p>
     This Service Pack contains all the PTFs (Program Temporary Fix) for each
     package released via the maintenance Web since the GA version which were
     suitable for integration into the maintained common codebase.
    </p></li></ul></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="TechInfo"></a>Chapter 14. Technical Information</h2></div></div></div><p>
    This section contains information about system limits,
    a number of technical changes and enhancements for the experienced user.
  </p><p>When talking about CPUs we are following this terminology:</p><div class="variablelist"><dl><dt><span class="term">CPU Socket</span></dt><dd><p>The visible physical entity, as it is typically
     mounted to a motherboard or an equivalent.</p></dd><dt><span class="term">CPU Core</span></dt><dd><p>The (usually not visible) physical entity as
     reported by the CPU vendor.</p><p>On System z this is equivalent to an IFL.</p></dd><dt><span class="term">Logical CPU</span></dt><dd><p>This is what the Linux Kernel recognizes as a "CPU".</p><p>We avoid the word "thread" (which is sometimes used),
     as the word "thread" would also become ambiguous subsequently.</p></dd><dt><span class="term">Virtual CPU</span></dt><dd><p>A logical CPU as seen from within a Virtual Machine.</p></dd></dl></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="TechInfo.Kernel"></a>14.1. Kernel Limits</h2></div></div></div><p><a class="ulink" href="http://www.suse.com/products/server/technical-information/#Kernel" target="_top">http://www.suse.com/products/server/technical-information/#Kernel</a>
 </p><p>This table summarizes the various limits which exist in our recent kernels and utilities (if related) for SUSE Linux Enterprise Server 11.</p><div class="informaltable"><table border="1"><colgroup><col><col><col><col><col><col></colgroup><thead><tr><th><span class="emphasis"><em>SLES 11 (3.0.10)</em></span></th><th><span class="emphasis"><em>x86</em></span></th><th><span class="emphasis"><em>ia64</em></span></th><th><span class="emphasis"><em>x86_64</em></span></th><th><span class="emphasis"><em>s390x</em></span></th><th><span class="emphasis"><em>ppc64</em></span></th></tr></thead><tbody><tr><td>
						  <p>CPU bits</p>
					  </td><td>
						  <p>32</p>
					  </td><td>
						  <p>64</p>
					  </td><td>
						  <p>64</p>
					  </td><td>
						  <p>64</p>
					  </td><td>
						  <p>64</p>
					  </td></tr><tr><td>
						  <p>max. # Logical CPUs</p>
					  </td><td>
						  <p>32</p>
					  </td><td>
						  <p>4096</p>
					  </td><td>
						  <p>4096</p>
					  </td><td>
						  <p>64</p>
					  </td><td>
						  <p>1024</p>
					  </td></tr><tr><td>
						  <p>max. RAM (theoretical / certified)</p>
					  </td><td>
						  <p>64/16 GiB</p>
					  </td><td>
						  <p>1 PiB/8+ TiB</p>
					  </td><td>
						  <p>64 TiB/16 TiB</p>
					  </td><td>
						  <p>4 TiB/256 GiB</p>
					  </td><td>
						  <p>1 PiB/512 GiB</p>
					  </td></tr><tr><td>
						  <p>max. user-/kernelspace</p>
					  </td><td>
						  <p>3/1 GiB</p>
					  </td><td>
						  <p>2 EiB/&#966;</p>
					  </td><td>
						  <p>128 TiB/128 TiB</p>
					  </td><td>
						  <p>&#966;/&#966;</p>
					  </td><td>
						  <p>2 TiB/2 EiB</p>
					  </td></tr><tr><td>
                                    
                                    <p>max. swap space</p>
                                   </td><td colspan="5">
                                    <p>up to 29 * 64 GB (i386 and x86_64) or 30 * 64 GB (other architectures)</p>
                                   </td></tr><tr><td>
						  <p>max. #processes</p>
					  </td><td colspan="5">
						  <p>1048576</p>
					  </td></tr><tr><td>
						  <p>max. #threads per process</p>
					  </td><td colspan="5">
						  <p>tested with more than 120000; maximum limit depends on memory and other parameters</p>
					  </td></tr><tr><td>
						  <p>max. size per block device</p>
					  </td><td>
						  <p>up to 16 TiB</p>
					  </td><td colspan="4">
                                           <p>and up to 8 EiB on all 64-bit architectures</p>
					  </td></tr><tr><td>
                                    <p>FD_SETSIZE</p>
                                   </td><td colspan="5">
                                    <p>1024</p>
                                   </td></tr></tbody></table></div><p></p><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-313187"></a>14.1.1. Howto Run Applications that Do Not Recognize Linux Kernel 3.0 as Valid and Require Kernel 2.6 Instead</h3></div></div></div><p><span class="emphasis"><em>With SUSE Linux Enterprise 11 SP2 we introduce Linux Kernel 3.0. This kernel is a direct successor of the Linux kernel 2.6 series, thus all applications run without change.</em></span></p><p><span class="emphasis"><em>However, some broken applications or installation programs may check for "2.6" literally, thus failing to accept the compatibility of our kernel.</em></span></p><p> We provide two mechanisms to encourage applications to recognize the kernel 3.0 in SUSE Linux Enterprise 11 SP2 as a Linux kernel 2.6 compatible system: </p><div class="orderedlist"><ol type="1"><li><p> Use the uname26 command line tool, to start a single application in a 2.6 context. Usage is as easy as typing <code class="literal">uname26  [PROGRAM]</code> . More information can be found in the manpage of "setarch". </p></li><li><p> Some database systems and enterprise business applications expect processes and tasks run under a specific user name (not root). The Pluggable Authentication Modules (PAM) stack in SUSE Linux Enterprise allows to put a user into a 2.6 context. To achieve this, please add the username to the file <code class="literal">/etc/security/uname26.conf</code> . For more information, see the manpage for "pam_unix2". Caveat: we do not support the "root" user to run in a 2.6 context. </p></li></ol></div><p>
        </p><p></p><p>If you are running SAP applications please have a look at SAP Note #1310037 for more information on running SAP Applications within a Kernel 2.6 compatibility environment.</p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="TechInfo.KVM"></a>14.2. KVM Limits</h2></div></div></div><div class="informaltable"><table border="1"><colgroup><col><col></colgroup><tbody><tr><td>
                           <p>Guest RAM size</p>
                          </td><td>
                           <p>512 GiB</p>
                          </td></tr><tr><td>
                           <p>Virtual CPUs per guest</p>
                          </td><td>
                           
                           <p>64</p>
                          </td></tr><tr><td>
                           <p>Maximum number of NICs per guest</p>
                          </td><td>
                           <p>8</p>
                          </td></tr><tr><td>
                           <p>Block devices per guest</p>
                          </td><td>
                           <p>4 emulated, 20 para-virtual</p>
                          </td></tr><tr><td>
                           <p>Maximum number of guests</p>
                          </td><td>
                           <p>
                            Limit is defined as the total number of
                            vCPUs in all guests being no greater than
                            eight times the number of CPU cores in the
                            host.
                           </p>
                          </td></tr></tbody></table></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="TechInfo.Xen"></a>14.3. Xen Limits</h2></div></div></div><div class="informaltable"><table border="1"><colgroup><col><col></colgroup><thead><tr><th><span class="emphasis"><em>SLES 11 SP2</em></span></th><th><span class="emphasis"><em>x86</em></span></th></tr></thead><tbody><tr><td>
      <p>CPU bits</p>
     </td><td>
      <p>64</p>
     </td></tr><tr><td>
      <p>Logical CPUs (Xen Hypervisor)</p>
     </td><td>
      <p>255</p>
     </td></tr><tr><td>
      <p>Virtual CPUs per VM</p>
     </td><td>
      <p>32</p>
     </td></tr><tr><td>
      <p>Maximum supported memory (Xen Hypervisor)</p>
     </td><td>
      <p>2 TiB</p>
     </td></tr><tr><td>
      <p>Maximum supported memory (Dom0)</p>
     </td><td>
      <p>512 GiB</p>
     </td></tr><tr><td>
      <p>Virtual memory per VM</p>
     </td><td>
      <p>128 MiB-256 GiB</p>
     </td></tr><tr><td>
      <p>Total virtual devices per host</p>
     </td><td>
      <p>2048</p>
     </td></tr><tr><td>
      <p>Maximum number of NICs per host</p>
     </td><td>
      <p>8</p>
     </td></tr><tr><td>
      <p>Maximum number of vNICs per guest</p>
     </td><td>
      <p>8</p>
     </td></tr><tr><td>
      <p>Maximum number of guests per host</p>
     </td><td>
      <p>128</p>
     </td></tr></tbody></table></div><p>In Xen 4.1, the hypervisor bundled with SUSE Linux Enterprise Server 11 SP2, dom0 is
 able to see and handle a maximum of 512 logical CPUs. The hypervisor
 itself, however, can access up to logical 256 logical CPUs and schedule
 those for the VMs.</p><p>With SUSE Linux Enterprise Server 11 SP2, we removed the 32-bit hypervisor as a
 virtualization host. 32-bit virtual guests are not affected and are
 fully supported with the provided 64-bit hypervisor.</p><p></p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="TechInfo.Filesystems"></a>14.4. File Systems</h2></div></div></div><p><a class="ulink" href="http://www.novell.com/linux/filesystems/features.html" target="_top">http://www.novell.com/linux/filesystems/features.html</a>
 </p><p>SUSE Linux Enterprise was the first enterprise Linux distribution to support journaling file systems and logical volume managers back in 2000. Today, we have customers running XFS and ReiserFS with more than 8TiB in one file system, and our own SUSE Linux Enterprise engineering team is using all 3 major Linux journaling file systems for all its servers.</p><p>We are excited to add the OCFS2 cluster file system to the range of supported file systems in SUSE Linux Enterprise.</p><p>We propose to use XFS for large-scale file systems, on systems
 with heavy load and multiple parallel read- and write-operations (e.g.,
 for file serving with Samba, NFS, etc.).  XFS has been developed for
 such conditions, while typical desktop use (single write or read) will
 not necessarily benefit from its capabilities.
 </p><p>
  Due to technical limitations (of the bootloader), we do not support
  XFS to be used for <code class="filename">/boot</code>.
 </p><div class="informaltable"><table border="1"><colgroup><col><col><col><col><col><col></colgroup><thead><tr><th><span class="emphasis"><em>Feature</em></span></th><th><span class="emphasis"><em>Ext 3</em></span></th><th><span class="emphasis"><em>Reiserfs 3.6</em></span></th><th><span class="emphasis"><em>XFS</em></span></th><th><span class="emphasis"><em>Btrfs *</em></span></th><th><span class="emphasis"><em>OCFS 2 **</em></span></th></tr></thead><tbody><tr><td>
                                     <p>Data/Metadata Journaling</p>
				    </td><td>
                                     <p>&#8226;/&#8226;</p>
				    </td><td>
                                     <p>&#9675;/&#8226;</p>
				    </td><td>
                                     <p>&#9675;/&#8226;</p>
				    </td><td>
                                     <p>n/a *</p>
				    </td><td>
                                     <p>&#9675;/&#8226;</p>
				    </td></tr><tr><td>
                                     <p>Journal internal/external</p>
				    </td><td>
                                     <p>&#8226;/&#8226;</p>
				    </td><td>
                                     <p>&#8226;/&#8226;</p>
				    </td><td>
                                     <p>&#8226;/&#8226;</p>
				    </td><td>
                                     <p>n/a *</p>
				    </td><td>
                                     <p>&#8226;/&#9675;</p>
				    </td></tr><tr><td>
                                     <p>Offline extend/shrink</p>
				    </td><td>
                                     <p>&#8226;/&#8226;</p>
				    </td><td>
                                     <p>&#8226;/&#8226;</p>
				    </td><td>
                                     <p>&#9675;/&#9675;</p>
				    </td><td>
                                     <p>&#8226;/&#8226;</p>
				    </td><td>
                                     <p>&#8226;/&#9675;</p>
				    </td></tr><tr><td>
                                     <p>Online extend/shrink</p>
				    </td><td>
                                     <p>&#8226;/&#9675;</p>
				    </td><td>
                                     <p>&#8226;/&#9675;</p>
				    </td><td>
                                     <p>&#8226;/&#9675;</p>
				    </td><td>
                                     <p>&#8226;/&#8226;</p>
				    </td><td>
                                     <p>&#8226;/&#9675;</p>
				    </td></tr><tr><td><p>Sparse Files</p></td><td><p>&#8226;</p></td><td><p>&#8226;</p></td><td><p>&#8226;</p></td><td><p>&#8226;</p></td><td><p>&#8226;</p></td></tr><tr><td><p>Tail Packing</p></td><td><p>&#9675;</p></td><td><p>&#8226;</p></td><td><p>&#9675;</p></td><td><p>&#8226;</p></td><td><p>&#9675;</p></td></tr><tr><td><p>Defrag</p></td><td><p>&#9675;</p></td><td><p>&#9675;</p></td><td><p>&#8226;</p></td><td><p>&#8226;</p></td><td><p>&#9675;</p></td></tr><tr><td><p>Extended Attributes/ Access Control Lists</p></td><td><p>&#8226;/&#8226;</p></td><td><p>&#8226;/&#8226;</p></td><td><p>&#8226;/&#8226;</p></td><td><p>&#8226;/&#8226;</p></td><td><p>&#8226;/&#8226;</p></td></tr><tr><td>
                                     <p>Quotas</p>
				    </td><td>
                                     <p>&#8226;</p>
				    </td><td>
                                     <p>&#8226;</p>
				    </td><td>
                                     <p>&#8226;</p>
				    </td><td>
                                     <p>^</p>
				    </td><td>
                                     <p>&#8226;</p>
				    </td></tr><tr><td>
                                     <p>Dump/Restore</p>
				    </td><td>
                                     <p>&#8226;</p>
				    </td><td>
                                     <p>&#9675;</p>
				    </td><td>
                                     <p>&#8226;</p>
				    </td><td>
                                     <p>&#9675;</p>
				    </td><td>
                                     <p>&#9675;</p>
				    </td></tr><tr><td>
                                     <p>Blocksize default</p>
				    </td><td colspan="5" align="center">
                                     <p>4KiB</p>
				    </td></tr><tr><td>
                                     <p>max. File System Size</p>
				    </td><td>
                                     <p>16 TiB</p>
				    </td><td>
                                     <p>16 TiB</p>
				    </td><td>
                                     <p>8 EiB</p>
				    </td><td>
                                     <p>16 EiB</p>
				    </td><td>
                                     <p>16 TiB</p>
				    </td></tr><tr><td>
                                     <p>max. Filesize</p>
				    </td><td>
                                     <p>2 TiB</p>
				    </td><td>
                                     <p>1 EiB</p>
				    </td><td>
                                     <p>8 EiB</p>
				    </td><td>
                                     <p>16 EiB</p>
				    </td><td>
                                     <p>1 EiB</p>
				    </td></tr><tr><td>
                                         <p> </p>
					</td><td colspan="5">
                                         <p>* Btrfs is supported in SUSE Linux Enterprise Server 11 Service Pack 2; 1. Btrfs is a copy-on-write logging-style file system. Rather than journaling changes before writing them in-place, it writes them to a new location, then links it in. Until the last write, the new changes are not "committed". Due to the nature of the filesystem, Quotas will be implemented based on subvolumes in a future release.
                                         </p>
					</td></tr><tr><td>
                                         <p> </p>
					</td><td colspan="5">
                                         <p>** OCFS2 is fully supported as part of the SUSE Linux Enterprise High Availability Extension.</p>
					</td></tr></tbody></table></div><p>
  The maximum file size above can be larger than the file system's actual
  size due to usage of sparse blocks. Note that unless a file system
  comes with large file support (LFS), the maximum file size on a 32-bit
  system is 2 GB (2^31 bytes). Currently all of our standard file systems
  (including ext3 and ReiserFS) have LFS, which gives a maximum file
  size of 2^63 bytes in theory. The numbers in the above tables assume
  that the file systems are using 4 KiB block size. When using different
  block sizes, the results are different, but 4 KiB reflects the most
  common standard.
 </p><p>
  In this document: 1024 Bytes = 1 KiB; 1024 KiB = 1 MiB; 1024 MiB = 1
  GiB; 1024 GiB = 1 TiB; 1024 TiB = 1 PiB; 1024 PiB = 1 EiB. See also
  <a class="ulink" href="http://physics.nist.gov/cuu/Units/binary.html" target="_top">http://physics.nist.gov/cuu/Units/binary.html</a>.
 </p><p>
  
  NFSv4 with IPv6 is only supported for the client side. A NFSv4 server
  with IPv6 is not supported.
 </p><p>
  
  This version of Samba delivers integration with Windows 7 Active
  Directory Domains. In addition we provide the clustered version of
  Samba as part of SUSE Linux Enterprise High Availability 11 SP 1.
 </p><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-306585"></a>14.4.1. Support for the btrfs File System</h3></div></div></div><p><span class="emphasis"><em>Btrfs is a copy-on-write (CoW) general purpose file system.  Based on the CoW functionality, btrfs provides snapshoting. Beyond that data and metadata checksums improve the reliability of the file system. btrfs is highly scalable, but also supports online shrinking to adopt to real-life environments. On appropriate storage devices btrfs also supports the TRIM command.</em></span></p><p>
          <span class="emphasis"><em>Support</em></span>
        </p><p>With SUSE Linux Enterprise 11 SP2 the btrfs file system is supported as root file system, i.e. the file system for the operating system, across all architectures of  SUSE Linux Enterprise 11 SP2. Customers are adviced to use the YaST partitioner (or AutoYaST) to build their systems: YaST will prepare the btrfs file system for use with subvolumes and snapshots. Snapshots will be automatically enabled for the root file system using SUSE's snapper infrastructure. For more information about snapper, it's integration into ZYpp and YaST, and the YaST snapper module, see the SUSE Linux Enterprise documentation.</p><p>Offline-Migration from existing "ext" file systems (ext2, ext3, ext4) is supported.</p><p>RAID</p><p>Btrfs is supported on top of MD (multiple devices) and DM (device mapper) configurations. Please use the YaST partitioner to achieve a proper setup.</p><p>
          <span class="emphasis"><em>Future Plans</em></span>
        </p><div class="itemizedlist"><ul type="disc"><li><p>We are planning to announce support for btrfs' built-in multi volume handling and RAID in a later version of SUSE Linux Enterprise.</p></li><li><p>Transparent compression is implemented and mature. We are planning to support this functionality in the YaST partitioner in a future release.</p></li><li><p>We are commited to actively work on the btrfs file system with the community, and we keep customers and partners informed about progress and experience in terms of scalability and performance. This may also apply to cloud and cloud storage infrastructures.</p></li></ul></div><p>
          <span class="emphasis"><em>Online Check and Repair Functionality</em></span>
        </p><p>Check and repair functionality ("scrub") is available as part of the btrfs command line tools. "Scrub" is aimed to verify data and metadata assuming the tree structures are fine.  "Scrub" can (and should) be run periodically on a mounted file system: it runs
as a background process during normal operation.</p><p>With the release of SUSE Linux Enterprise 11 SP2, the long awaited "fsck.btrfs" tool is available in the SUSE Linux Enterprise update repositories.</p><p>
          <span class="emphasis"><em>Capacity Planning</em></span>
        </p><p>If you are planning to use btrfs with its snapshot capability, it is advisable to reserve twice as much disk space than the standard storage proposal. This is automatically done by the YaST2 partitioner for the root file system.</p><p>More information about btrfs can be found in the SUSE Linux Enterprise 11 documentation.</p></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="TechInfo.KernelModules"></a>14.5. Kernel Modules</h2></div></div></div><p>
  An important requirement for every Enterprise operating system is the
  level of support a customer receives for his environment.  Kernel
  modules are the most relevant connector between hardware
  ("controllers") and the operating system. Every kernel module in
  SUSE Linux Enterprise Server 11 has a flag 'supported' with three possible values:
  <code class="literal">"yes", "external", ""</code> (empty, not set,
  "unsupported").
 </p><p>
  The following rules apply:
 </p><div class="itemizedlist"><ul type="disc"><li><p>
    All modules of a self-recompiled kernel are by default marked as
    unsupported.
   </p></li><li><p>
    Kernel Modules supported by SUSE partners and delivered using SUSE's
    Partner Linux Driver process are marked "external".
   </p></li><li><p>
    If the <code class="literal">"supported"</code> flag is not set, loading this
    module will taint the kernel.  Tainted kernels are not supported. To
    avoid this, not supported Kernel modules are included in an extra
    RPM (kernel-&lt;flavor&gt;-extra) and will not be loaded by default
    ("flavor"=default|smp|xen|...).  In addition, these unsupported
    modules are not available in the installer, and the package
    kernel-$flavor-extra is not on the SUSE Linux Enterprise Server media.
   </p></li><li><p>
    Kernel Modules not provided under a license compatible to the
    license of the Linux kernel will also taint the kernel; see
    <code class="filename">/usr/src/linux/Documentation/sysctl/kernel.txt</code>
    and the state of <code class="filename">/proc/sys/kernel/tainted</code>.
   </p></li></ul></div><p>Technical Background</p><p></p><div class="itemizedlist"><ul type="disc"><li><p>Linux Kernel</p><p>The value of /proc/sys/kernel/unsupported defaults to 2 on
   SUSE Linux Enterprise Server 11 ("do not warn in syslog when loading unsupported
   modules"). This is the default used in the installer as well as in
   the installed system.  See
   <code class="filename">/usr/src/linux/Documentation/sysctl/kernel.txt</code>
   for more information.
   </p></li><li><p>modprobe</p><p>The <span class="command"><strong>modprobe</strong></span> utility for checking module
   dependencies and loading modules appropriately checks for the value
   of the "supported" flag. If the value is <code class="literal">"yes"</code> or
   <code class="literal">"external"</code> the module will be loaded, otherwise it
   will not. See below, for information on how to override this
   behavior.</p><p>Note: SUSE does not generally support removing of storage modules via <span class="command"><strong>modprobe -r</strong></span>.</p></li></ul></div><p>Working with Unsupported Modules</p><p>While the general supportability is important, there might occur
 situations where loading an unsupported module is required (e.g., for
 testing or debugging purposes, or if your hardware vendor provides a
 hotfix):</p><div class="itemizedlist"><ul type="disc"><li><p>
    You can override the default by changing the variable
    <code class="option">allow_unsupported_modules</code> in
    <code class="filename">/etc/modprobe.d/unsupported-modules</code> and set the
    value to "<code class="literal">1</code>".
   </p><p>
    If you only want to try loading a module once, the
    <code class="option">--allow-unsupported-modules</code> command-line switch can
    be used with <code class="filename">modprobe</code>.  (For more information,
    see <span class="command"><strong>man modprobe</strong></span>).
   </p></li><li><p>
    During installation, unsupported modules may be added through driver
    update disks, and they will be loaded.
   </p><p>
    To enforce loading of unsupported modules during boot and
    afterwards, please use the kernel command line option
    <code class="option">oem-modules</code>.
   </p><p>
    While installing and initializing the <code class="systemitem">module-init-tools</code> package, the kernel
    flag <code class="option">"TAINT_NO_SUPPORT"</code>
    (<code class="filename">/proc/sys/kernel/tainted</code>) will be
    evaluated. If the kernel is already tainted,
    <code class="option">allow_unsupported_modules</code> will be enabled. This
    will prevent unsupported modules from failing in the system being
    installed. (If no unsupported modules are present during
    installation and the other special kernel command line option is not
    used, the default will still be to disallow unsupported modules.)
   </p></li><li><p>
    If you install unsupported modules after the initial installation
    and want to enable those modules to be loaded during system boot,
    please do not forget to run <span class="command"><strong>depmod</strong></span> and
    <span class="command"><strong>mkinitrd</strong></span>.
   </p></li></ul></div><p>
  Remember that loading and running unsupported modules will make the
  kernel and the whole system unsupported by SUSE.
 </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="TechInfo.IPv6"></a>14.6. IPv6 Implementation and Compliance</h2></div></div></div><p>
  SUSE Linux Enterprise Server 11 is compliant to IPv6 Logo Phase 2. However, when running the
  respective tests, you may see some tests failing.  For various
  reasons, we cannot enable all the configuration options by default,
  which are necessary to pass all the tests. For details, see below.
 </p><div class="itemizedlist"><ul type="disc"><li><p>
    Section 3: RFC 4862 - IPv6 Stateless Address Autoconfiguration
   </p><p>
    Some tests fail because of the default DAD handling in Linux;
    disabling the complete interface is possible, but not the default
    behavior (because security-wise, this might open a DoS attack
    vector, a malicious node on a network could shutdown the complete
    segment) this is still conforming to RFC 4862: the shutdown of the
    interface is a "should", not a mandatory ("must") rule.
   </p><p>
    The Linux kernel allows you to change the default behavior with a
    sysctl parameter.  To do this on SUSE Linux Enterprise Server 11, you need to make the
    following changes in configuration:
   </p><div class="itemizedlist"><ul type="circle"><li><p>
      Add ipv6 to the modules load early on boot
     </p><p>
      Edit <code class="filename">/etc/sysconfig/kernel</code> and add ipv6 to
      MODULES_LOADED_ON_BOOT
      e.g. <code class="option">MODULES_LOADED_ON_BOOT="ipv6"</code>. This is needed
      for the second change to work, if ipv6 is not loaded early enough,
      setting the sysctl fails.
     </p></li><li><p>Add the following lines to /etc/sysctl.conf
     </p><pre class="screen">## shutdown IPV6 on MAC based duplicate address detection
net.ipv6.conf.default.accept_dad = 2
net.ipv6.conf.all.accept_dad = 2
net.ipv6.conf.eth0.accept_dad = 2
net.ipv6.conf.eth1.accept_dad = 2
      </pre><p>
      Note: if you use other interfaces (e.g., eth2), please modify the
      lines. With these changes, all tests for RFC 4862 should
      pass.</p></li></ul></div></li><li><p>Section 4: RFC 1981 - Path MTU Discovery for IPv6	</p><div class="itemizedlist"><ul type="circle"><li><p>
      Test v6LC.4.1.10: Multicast Destination - One Router
     </p></li><li><p>
      Test v6LC.4.1.11: Multicast Destination - Two Routers
     </p></li></ul></div><p>On these two tests ping6 needs to be told to allow
   defragmentation of multicast packets. Newer ping6 versions have this
   disabled by default. Use: <span class="command"><strong>ping6 -M want &lt;other
   parameters&gt;</strong></span>. See <span class="command"><strong>man ping6</strong></span> for more
   information.
   </p></li><li><p>Enable IPv6 in YaST for SCTP Support</p><p>SCTP is dependent on IPv6, so in order to successfully insert
   the SCTP module, IPv6 must be enabled in YaST. This allows for the
   IPv6 module to be automatically inserted when <span class="command"><strong>modprobe
   sctp</strong></span> is called.</p></li></ul></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="TechInfo.Other"></a>14.7. Other Technical Information</h2></div></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="fate-312605"></a>14.7.1. Storing Log Files on the tmpfs File System Is Unsupported</h3></div></div></div><p> Ensure all your logs go through permanent local storage or the
    network.  For example, putting <code class="literal">/var/log</code> on a tmpfs file system means
    that they will not survive a system boot. This limits your ability,
    and the one of SUSE, to analyze log files in case of a support
    request. </p><p>Exceptions are configurations where you save log files via
    syslog on a remote log server and permanently store the log files on the log server. Note: Not all log files can be redirected to a remote log server
    (e.g. yast-logs, boot logs and others); if these files are not
    available, support may be very hard to effectively diagnose issues
    and support the system.</p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1175798"></a>14.7.2. libica 2.0.2 is available in SLES 11 SP2 for s390x customers</h3></div></div></div><p>
    The libica package contains the interface library routines used by
    IBM modules to interface with IBM Cryptographic Hardware
    (ICA). Starting with SLES 11 SP1, libica is provided in the s390x
    distribution in two flavors of packages: libica-1_3_9 and
    libica-2_0_2, providing libica versions 1.3.9 and 2.0.2
    respectively.
   </p><p>
    libica 1.3.9 is provided for compatibility reasons with legacy
    hardware present e.g. in the ppc64 architecture. For s390x users
    it's always recommended to use the new libica 2.0.2 library since it
    supports all newer s390x hardware, larger key sizes and is backwards
    compatible with any ICA device driver in the s390x architecture.
   </p><p>
    You may choose to continue using libica 1.3.9 if you don't have
    newer Cryptographic hardware to exploit or wish continue using
    custom applications that don't support the libica 2.0.2 library
    yet. Both openCryptoki and openssl-ibmca, the two main exploiters
    for the libica interface, are provided in SLES 11 SP2 to support the
    newer libica 2.0.2 library.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1175275"></a>14.7.3. Yast support for layer 2 devices</h3></div></div></div><p>
	YaST writes the MAC address for layer 2 devices only if they of the card_types:
    </p><div class="orderedlist"><ol type="1"><li><p>OSD_100</p></li><li><p>OSD_1000</p></li><li><p>OSD_10GIG</p></li><li><p>OSD_FE_LANE</p></li><li><p>OSD_GbE_LANE</p></li><li><p>OSD_Express</p></li></ol></div><p>
	Per intent Yast does not write the MAC address for devices of the types:
   </p><div class="orderedlist"><ol type="1"><li><p>HiperSockets</p></li><li><p>GuestLAN/VSWITCH QDIO</p></li><li><p>OSM</p></li><li><p>OSX</p></li></ol></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1173123"></a>14.7.4. Changes to Network Setup</h3></div></div></div><p>
     The script <span class="command"><strong>modify_resolvconf</strong></span> is removed in favor
     of a more versatile script called
     <span class="command"><strong>netconfig</strong></span>. This new script handles specific
     network settings from multiple sources more flexibly and
     transparently. See the documentation and man-page of
     netconfig for more information.
    </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1173872"></a>14.7.5. Memory cgroups</h3></div></div></div><p>
    Memory cgroups are now disabled for machines where they cause memory
    exhaustion and crashes. Namely, X86 32-bit systems with PAE support
    and more than 8G in any memory node have this feature disabled.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1175859"></a>14.7.6. MCELog</h3></div></div></div><p>
    The mcelog package logs and parses/translates Machine Check
    Exceptions (MCE) on hardware errors (also including memory
    errors). Formerly this has been done by a cron job executed
    hourly. Now hardware errors are immediately processed by an mcelog
    daemon.
   </p><p>
      However, the mcelog service is not enabled by default resulting in
      memory and CPU errors also not being logged by default. In
      addition, mcelog has a new feature to also handle predictive bad
      page offlining and automatic core offlining when cache errors
      happen.
    </p><p>
     The service can either be enabled via the YaST runlevel editor or
     via commandline with:
    </p><pre class="screen">chkconfig mcelog on
rcmcelog start</pre></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1175878"></a>14.7.7. Locale Settings in <code class="filename">~/.i18n</code></h3></div></div></div><p>
    If you are not satisfied with locale system defaults, change the
    settings in <code class="filename">~/.i18n</code>. Entries in
    <code class="filename">~/.i18n</code> override system defaults from
    <code class="filename">/etc/sysconfig/language</code>.  Use the same variable
    names but without the <code class="literal">RC_</code> namespace prefixes; for
    example, use <code class="literal">LANG</code> instead of
    <code class="literal">RC_LANG</code>. For more information about locales in
    general, see "Language and Country-Specific Settings" in the
    Administration Guide.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1174674"></a>14.7.8. Configuration of kdump</h3></div></div></div><p>
    kdump is useful, if the kernel is crashing or otherwise misbehaving
    and a kernel core dump needs to be captured for analysis.
   </p><p>
    Use YaST (<span class="guimenu">System</span>+<span class="guimenu">Kernel
    Kdump</span>) to configure your environment.
    </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1175520"></a>14.7.9. Configuring Authentication for kdump through YaST with ssh/scp as
    Target</h3></div></div></div><p>
    When kdump is configured through YaST with ssh/scp as target and the
    target system is SUSE Linux Enterprise, then enable authentication
    using either of the following ways:
   </p><div class="orderedlist"><ol type="1"><li><p>
      Copy the public keys to the target system:
     </p><pre class="screen">ssh-copy-id -i ~/.ssh/id_*.pub  &lt;username&gt;@&lt;target system IP&gt;</pre><p>
      or
     </p></li><li><p>
      Change the <code class="literal">PasswordAuthentication</code> setting in
      <code class="filename">/etc/ssh/sshd_config</code> of the target system
      from:
     </p><pre class="screen">PasswordAuthentication no</pre><p>
      to:
     </p><pre class="screen">PasswordAuthentication yes</pre></li><li><p>
      After the changing <code class="literal">PasswordAuthentication</code> in
      <code class="filename">/etc/ssh/sshd_config</code> restart the sshd service
      on the target system with:
     </p><pre class="screen">rcsshd restart</pre></li></ol></div></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1175968"></a>14.7.10. JPackage Standard for Java Packages</h3></div></div></div><p>
    Java packages are changed to follow the JPackage Standard (<a class="ulink" href="http://www.jpackage.org/" target="_top">http://www.jpackage.org/</a>). For more information, see the
    documentation in
    <code class="filename">/usr/share/doc/packages/jpackage-utils/</code>.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1175677"></a>14.7.11. Pulseaudio</h3></div></div></div><p>
    For better sound functionality on SUSE Linux Enterprise systems we
    strongly recommend that pulseaudio 0.9.14 or higher is installed.
    This version is available via maintenance channels for SUSE Linux
    Enterprise systems registered with SUSE.
   </p></div><div class="section" lang="en"><div class="titlepage"><div><div><h3 class="title"><a name="id1175933"></a>14.7.12. Stopping Cron Status Messages</h3></div></div></div><p>
To avoid the mail-flood caused by cron status messages, the default
value of <code class="literal">SEND_MAIL_ON_NO_ERROR</code> in
<code class="filename">/etc/sysconfig/cron</code> is now set to
"<code class="literal">no</code>" for new installations.  Even with this setting
to "<code class="literal">no</code>", cron data output will still be send to the
<code class="literal">MAILTO</code> address, as documented in the cron manpage.
   </p><p>
In the update case it is recommended to set these values according to
your needs.
   </p></div></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="Documentation"></a>Chapter 15. Documentation and Other Information</h2></div></div></div><div class="itemizedlist"><ul type="disc"><li><p>
     Read the READMEs on the DVDs.
    </p></li><li><p>
     Get the detailed changelog information about a particular
     package from the RPM (with filename &lt;FILENAME&gt;):
    </p><pre class="screen">rpm --changelog -qp &lt;FILENAME&gt;.rpm
    </pre></li><li><p>
     Check the <code class="filename">ChangeLog</code> file in the top level of DVD1 for
     a chronological log of all changes made to the updated packages.
    </p></li><li><p>
     Find more information in the <code class="filename">docu</code> directory
     of DVD1 of the SUSE Linux Enterprise Server 11 Service Pack 2 DVDs. This directory includes PDF versions
     of the SUSE Linux Enterprise Server 11 Installation Quick Start and Deployment Guides.
    </p></li><li><p>
       <a class="ulink" href="http://www.suse.com/documentation/sles11/" target="_top">http://www.suse.com/documentation/sles11/</a> contains
       additional or updated documentation for SUSE Linux Enterprise Server 11 Service Pack 2.
     </p></li><li><p>
     
     These Release Notes are identical across all architectures, and are
     available online at <a class="ulink" href="http://www.suse.com/releasenotes/" target="_top">http://www.suse.com/releasenotes/</a>.
     
     
     
     
     
     
    </p></li><li><p>
     Visit <a class="ulink" href="http://www.suse.com/products/" target="_top">http://www.suse.com/products/</a> for the latest
     product news from SUSE and <a class="ulink" href="http://www.suse.com/download-linux/source-code.html" target="_top">http://www.suse.com/download-linux/source-code.html</a> for
     additional information on the source code of SUSE Linux Enterprise products.
    </p></li></ul></div><div class="section" lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="fate-310907"></a>15.1. AutoYaST Documentation</h2></div></div></div><p>AutoYaST documentation is available as part of the sles-manuals_en package (HTML) and as the sles-autoyast_en-pdf subpackage (PDF).</p></div></div><div class="chapter" lang="en"><div class="titlepage"><div><div><h2 class="title"><a name="Legal"></a>Chapter 16. Legal Notices</h2></div></div></div><p>SUSE makes no representations or warranties with respect to the
  contents or use of this documentation, and specifically disclaims any express
  or implied warranties of merchantability or fitness for any particular
  purpose. Further, SUSE reserves the right to revise this publication
  and to make changes to its content, at any time, without the obligation to notify
  any person or entity of such revisions or changes.</p><p>Further, SUSE makes no representations or warranties with
  respect to any software, and specifically disclaims any express or implied
  warranties of merchantability or fitness for any particular purpose. Further,
  SUSE reserves the right to make changes to any and all parts of
  SUSE software, at any time, without any obligation to notify any person or
  entity of such changes.</p><p>Any products or technical information provided under this Agreement may
  be subject to U.S. export controls and the trade laws of other countries. You
  agree to comply with all export control regulations and to obtain any
  required licenses or classifications to export, re-export, or import
  deliverables. You agree not to export or re-export to entities on the current
  U.S. export exclusion lists or to any embargoed or terrorist countries as
  specified in U.S. export laws. You agree to not use deliverables for
  prohibited nuclear, missile, or chemical/biological weaponry end uses. Please
  refer to <a class="ulink" href="http://www.novell.com/info/exports/" target="_top">http://www.novell.com/info/exports/</a> for more information on
  exporting SUSE software. SUSE assumes no responsibility for your failure
  to obtain any necessary export approvals.</p><p>Copyright (c) 2010, 2011, 2012 SUSE. All rights reserved. No part of this
  publication may be reproduced, photocopied, stored on a retrieval system, or
  transmitted without the express written consent of the publisher.</p><p>SUSE has intellectual property rights relating to technology
  embodied in the product that is described in this document. In particular,
  and without limitation, these intellectual property rights may include one or
  more of the U.S. patents listed at
  <a class="ulink" href="http://www.novell.com/company/legal/patents/" target="_top">http://www.novell.com/company/legal/patents/</a> and one or more additional
  patents or pending patent applications in the U.S. and other
  countries.</p><p>For SUSE trademarks, see Novell Trademark ad Service Mark list
  (<a class="ulink" href="http://www.novell.com/company/legal/trademarks/tmlist.html" target="_top">http://www.novell.com/company/legal/trademarks/tmlist.html</a>).
  All third-party trademarks are the property of their respective owners.</p></div><div class="colophon" lang="en"><h2 class="title"><a name="id1173712"></a>Colophon</h2><p>
		Thanks for using SUSE Linux Enterprise Server in your business.
	</p><p>
		The SUSE Linux Enterprise Server Team.
	</p></div></div></body></html>
